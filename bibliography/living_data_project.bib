
@article{purgar_quantifying_2022,
	title = {Quantifying research waste in ecology},
	volume = {6},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-022-01820-0},
	doi = {10.1038/s41559-022-01820-0},
	language = {en},
	number = {9},
	urldate = {2025-02-27},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Klanjscek, Tin and Culina, Antica},
	month = jul,
	year = {2022},
	pages = {1390--1397},
}

@article{purgar_supporting_2024,
	title = {Supporting study registration to reduce research waste},
	volume = {8},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-024-02433-5},
	doi = {10.1038/s41559-024-02433-5},
	language = {en},
	number = {8},
	urldate = {2025-02-11},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Glasziou, Paul and Klanjscek, Tin and Nakagawa, Shinichi and Culina, Antica},
	month = jun,
	year = {2024},
	pages = {1391--1399},
}

@article{nakagawa_poor_2025,
	title = {Poor hypotheses and research waste in biology: learning from a theory crisis in psychology},
	volume = {23},
	issn = {1741-7007},
	shorttitle = {Poor hypotheses and research waste in biology},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-025-02134-w},
	doi = {10.1186/s12915-025-02134-w},
	abstract = {Abstract While psychologists have extensively discussed the notion of a “theory crisis” arising from vague and incorrect hypotheses, there has been no debate about such a crisis in biology. However, biologists have long discussed communication failures between theoreticians and empiricists. We argue such failure is one aspect of a theory crisis because misapplied and misunderstood theories lead to poor hypotheses and research waste. We review its solutions and compare them with methodology-focused solutions proposed for replication crises. We conclude by discussing how promoting inclusion, diversity, equity, and accessibility (IDEA) in theoretical biology could contribute to ameliorating breakdowns in the theory-empirical cycle.},
	language = {en},
	number = {1},
	urldate = {2025-02-11},
	journal = {BMC Biology},
	author = {Nakagawa, Shinichi and Armitage, David W. and Froese, Tom and Yang, Yefeng and Lagisz, Malgorzata},
	month = feb,
	year = {2025},
	pages = {33},
}

@article{gould_same_2025,
	title = {Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology},
	volume = {23},
	issn = {1741-7007},
	shorttitle = {Same data, different analysts},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-02101-x},
	doi = {10.1186/s12915-024-02101-x},
	language = {en},
	number = {1},
	urldate = {2025-08-28},
	journal = {BMC Biology},
	author = {Gould, Elliot and Fraser, Hannah S. and Parker, Timothy H. and Nakagawa, Shinichi and Griffith, Simon C. and Vesk, Peter A. and Fidler, Fiona and Hamilton, Daniel G. and Abbey-Lee, Robin N. and Abbott, Jessica K. and Aguirre, Luis A. and Alcaraz, Carles and Aloni, Irith and Altschul, Drew and Arekar, Kunal and Atkins, Jeff W. and Atkinson, Joe and Baker, Christopher M. and Barrett, Meghan and Bell, Kristian and Bello, Suleiman Kehinde and Beltrán, Iván and Berauer, Bernd J. and Bertram, Michael Grant and Billman, Peter D. and Blake, Charlie K. and Blake, Shannon and Bliard, Louis and Bonisoli-Alquati, Andrea and Bonnet, Timothée and Bordes, Camille Nina Marion and Bose, Aneesh P. H. and Botterill-James, Thomas and Boyd, Melissa Anna and Boyle, Sarah A. and Bradfer-Lawrence, Tom and Bradham, Jennifer and Brand, Jack A. and Brengdahl, Martin I. and Bulla, Martin and Bussière, Luc and Camerlenghi, Ettore and Campbell, Sara E. and Campos, Leonardo L. F. and Caravaggi, Anthony and Cardoso, Pedro and Carroll, Charles J. W. and Catanach, Therese A. and Chen, Xuan and Chik, Heung Ying Janet and Choy, Emily Sarah and Christie, Alec Philip and Chuang, Angela and Chunco, Amanda J. and Clark, Bethany L. and Contina, Andrea and Covernton, Garth A. and Cox, Murray P. and Cressman, Kimberly A. and Crotti, Marco and Crouch, Connor Davidson and D’Amelio, Pietro B. and De Sousa, Alexandra Allison and Döbert, Timm Fabian and Dobler, Ralph and Dobson, Adam J. and Doherty, Tim S. and Drobniak, Szymon Marian and Duffy, Alexandra Grace and Duncan, Alison B. and Dunn, Robert P. and Dunning, Jamie and Dutta, Trishna and Eberhart-Hertel, Luke and Elmore, Jared Alan and Elsherif, Mahmoud Medhat and English, Holly M. and Ensminger, David C. and Ernst, Ulrich Rainer and Ferguson, Stephen M. and Fernandez-Juricic, Esteban and Ferreira-Arruda, Thalita and Fieberg, John and Finch, Elizabeth A. and Fiorenza, Evan A. and Fisher, David N. and Fontaine, Amélie and Forstmeier, Wolfgang and Fourcade, Yoan and Frank, Graham S. and Freund, Cathryn A. and Fuentes-Lillo, Eduardo and Gandy, Sara L. and Gannon, Dustin G. and García-Cervigón, Ana I. and Garretson, Alexis C. and Ge, Xuezhen and Geary, William L. and Géron, Charly and Gilles, Marc and Girndt, Antje and Gliksman, Daniel and Goldspiel, Harrison B. and Gomes, Dylan G. E. and Good, Megan Kate and Goslee, Sarah C. and Gosnell, J. Stephen and Grames, Eliza M. and Gratton, Paolo and Grebe, Nicholas M. and Greenler, Skye M. and Griffioen, Maaike and Griffith, Daniel M. and Griffith, Frances J. and Grossman, Jake J. and Güncan, Ali and Haesen, Stef and Hagan, James G. and Hager, Heather A. and Harris, Jonathan Philo and Harrison, Natasha Dean and Hasnain, Sarah Syedia and Havird, Justin Chase and Heaton, Andrew J. and Herrera-Chaustre, María Laura and Howard, Tanner J. and Hsu, Bin-Yan and Iannarilli, Fabiola and Iranzo, Esperanza C. and Iverson, Erik N. K. and Jimoh, Saheed Olaide and Johnson, Douglas H. and Johnsson, Martin and Jorna, Jesse and Jucker, Tommaso and Jung, Martin and Kačergytė, Ineta and Kaltz, Oliver and Ke, Alison and Kelly, Clint D. and Keogan, Katharine and Keppeler, Friedrich Wolfgang and Killion, Alexander K. and Kim, Dongmin and Kochan, David P. and Korsten, Peter and Kothari, Shan and Kuppler, Jonas and Kusch, Jillian M. and Lagisz, Malgorzata and Lalla, Kristen Marianne and Larkin, Daniel J. and Larson, Courtney L. and Lauck, Katherine S. and Lauterbur, M. Elise and Law, Alan and Léandri-Breton, Don-Jean and Lembrechts, Jonas J. and L’Herpiniere, Kiara and Lievens, Eva J. P. and De Lima, Daniela Oliveira and Lindsay, Shane and Luquet, Martin and MacLeod, Ross and Macphie, Kirsty H. and Magellan, Kit and Mair, Magdalena M. and Malm, Lisa E. and Mammola, Stefano and Mandeville, Caitlin P. and Manhart, Michael and Manrique-Garzon, Laura Milena and Mäntylä, Elina and Marchand, Philippe and Marshall, Benjamin Michael and Martin, Charles A. and Martin, Dominic Andreas and Martin, Jake Mitchell and Martinig, April Robin and McCallum, Erin S. and McCauley, Mark and McNew, Sabrina M. and Meiners, Scott J. and Merkling, Thomas and Michelangeli, Marcus and Moiron, Maria and Moreira, Bruno and Mortensen, Jennifer and Mos, Benjamin and Muraina, Taofeek Olatunbosun and Murphy, Penelope Wrenn and Nelli, Luca and Niemelä, Petri and Nightingale, Josh and Nilsonne, Gustav and Nolazco, Sergio and Nooten, Sabine S. and Novotny, Jessie Lanterman and Olin, Agnes Birgitta and Organ, Chris L. and Ostevik, Kate L. and Palacio, Facundo Xavier and Paquet, Matthieu and Parker, Darren James and Pascall, David J. and Pasquarella, Valerie J. and Paterson, John Harold and Payo-Payo, Ana and Pedersen, Karen Marie and Perez, Grégoire and Perry, Kayla I. and Pottier, Patrice and Proulx, Michael J. and Proulx, Raphaël and Pruett, Jessica L and Ramananjato, Veronarindra and Randimbiarison, Finaritra Tolotra and Razafindratsima, Onja H. and Rennison, Diana J. and Riva, Federico and Riyahi, Sepand and Roast, Michael James and Rocha, Felipe Pereira and Roche, Dominique G. and Román-Palacios, Cristian and Rosenberg, Michael S. and Ross, Jessica and Rowland, Freya E. and Rugemalila, Deusdedith and Russell, Avery L. and Ruuskanen, Suvi and Saccone, Patrick and Sadeh, Asaf and Salazar, Stephen M. and Sales, Kris and Salmón, Pablo and Sánchez-Tójar, Alfredo and Santos, Leticia Pereira and Santostefano, Francesca and Schilling, Hayden T. and Schmidt, Marcus and Schmoll, Tim and Schneider, Adam C. and Schrock, Allie E. and Schroeder, Julia and Schtickzelle, Nicolas and Schultz, Nick L. and Scott, Drew A. and Scroggie, Michael Peter and Shapiro, Julie Teresa and Sharma, Nitika and Shearer, Caroline L. and Simón, Diego and Sitvarin, Michael I. and Skupien, Fabrício Luiz and Slinn, Heather Lea and Smith, Grania Polly and Smith, Jeremy A. and Sollmann, Rahel and Whitney, Kaitlin Stack and Still, Shannon Michael and Stuber, Erica F. and Sutton, Guy F. and Swallow, Ben and Taff, Conor Claverie and Takola, Elina and Tanentzap, Andrew J. and Tarjuelo, Rocío and Telford, Richard J. and Thawley, Christopher J. and Thierry, Hugo and Thomson, Jacqueline and Tidau, Svenja and Tompkins, Emily M. and Tortorelli, Claire Marie and Trlica, Andrew and Turnell, Biz R. and Urban, Lara and Van De Vondel, Stijn and Van Der Wal, Jessica Eva Megan and Van Eeckhoven, Jens and Van Oordt, Francis and Vanderwel, K. Michelle and Vanderwel, Mark C. and Vanderwolf, Karen J. and Vélez, Juliana and Vergara-Florez, Diana Carolina and Verrelli, Brian C. and Vieira, Marcus Vinícius and Villamil, Nora and Vitali, Valerio and Vollering, Julien and Walker, Jeffrey and Walker, Xanthe J. and Walter, Jonathan A. and Waryszak, Pawel and Weaver, Ryan J. and Wedegärtner, Ronja E. M. and Weller, Daniel L. and Whelan, Shannon and White, Rachel Louise and Wolfson, David William and Wood, Andrew and Yanco, Scott W. and Yen, Jian D. L. and Youngflesh, Casey and Zilio, Giacomo and Zimmer, Cédric and Zimmerman, Gregory Mark and Zitomer, Rachel A.},
	month = feb,
	year = {2025},
	pages = {35},
}

@article{bourne_ten_2017,
	title = {Ten simple rules to consider regarding preprint submission},
	volume = {13},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005473},
	doi = {10.1371/journal.pcbi.1005473},
	language = {en},
	number = {5},
	urldate = {2023-09-21},
	journal = {PLOS Computational Biology},
	author = {Bourne, Philip E. and Polka, Jessica K. and Vale, Ronald D. and Kiley, Robert},
	month = may,
	year = {2017},
	keywords = {Medicine and health sciences, Scientists, Scientific publishing, Internet, Mathematical physics, Medical journals, Peer review, Software tools},
	pages = {e1005473},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{gopalakrishna_prevalence_2022,
	title = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors: {A} survey among academic researchers in {The} {Netherlands}},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263023},
	doi = {10.1371/journal.pone.0263023},
	abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the “publish or perish” incentive system promotes research integrity.},
	language = {en},
	number = {2},
	urldate = {2023-09-05},
	journal = {PLOS ONE},
	author = {Gopalakrishna, Gowri and Riet, Gerben ter and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
	month = feb,
	year = {2022},
	keywords = {Deception, Linear regression analysis, Medical humanities, Medicine and health sciences, Open science, Research integrity, Scientific misconduct, Surveys},
	pages = {e0263023},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{ulrich_questionable_2020,
	title = {Questionable research practices may have little effect on replicability},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.58237},
	doi = {10.7554/eLife.58237},
	abstract = {This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.},
	urldate = {2023-09-05},
	journal = {eLife},
	author = {Ulrich, Rolf and Miller, Jeff},
	editor = {Rodgers, Peter and Thompson, William Hedley and Francis, Gregory},
	month = sep,
	year = {2020},
	keywords = {base rate of true effects, false positives, mathematical modelling of research process, meta-research, p-hacking, replicability},
	pages = {e58237},
	annote = {Publisher: eLife Sciences Publications, Ltd},
	annote = {Publisher: eLife Sciences Publications, Ltd},
}

@article{filazzola_replication_2021,
	title = {Replication in field ecology: {Identifying} challenges and proposing solutions},
	volume = {12},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Replication in field ecology},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13657},
	doi = {10.1111/2041-210X.13657},
	language = {en},
	number = {10},
	urldate = {2021-10-09},
	journal = {Methods in Ecology and Evolution},
	author = {Filazzola, Alessandro and Cahill, James F.},
	month = oct,
	year = {2021},
	pages = {1780--1792},
}

@misc{magazine_where_2023,
	title = {Where the ‘{Wood}-{Wide} {Web}’ {Narrative} {Went} {Wrong}},
	url = {https://undark.org/2023/05/25/where-the-wood-wide-web-narrative-went-wrong/},
	abstract = {Opinion {\textbackslash}textbar A fascinating story about forest fungal networks has captured the public imagination in recent years. Is any of it true?},
	language = {en-US},
	urldate = {2023-09-05},
	author = {Magazine, Undark},
	month = may,
	year = {2023},
	note = {Publication Title: Undark Magazine},
}

@article{karst_positive_2023,
	title = {Positive citation bias and overinterpreted results lead to misinformation on common mycorrhizal networks in forests},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-01986-1},
	doi = {10.1038/s41559-023-01986-1},
	abstract = {A common mycorrhizal network (CMN) is formed when mycorrhizal fungal hyphae connect the roots of multiple plants of the same or different species belowground. Recently, CMNs have captured the interest of broad audiences, especially with respect to forest function and management. We are concerned, however, that recent claims in the popular media about CMNs in forests are disconnected from evidence, and that bias towards citing positive effects of CMNs has developed in the scientific literature. We first evaluated the evidence supporting three common claims. The claims that CMNs are widespread in forests and that resources are transferred through CMNs to increase seedling performance are insufficiently supported because results from field studies vary too widely, have alternative explanations or are too limited to support generalizations. The claim that mature trees preferentially send resources and defence signals to offspring through CMNs has no peer-reviewed, published evidence. We next examined how the results from CMN research are cited and found that unsupported claims have doubled in the past 25 years; a bias towards citing positive effects may obscure our understanding of the structure and function of CMNs in forests. We conclude that knowledge on CMNs is presently too sparse and unsettled to inform forest management.},
	language = {en},
	urldate = {2023-02-13},
	journal = {Nature Ecology \& Evolution},
	author = {Karst, Justine and Jones, Melanie D. and Hoeksema, Jason D.},
	month = feb,
	year = {2023},
	keywords = {Forest ecology, Forestry},
	pages = {1--11},
	annote = {Publisher: Nature Publishing Group},
	annote = {Publisher: Nature Publishing Group},
}

@article{roche_troubleshooting_2014,
	title = {Troubleshooting {Public} {Data} {Archiving}: {Suggestions} to {Increase} {Participation}},
	volume = {12},
	issn = {1545-7885},
	shorttitle = {Troubleshooting {Public} {Data} {Archiving}},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001779},
	doi = {10.1371/journal.pbio.1001779},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {PLoS Biology},
	author = {Roche, Dominique G. and Lanfear, Robert and Binning, Sandra A. and Haff, Tonya M. and Schwanz, Lisa E. and Cain, Kristal E. and Kokko, Hanna and Jennions, Michael D. and Kruuk, Loeske E. B.},
	editor = {Eisen, Jonathan A.},
	month = jan,
	year = {2014},
	pages = {e1001779},
}

@article{mejlgaard_research_2020,
	title = {Research integrity: nine ways to move from talk to walk},
	volume = {586},
	copyright = {2020 Nature},
	shorttitle = {Research integrity},
	url = {https://www.nature.com/articles/d41586-020-02847-8},
	doi = {10.1038/d41586-020-02847-8},
	abstract = {Counselling, coaches and collegiality — how institutions can share resources to promote best practice in science.},
	language = {en},
	number = {7829},
	urldate = {2020-10-27},
	journal = {Nature},
	author = {Mejlgaard, Niels and Bouter, Lex M. and Gaskell, George and Kavouras, Panagiotis and Allum, Nick and Bendtsen, Anna-Kathrine and Charitidis, Costas A. and Claesen, Nik and Dierickx, Kris and Domaradzka, Anna and Elizondo, Andrea Reyes and Foeger, Nicole and Hiney, Maura and Kaltenbrunner, Wolfgang and Labib, Krishma and Marušić, Ana and Sørensen, Mads P. and Ravn, Tine and Ščepanović, Rea and Tijdink, Joeri K. and Veltri, Giuseppe A.},
	month = oct,
	year = {2020},
	pages = {358--360},
	annote = {Number: 7829 Publisher: Nature Publishing Group},
	annote = {Number: 7829 Publisher: Nature Publishing Group},
}

@article{gelman_garden_nodate,
	title = {The garden of forking paths: {Why} multiple comparisons can be a problem, even when there is no “ﬁshing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
	abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of ﬁshing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-speciﬁed and, as a result, were contingent on data.},
	language = {en},
	author = {Gelman, Andrew and Loken, Eric},
}

@article{azevedo_towards_2022,
	title = {Towards a culture of open scholarship: the role of pedagogical communities},
	volume = {15},
	issn = {1756-0500},
	shorttitle = {Towards a culture of open scholarship},
	url = {https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-05944-1},
	doi = {10.1186/s13104-022-05944-1},
	abstract = {Abstract The UK House of Commons Science and Technology Committee has called for evidence on the roles that different stakeholders play in reproducibility and research integrity. Of central priority are proposals for improving research integrity and quality, as well as guidance and support for researchers. In response to this, we argue that there is one important component of research integrity that is often absent from discussion: the pedagogical consequences of how we teach, mentor, and supervise students through open scholarship. We justify the need to integrate open scholarship principles into research training within higher education and argue that pedagogical communities play a key role in fostering an inclusive culture of open scholarship. We illustrate these benefits by presenting the Framework for Open and Reproducible Research Training (FORRT) , an international grassroots community whose goal is to provide support, resources, visibility, and advocacy for the adoption of principled, open teaching and mentoring practices, whilst generating conversations about the ethics and social impact of higher-education pedagogy. Representing a diverse group of early-career researchers and students across specialisms, we advocate for greater recognition of and support for pedagogical communities, and encourage all research stakeholders to engage with these communities to enable long-term, sustainable change.},
	language = {en},
	number = {1},
	urldate = {2022-04-28},
	journal = {BMC Research Notes},
	author = {Azevedo, Flávio and Liu, Meng and Pennington, Charlotte R. and Pownall, Madeleine and Evans, Thomas Rhys and Parsons, Sam and Elsherif, Mahmoud Medhat and Micheli, Leticia and Westwood, Samuel J. and {Framework for Open, Reproducible Research Training (FORRT)}},
	month = dec,
	year = {2022},
	pages = {75},
}

@article{davis_writing_2023,
	title = {Writing statistical methods for ecologists},
	volume = {14},
	issn = {2150-8925, 2150-8925},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.4539},
	doi = {10.1002/ecs2.4539},
	language = {en},
	number = {5},
	urldate = {2023-05-26},
	journal = {Ecosphere},
	author = {Davis, Amy J. and Kay, Shannon},
	month = may,
	year = {2023},
	pages = {e4539},
}

@article{soeharjono_reported_2021,
	title = {Reported {Individual} {Costs} and {Benefits} of {Sharing} {Open} {Data} among {Canadian} {Academic} {Faculty} in {Ecology} and {Evolution}},
	volume = {71},
	issn = {0006-3568, 1525-3244},
	url = {https://academic.oup.com/bioscience/article/71/7/750/6225906},
	doi = {10.1093/biosci/biab024},
	abstract = {Abstract Open data facilitate reproducibility and accelerate scientific discovery but are hindered by perceptions that researchers bear costs and gain few benefits from publicly sharing their data, with limited empirical evidence to the contrary. We surveyed 140 faculty members working in ecology and evolution across Canada's top 20 ranked universities and found that more researchers report benefits (47.9\%) and neutral outcomes (43.6\%) than costs (21.4\%) from openly sharing data. The benefits were independent of career stage and gender, but men and early career researchers were more likely to report costs. We outline mechanisms proposed by the study participants to reduce the individual costs and increase the benefits of open data for faculty members.},
	language = {en},
	number = {7},
	urldate = {2023-02-10},
	journal = {BioScience},
	author = {Soeharjono, Sandrine and Roche, Dominique G},
	month = jul,
	year = {2021},
	pages = {750--756},
}

@article{jenkins_reproducibility_2023,
	title = {Reproducibility in ecology and evolution: {Minimum} standards for data and code},
	volume = {13},
	issn = {2045-7758, 2045-7758},
	shorttitle = {Reproducibility in ecology and evolution},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ece3.9961},
	doi = {10.1002/ece3.9961},
	language = {en},
	number = {5},
	urldate = {2023-05-12},
	journal = {Ecology and Evolution},
	author = {Jenkins, Gareth B. and Beckerman, Andrew P. and Bellard, Céline and Benítez‐López, Ana and Ellison, Aaron M. and Foote, Christopher G. and Hufton, Andrew L. and Lashley, Marcus A. and Lortie, Christopher J. and Ma, Zhaoxue and Moore, Allen J. and Narum, Shawn R. and Nilsson, Johan and O'Boyle, Bridget and Provete, Diogo B. and Razgour, Orly and Rieseberg, Loren and Riginos, Cynthia and Santini, Luca and Sibbett, Benjamin and Peres‐Neto, Pedro R.},
	month = may,
	year = {2023},
	pages = {e9961},
}

@article{yang_publication_2023,
	title = {Publication bias impacts on effect size, statistical power, and magnitude ({Type} {M}) and sign ({Type} {S}) errors in ecology and evolutionary biology},
	volume = {21},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-022-01485-y},
	doi = {10.1186/s12915-022-01485-y},
	abstract = {Collaborative efforts to directly replicate empirical studies in the medical and social sciences have revealed alarmingly low rates of replicability, a phenomenon dubbed the ‘replication crisis’. Poor replicability has spurred cultural changes targeted at improving reliability in these disciplines. Given the absence of equivalent replication projects in ecology and evolutionary biology, two inter-related indicators offer the opportunity to retrospectively assess replicability: publication bias and statistical power. This registered report assesses the prevalence and severity of small-study (i.e., smaller studies reporting larger effect sizes) and decline effects (i.e., effect sizes decreasing over time) across ecology and evolutionary biology using 87 meta-analyses comprising 4,250 primary studies and 17,638 effect sizes. Further, we estimate how publication bias might distort the estimation of effect sizes, statistical power, and errors in magnitude (Type M or exaggeration ratio) and sign (Type S). We show strong evidence for the pervasiveness of both small-study and decline effects in ecology and evolution. There was widespread prevalence of publication bias that resulted in meta-analytic means being over-estimated by (at least) 0.12 standard deviations. The prevalence of publication bias distorted confidence in meta-analytic results, with 66\% of initially statistically significant meta-analytic means becoming non-significant after correcting for publication bias. Ecological and evolutionary studies consistently had low statistical power (15\%) with a 4-fold exaggeration of effects on average (Type M error rates = 4.4). Notably, publication bias reduced power from 23\% to 15\% and increased type M error rates from 2.7 to 4.4 because it creates a non-random sample of effect size evidence. The sign errors of effect sizes (Type S error) increased from 5\% to 8\% because of publication bias. Our research provides clear evidence that many published ecological and evolutionary findings are inflated. Our results highlight the importance of designing high-power empirical studies (e.g., via collaborative team science), promoting and encouraging replication studies, testing and correcting for publication bias in meta-analyses, and adopting open and transparent research practices, such as (pre)registration, data- and code-sharing, and transparent reporting.},
	number = {1},
	urldate = {2023-09-03},
	journal = {BMC Biology},
	author = {Yang, Yefeng and Sánchez-Tójar, Alfredo and O’Dea, Rose E. and Noble, Daniel W. A. and Koricheva, Julia and Jennions, Michael D. and Parker, Timothy H. and Lagisz, Malgorzata and Nakagawa, Shinichi},
	month = apr,
	year = {2023},
	keywords = {Open science, Generalizability, Many labs, Meta-research, P-hacking, Questionable research practices, Registered report, Replicability, Reproducibility, Selective reporting, Transparency},
	pages = {71},
}

@article{gomes_why_2022,
	title = {Why don't we share data and code? {Perceived} barriers and benefits to public archiving practices},
	volume = {289},
	shorttitle = {Why don't we share data and code?},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2022.1113},
	doi = {10.1098/rspb.2022.1113},
	abstract = {The biological sciences community is increasingly recognizing the value of open, reproducible and transparent research practices for science and society at large. Despite this recognition, many researchers fail to share their data and code publicly. This pattern may arise from knowledge barriers about how to archive data and code, concerns about its reuse, and misaligned career incentives. Here, we define, categorize and discuss barriers to data and code sharing that are relevant to many research fields. We explore how real and perceived barriers might be overcome or reframed in the light of the benefits relative to costs. By elucidating these barriers and the contexts in which they arise, we can take steps to mitigate them and align our actions with the goals of open science, both as individual scientists and as a scientific community.},
	number = {1987},
	urldate = {2023-08-31},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Gomes, Dylan G. E. and Pottier, Patrice and Crystal-Ornelas, Robert and Hudgins, Emma J. and Foroughirad, Vivienne and Sánchez-Reyes, Luna L. and Turba, Rachel and Martinez, Paula Andrea and Moreau, David and Bertram, Michael G. and Smout, Cooper A. and Gaynor, Kaitlyn M.},
	month = nov,
	year = {2022},
	keywords = {code reuse‌, data reuse, data science, open science, reproducibility, transparency},
	pages = {20221113},
	annote = {Publisher: Royal Society},
	annote = {Publisher: Royal Society},
}

@article{simmons_preregistration_2021,
	title = {Pre‐registration: {Why} and {How}},
	volume = {31},
	issn = {1057-7408, 1532-7663},
	shorttitle = {Pre‐registration},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jcpy.1208},
	doi = {10.1002/jcpy.1208},
	language = {en},
	number = {1},
	urldate = {2023-07-31},
	journal = {Journal of Consumer Psychology},
	author = {Simmons, Joseph and Nelson, Leif and Simonsohn, Uri},
	month = jan,
	year = {2021},
	pages = {151--162},
}

@article{carroll_operationalizing_2021,
	title = {Operationalizing the {CARE} and {FAIR} {Principles} for {Indigenous} data futures},
	volume = {8},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-021-00892-0},
	doi = {10.1038/s41597-021-00892-0},
	language = {en},
	number = {1},
	urldate = {2022-01-28},
	journal = {Scientific Data},
	author = {Carroll, Stephanie Russo and Herczog, Edit and Hudson, Maui and Russell, Keith and Stall, Shelley},
	month = dec,
	year = {2021},
	pages = {108},
}

@article{smaldino_natural_2016,
	title = {The natural selection of bad science},
	volume = {3},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.160384},
	doi = {10.1098/rsos.160384},
	abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
	language = {en},
	number = {9},
	urldate = {2022-09-05},
	journal = {Royal Society Open Science},
	author = {Smaldino, Paul E. and McElreath, Richard},
	month = sep,
	year = {2016},
	pages = {160384},
}

@article{nosek_preregistration_2019,
	title = {Preregistration {Is} {Hard}, {And} {Worthwhile}},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319301846},
	doi = {10.1016/j.tics.2019.07.009},
	language = {en},
	number = {10},
	urldate = {2022-06-20},
	journal = {Trends in Cognitive Sciences},
	author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and van ’t Veer, Anna E. and Vazire, Simine},
	month = oct,
	year = {2019},
	pages = {815--818},
}

@article{oboyle_chrysalis_2017,
	title = {The {Chrysalis} {Effect}: {How} {Ugly} {Initial} {Results} {Metamorphosize} {Into} {Beautiful} {Articles}},
	volume = {43},
	issn = {0149-2063, 1557-1211},
	shorttitle = {The {Chrysalis} {Effect}},
	url = {http://journals.sagepub.com/doi/10.1177/0149206314527133},
	doi = {10.1177/0149206314527133},
	abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the “Chrysalis Effect.”},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {Journal of Management},
	author = {O’Boyle, Ernest Hugh and Banks, George Christopher and Gonzalez-Mulé, Erik},
	month = feb,
	year = {2017},
	pages = {376--399},
}

@article{thibault_rigour_2022,
	title = {Rigour and reproducibility in {Canadian} research: call for a coordinated approach},
	volume = {7},
	issn = {2371-1671},
	shorttitle = {Rigour and reproducibility in {Canadian} research},
	url = {https://facetsjournal.com/doi/10.1139/facets-2021-0162},
	doi = {10.1139/facets-2021-0162},
	abstract = {Shortcomings in the rigour and reproducibility of research have become well-known issues and persist despite repeated calls for improvement. A coordinated effort among researchers, institutions, funders, publishers, learned societies, and regulators may be the most effective way of tackling these issues. The UK Reproducibility Network (UKRN) has fostered collaboration across various stakeholders in research and are creating the infrastructure necessary to advance rigorous and reproducible research practices across the United Kingdom. Other Reproducibility Networks, modelled on UKRN, are now emerging in other countries. Canada could benefit from a comparable network to unify the voices around research quality and maximize the value of Canadian research.},
	language = {en},
	urldate = {2022-08-23},
	journal = {FACETS},
	author = {Thibault, Robert T. and Munafò, Marcus R. and Moher, David},
	editor = {Taylor, Iain E.P.},
	month = jan,
	year = {2022},
	pages = {18--24},
}

@article{button_power_2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Power failure},
	url = {http://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
	language = {en},
	number = {5},
	urldate = {2020-03-17},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	pages = {365--376},
}

@article{clark_reply_2020,
	title = {Reply to: {Methods} matter in repeating ocean acidification studies},
	volume = {586},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Reply to},
	url = {http://www.nature.com/articles/s41586-020-2804-9},
	doi = {10.1038/s41586-020-2804-9},
	language = {en},
	number = {7830},
	urldate = {2021-09-02},
	journal = {Nature},
	author = {Clark, Timothy D. and Raby, Graham D. and Roche, Dominique G. and Binning, Sandra A. and Speers-Roesch, Ben and Jutfelt, Fredrik and Sundin, Josefin},
	month = oct,
	year = {2020},
	pages = {E25--E27},
}

@article{odea_towards_2021,
	title = {Towards open, reliable, and transparent ecology and evolutionary biology},
	volume = {19},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-021-01006-3},
	doi = {10.1186/s12915-021-01006-3},
	abstract = {Unreliable research programmes waste funds, time, and even the lives of the organisms we seek to help and understand. Reducing this waste and increasing the value of scientific evidence require changing the actions of both individual researchers and the institutions they depend on for employment and promotion. While ecologists and evolutionary biologists have somewhat improved research transparency over the past decade (e.g. more data sharing), major obstacles remain. In this commentary, we lift our gaze to the horizon to imagine how researchers and institutions can clear the path towards more credible and effective research programmes.},
	number = {1},
	urldate = {2021-04-09},
	journal = {BMC Biology},
	author = {O’Dea, Rose E. and Parker, Timothy H. and Chee, Yung En and Culina, Antica and Drobniak, Szymon M. and Duncan, David H. and Fidler, Fiona and Gould, Elliot and Ihle, Malika and Kelly, Clint D. and Lagisz, Malgorzata and Roche, Dominique G. and Sánchez-Tójar, Alfredo and Wilkinson, David P. and Wintle, Bonnie C. and Nakagawa, Shinichi},
	month = apr,
	year = {2021},
	pages = {68},
}

@article{desjardins-proulx_case_2013,
	title = {The {Case} for {Open} {Preprints} in {Biology}},
	volume = {11},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001563},
	doi = {10.1371/journal.pbio.1001563},
	language = {en},
	number = {5},
	urldate = {2020-10-12},
	journal = {PLoS Biology},
	author = {Desjardins-Proulx, Philippe and White, Ethan P. and Adamson, Joel J. and Ram, Karthik and Poisot, Timothée and Gravel, Dominique},
	month = may,
	year = {2013},
	pages = {e1001563},
}

@article{hampton_tao_2015,
	title = {The {Tao} of open science for ecology},
	volume = {6},
	issn = {2150-8925},
	url = {http://doi.wiley.com/10.1890/ES14-00402.1},
	doi = {10.1890/ES14-00402.1},
	language = {en},
	number = {7},
	urldate = {2020-10-12},
	journal = {Ecosphere},
	author = {Hampton, Stephanie E. and Anderson, Sean S. and Bagby, Sarah C. and Gries, Corinna and Han, Xueying and Hart, Edmund M. and Jones, Matthew B. and Lenhardt, W. Christopher and MacDonald, Andrew and Michener, William K. and Mudge, Joe and Pourmokhtarian, Afshin and Schildhauer, Mark P. and Woo, Kara H. and Zimmerman, Naupaka},
	month = jul,
	year = {2015},
	pages = {art120},
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab2374},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	urldate = {2020-10-07},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pages = {1422--1425},
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	language = {en},
	number = {11},
	urldate = {2020-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	pages = {2600--2606},
}

@article{fanelli_positive_2010,
	title = {“{Positive}” {Results} {Increase} {Down} the {Hierarchy} of the {Sciences}},
	volume = {5},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010068},
	doi = {10.1371/journal.pone.0010068},
	abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
	language = {en},
	number = {4},
	urldate = {2020-10-06},
	journal = {PLOS ONE},
	author = {Fanelli, Daniele},
	month = apr,
	year = {2010},
	keywords = {Forecasting, Mental health and psychiatry, Physical sciences, Scientists, Social psychology, Social research, Social sciences, Sociology},
	pages = {e10068},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{parker_transparency_2016,
	title = {Transparency in {Ecology} and {Evolution}: {Real} {Problems}, {Real} {Solutions}},
	volume = {31},
	issn = {0169-5347},
	shorttitle = {Transparency in {Ecology} and {Evolution}},
	url = {https://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(16)30095-7},
	doi = {10.1016/j.tree.2016.07.002},
	abstract = {Evidence suggests that insufficient transparency is a problem across much of ecology and evolution. Results and methods are often reported in insufficient detail or go entirely unreported. Further, these unreported results are often a biased subset, thus substantially hampering interpretation and meta-analysis. Journals and other institutions, such as funding agencies, influence researchers’ decisions about disseminating results. There is a movement across empirical disciplines, including ecology and evolution, to shape institutional policies to better promote transparency. Institutions can promote transparency by requiring or encouraging more disclosure, as with the now-familiar data archiving, or by developing an incentive structure promoting disclosure, such as preregistration of studies and analysis plans. To make progress scientists need to know what other researchers have found and how they found it. However, transparency is often insufficient across much of ecology and evolution. Researchers often fail to report results and methods in detail sufficient to permit interpretation and meta-analysis, and many results go entirely unreported. Further, these unreported results are often a biased subset. Thus the conclusions we can draw from the published literature are themselves often biased and sometimes might be entirely incorrect. Fortunately there is a movement across empirical disciplines, and now within ecology and evolution, to shape editorial policies to better promote transparency. This can be done by either requiring more disclosure by scientists or by developing incentives to encourage disclosure.},
	language = {English},
	number = {9},
	urldate = {2020-09-19},
	journal = {Trends in Ecology \& Evolution},
	author = {Parker, Timothy H. and Forstmeier, Wolfgang and Koricheva, Julia and Fidler, Fiona and Hadfield, Jarrod D. and Chee, Yung En and Kelly, Clint D. and Gurevitch, Jessica and Nakagawa, Shinichi},
	month = sep,
	year = {2016},
	pmid = {27461041},
	keywords = {P-hacking, confirmation bias, inflated effect size, preregistration, replication, selective reporting},
	pages = {711--719},
	annote = {Publisher: Elsevier},
	annote = {Publisher: Elsevier},
}

@article{roche_public_2015,
	title = {Public {Data} {Archiving} in {Ecology} and {Evolution}: {How} {Well} {Are} {We} {Doing}?},
	volume = {13},
	issn = {1545-7885},
	shorttitle = {Public {Data} {Archiving} in {Ecology} and {Evolution}},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002295},
	doi = {10.1371/journal.pbio.1002295},
	abstract = {Policies that mandate public data archiving (PDA) successfully increase accessibility to data underlying scientific publications. However, is the data quality sufficient to allow reuse and reanalysis? We surveyed 100 datasets associated with nonmolecular studies in journals that commonly publish ecological and evolutionary research and have a strong PDA policy. Out of these datasets, 56\% were incomplete, and 64\% were archived in a way that partially or entirely prevented reuse. We suggest that cultural shifts facilitating clearer benefits to authors are necessary to achieve high-quality PDA and highlight key guidelines to help authors increase their data’s reuse potential and compliance with journal data policies.},
	language = {en},
	number = {11},
	urldate = {2020-09-19},
	journal = {PLOS Biology},
	author = {Roche, Dominique G. and Kruuk, Loeske E. B. and Lanfear, Robert and Binning, Sandra A.},
	month = nov,
	year = {2015},
	keywords = {Reproducibility, Archives, Computer software, Evolutionary biology, Metadata, Public policy, Science policy, Scientific publishing},
	pages = {e1002295},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{powers_open_2019,
	title = {Open science, reproducibility, and transparency in ecology},
	volume = {29},
	copyright = {© 2018 The Authors Ecological Applications published by Wiley Periodicals, Inc. on behalf of Ecological Society of America},
	issn = {1939-5582},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.1822},
	doi = {10.1002/eap.1822},
	abstract = {Reproducibility is a key tenet of the scientific process that dictates the reliability and generality of results and methods. The complexities of ecological observations and data present novel challenges in satisfying needs for reproducibility and also transparency. Ecological systems are dynamic and heterogeneous, interacting with numerous factors that sculpt natural history and that investigators cannot completely control. Observations may be highly dependent on spatial and temporal context, making them very difficult to reproduce, but computational reproducibility can still be achieved. Computational reproducibility often refers to the ability to produce equivalent analytical outcomes from the same data set using the same code and software as the original study. When coded workflows are shared, authors and editors provide transparency for readers and allow other researchers to build directly and efficiently on primary work. These qualities may be especially important in ecological applications that have important or controversial implications for science, management, and policy. Expectations for computational reproducibility and transparency are shifting rapidly in the sciences. In this work, we highlight many of the unique challenges for ecology along with practical guidelines for reproducibility and transparency, as ecologists continue to participate in the stewardship of critical environmental information and ensure that research methods demonstrate integrity.},
	language = {en},
	number = {1},
	urldate = {2020-09-21},
	journal = {Ecological Applications},
	author = {Powers, Stephen M. and Hampton, Stephanie E.},
	year = {2019},
	keywords = {replicability, data science, open science, collaborative tools, data policy, ecoinformatics, ecosystem, environmental science, repeatability, reproducible, transparent, workflows},
	pages = {e01822},
	annote = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822},
	annote = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822},
}

@techreport{marwick_packaging_2018,
	type = {preprint},
	title = {Packaging data analytical work reproducibly using {R} (and friends)},
	url = {https://peerj.com/preprints/3192v2},
	abstract = {Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
	language = {en},
	urldate = {2020-09-24},
	institution = {PeerJ Preprints},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	month = mar,
	year = {2018},
	doi = {10.7287/peerj.preprints.3192v2},
}

@article{kelly_rate_2019,
	title = {Rate and success of study replication in ecology and evolution},
	volume = {7},
	issn = {2167-8359},
	url = {https://peerj.com/articles/7654},
	doi = {10.7717/peerj.7654},
	abstract = {The recent replication crisis has caused several scientific disciplines to self-reflect on the frequency with which they replicate previously published studies and to assess their success in such endeavours. The rate of replication, however, has yet to be assessed for ecology and evolution. Here, I survey the open-access ecology and evolution literature to determine how often ecologists and evolutionary biologists replicate, or at least claim to replicate, previously published studies. I found that approximately 0.023\% of ecology and evolution studies are described by their authors as replications. Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of “replication failure” by the authors.},
	language = {en},
	urldate = {2020-09-19},
	journal = {PeerJ},
	author = {Kelly, Clint D.},
	month = sep,
	year = {2019},
	pages = {e7654},
	annote = {Publisher: PeerJ Inc.},
	annote = {Publisher: PeerJ Inc.},
}

@article{jamieson_signaling_2020,
	title = {Signaling the trustworthiness of science},
	volume = {117},
	number = {14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jamieson, Kathleen Hall and McNutt, Marcia and Kiermer, Veronique and Sever, Richard},
	year = {2020},
	pages = {8212--8212},
	annote = {Publisher: NATL ACAD SCIENCES 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA},
	annote = {Publisher: NATL ACAD SCIENCES 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2020-09-29},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	keywords = {Cancer risk factors, Finance, Genetic epidemiology, Genetics of disease, Metaanalysis, Randomized controlled trials, Research design, Schizophrenia},
	pages = {e124},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{fraser_questionable_2018,
	title = {Questionable research practices in ecology and evolution},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
	doi = {10.1371/journal.pone.0200303},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	language = {en},
	number = {7},
	urldate = {2020-09-19},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	month = jul,
	year = {2018},
	keywords = {Evolutionary biology, Behavioral ecology, Community ecology, Evolutionary ecology, Evolutionary rate, Psychology, Publication ethics, Statistical data},
	pages = {e0200303},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{fraser_role_2020,
	title = {The role of replication studies in ecology},
	volume = {10},
	copyright = {© 2020 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
	issn = {2045-7758},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.6330},
	doi = {10.1002/ece3.6330},
	abstract = {Recent large-scale projects in other disciplines have shown that results often fail to replicate when studies are repeated. The conditions contributing to this problem are also present in ecology, but there have not been any equivalent replication projects. Here, we survey ecologists' understanding of and opinions about replication studies. The majority of ecologists in our sample considered replication studies to be important (97\%), not prevalent enough (91\%), worth funding even given limited resources (61\%), and suitable for publication in all journals (62\%). However, there is a disconnect between this enthusiasm and the prevalence of direct replication studies in the literature which is much lower (0.023\%: Kelly 2019) than our participants' median estimate of 10\%. This may be explained by the obstacles our participants identified including the difficulty of conducting replication studies and of funding and publishing them. We conclude by offering suggestions for how replications could be better integrated into ecological research.},
	language = {en},
	number = {12},
	urldate = {2020-09-19},
	journal = {Ecology and Evolution},
	author = {Fraser, Hannah and Barnett, Ashley and Parker, Timothy H. and Fidler, Fiona},
	year = {2020},
	keywords = {replicability, open science, reproducibility, transparency, repeatability, conceptual replication, direct replication, generalizability},
	pages = {5197--5207},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.6330},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.6330},
}

@article{bafeta_ten_2020,
	title = {Ten simple rules for open human health research},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007846},
	doi = {10.1371/journal.pcbi.1007846},
	language = {en},
	number = {9},
	urldate = {2020-09-27},
	journal = {PLOS Computational Biology},
	author = {Bafeta, Aïda and Bobe, Jason and Clucas, Jon and Gonsalves, Pattie Pramila and Gruson-Daniel, Célya and Hudson, Kathy L. and Klein, Arno and Krishnakumar, Anirudh and McCollister-Slipp, Anna and Lindner, Ariel B. and Misevic, Dusan and Naslund, John A. and Nebeker, Camille and Nikolaidis, Aki and Pasquetto, Irene and Sanchez, Gabriela and Schapira, Matthieu and Scheininger, Tohar and Schoeller, Félix and Heinsfeld, Anibal Sólon and Taddei, François},
	month = sep,
	year = {2020},
	keywords = {Open science, Reproducibility, Scientists, Computer software, Research design, Medical risk factors, Research ethics, Social media},
	pages = {e1007846},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{culumber_widespread_2019,
	title = {Widespread {Biases} in {Ecological} and {Evolutionary} {Studies}},
	volume = {69},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/69/8/631/5523265},
	doi = {10.1093/biosci/biz063},
	abstract = {There has been widespread discussion of biases in the sciences. The extent of most forms of bias has scarcely been confronted with rigorous data. In the present article, we evaluated the potential for geographic, taxonomic, and citation biases in publications between temperate and tropical systems for nine broad topics in ecology and evolutionary biology. Across 1,800 papers sampled from 60,000 peer-reviewed, empirical studies, we found consistent patterns of bias in the form of increased numbers of studies in temperate systems. Tropical studies were nearly absent from some topics. Furthermore, there were strong taxonomic biases across topics and geographic regions, as well as evidence for citation biases in many topics. Our results indicate a strong geographic imbalance in publishing patterns and among different taxonomic groups across a wide range of topics. The task ahead is to address what these biases mean and how they influence the state of our knowledge in ecology and evolution.},
	language = {en},
	number = {8},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Culumber, Zachary W. and Anaya-Rojas, Jaime M. and Booker, William W. and Hooks, Alexandra P. and Lange, Elizabeth C. and Pluer, Benjamin and Ramírez-Bullón, Natali and Travis, Joseph},
	month = aug,
	year = {2019},
	pages = {631--640},
	annote = {Publisher: Oxford Academic},
	annote = {Publisher: Oxford Academic},
}

@misc{noauthor_replication_nodate,
	title = {Replication {Failures} {Highlight} {Biases} in {Ecology} and {Evolution} {Science}},
	url = {https://www.the-scientist.com/features/replication-failures-highlight-biases-in-ecology-and-evolution-science-64475},
	abstract = {As robust efforts fail to reproduce findings of influential zebra finch studies from the 1980s, scientists discuss ways to reduce bias in such research.},
	language = {en},
	urldate = {2020-09-30},
	note = {Publication Title: The Scientist Magazine®},
}

@article{yang_large-scale_2024,
	title = {A large-scale in silico replication of ecological and evolutionary studies},
	volume = {8},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-024-02530-5},
	doi = {10.1038/s41559-024-02530-5},
	abstract = {Abstract Despite the growing concerns about the replicability of ecological and evolutionary studies, no results exist from a field-wide replication project. We conduct a large-scale in silico replication project, leveraging cutting-edge statistical methodologies. Replicability is 30\%–40\% for studies with marginal statistical significance in the absence of selective reporting, whereas the replicability of studies presenting ‘strong’ evidence against the null hypothesis H 0 is {\textbackslash}textgreater70\%. The former requires a sevenfold larger sample size to reach the latter’s replicability. We call for a change in planning, conducting and publishing research towards a transparent, credible and replicable ecology and evolution.},
	language = {en},
	number = {12},
	urldate = {2025-02-11},
	journal = {Nature Ecology \& Evolution},
	author = {Yang, Yefeng and Van Zwet, Erik and Ignatiadis, Nikolaos and Nakagawa, Shinichi},
	month = sep,
	year = {2024},
	pages = {2179--2183},
}

@article{kimmel_empirical_2023,
	title = {Empirical evidence of widespread exaggeration bias and selective reporting in ecology},
	volume = {7},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-02144-3},
	doi = {10.1038/s41559-023-02144-3},
	language = {en},
	number = {9},
	urldate = {2025-04-06},
	journal = {Nature Ecology \& Evolution},
	author = {Kimmel, Kaitlin and Avolio, Meghan L. and Ferraro, Paul J.},
	month = aug,
	year = {2023},
	pages = {1525--1536},
}

@article{buxton_avoiding_2021,
	title = {Avoiding wasted research resources in conservation science},
	volume = {3},
	issn = {2578-4854, 2578-4854},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/csp2.329},
	doi = {10.1111/csp2.329},
	language = {en},
	number = {2},
	urldate = {2022-01-17},
	journal = {Conservation Science and Practice},
	author = {Buxton, Rachel T. and Nyboer, Elizabeth A. and Pigeon, Karine E. and Raby, Graham D. and Rytwinski, Trina and Gallagher, Austin J. and Schuster, Richard and Lin, Hsien‐Yung and Fahrig, Lenore and Bennett, Joseph R. and Cooke, Steven J. and Roche, Dominique G.},
	month = feb,
	year = {2021},
}

@article{nakagawa_finding_2024,
	title = {Finding the right power balance: {Better} study design and collaboration can reduce dependence on statistical power},
	volume = {22},
	issn = {1545-7885},
	shorttitle = {Finding the right power balance},
	url = {https://dx.plos.org/10.1371/journal.pbio.3002423},
	doi = {10.1371/journal.pbio.3002423},
	abstract = {Power analysis currently dominates sample size determination for experiments, particularly in grant and ethics applications. Yet, this focus could paradoxically result in suboptimal study design because publication biases towards studies with the largest effects can lead to the overestimation of effect sizes. In this Essay, we propose a paradigm shift towards better study designs that focus less on statistical power. We also advocate for (pre)registration and obligatory reporting of all results (regardless of statistical significance), better facilitation of team science and multi-institutional collaboration that incorporates heterogenization, and the use of prospective and living meta-analyses to generate generalizable results. Such changes could make science more effective and, potentially, more equitable, helping to cultivate better collaborations.},
	language = {en},
	number = {1},
	urldate = {2025-08-28},
	journal = {PLOS Biology},
	author = {Nakagawa, Shinichi and Lagisz, Malgorzata and Yang, Yefeng and Drobniak, Szymon M.},
	month = jan,
	year = {2024},
	pages = {e3002423},
}

@misc{noauthor_data_nodate,
	title = {Data management checklist},
	url = {https://www.fairdata.fi/en/data-management-checklist/},
	abstract = {In order to make research more transparent, verifiable, replicable or reproducible, one should be able to provide the data supporting the findings. Many funding bodies nowadays require that a research…},
	language = {en-GB},
	urldate = {2023-09-21},
	note = {Publication Title: Fairdata},
}

@article{sholler_enforcing_2019,
	title = {Enforcing public data archiving policies in academic publishing: {A} study of ecology journals},
	volume = {6},
	issn = {2053-9517, 2053-9517},
	shorttitle = {Enforcing public data archiving policies in academic publishing},
	url = {http://journals.sagepub.com/doi/10.1177/2053951719836258},
	doi = {10.1177/2053951719836258},
	abstract = {To improve the quality and efficiency of research, groups within the scientific community seek to exploit the value of data sharing. Funders, institutions, and specialist organizations are developing and implementing strategies to encourage or mandate data sharing within and across disciplines, with varying degrees of success. Academic journals in ecology and evolution have adopted several types of public data archiving policies requiring authors to make data underlying scholarly manuscripts freely available. The effort to increase data sharing in the sciences is one part of a broader “data revolution” that has prompted discussion about a paradigm shift in scientific research. Yet anecdotes from the community and studies evaluating data availability suggest that these policies have not obtained the desired effects, both in terms of quantity and quality of available datasets. We conducted a qualitative, interview-based study with journal editorial staff and other stakeholders in the academic publishing process to examine how journals enforce data archiving policies. We specifically sought to establish who editors and other stakeholders perceive as responsible for ensuring data completeness and quality in the peer review process. Our analysis revealed little consensus with regard to how data archiving policies should be enforced and who should hold authors accountable for dataset submissions. Themes in interviewee responses included hopefulness that reviewers would take the initiative to review datasets and trust in authors to ensure the completeness and quality of their datasets. We highlight problematic aspects of these thematic responses and offer potential starting points for improvement of the public data archiving process.},
	language = {en},
	number = {1},
	urldate = {2022-01-17},
	journal = {Big Data \& Society},
	author = {Sholler, Dan and Ram, Karthik and Boettiger, Carl and Katz, Daniel S},
	month = jan,
	year = {2019},
	pages = {205395171983625},
}

@article{ettinger_guide_2022,
	title = {A guide to preprinting for early-career researchers},
	volume = {11},
	issn = {2046-6390},
	url = {https://journals.biologists.com/bio/article/11/7/bio059310/276073/A-guide-to-preprinting-for-early-career},
	doi = {10.1242/bio.059310},
	abstract = {ABSTRACT The use of preprints, research manuscripts shared publicly before completing the traditional peer-review process, is becoming a more common practice among life science researchers. Early-career researchers (ECRs) benefit from posting preprints as they are shareable, citable, and prove productivity. However, preprinting a manuscript involves a discussion among all co-authors, and ECRs are often not the decision-makers. Therefore, ECRs may find themselves in situations where they are interested in depositing a preprint but are unsure how to approach their co-authors or advisor about preprinting. Leveraging our own experiences as ECRs, and feedback from the research community, we have constructed a guide for ECRs who are considering preprinting to enable them to take ownership over the process and to raise awareness about preprinting options. We hope that this guide helps ECRs to initiate conversations about preprinting with co-authors and encourage them to preprint their future research.},
	language = {en},
	number = {7},
	urldate = {2023-09-21},
	journal = {Biology Open},
	author = {Ettinger, Cassandra L. and Sadanandappa, Madhumala K. and Görgülü, Kıvanç and Coghlan, Karen L. and Hallenbeck, Kenneth K. and Puebla, Iratxe},
	month = jul,
	year = {2022},
	pages = {bio059310},
}

@article{blastland_five_2020,
	title = {Five rules for evidence communication},
	volume = {587},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/d41586-020-03189-1},
	doi = {10.1038/d41586-020-03189-1},
	language = {en},
	number = {7834},
	urldate = {2023-09-21},
	journal = {Nature},
	author = {Blastland, Michael and Freeman, Alexandra L. J. and Van Der Linden, Sander and Marteau, Theresa M. and Spiegelhalter, David},
	month = nov,
	year = {2020},
	pages = {362--364},
}

@article{roche_open_2020,
	title = {Open government data and environmental science: a federal {Canadian} perspective},
	volume = {5},
	issn = {2371-1671},
	shorttitle = {Open government data and environmental science},
	url = {https://facetsjournal.com/doi/10.1139/facets-2020-0008},
	doi = {10.1139/facets-2020-0008},
	abstract = {Governments worldwide are releasing data into the public domain via open government data initiatives. Many such data sets are directly relevant to environmental science and complement data collected by academic researchers to address complex and challenging environmental problems. The Government of Canada is a leader in open data among Organisation for Economic Co-operation and Development countries, generating and releasing troves of valuable research data. However, achieving comprehensive and FAIR (findable, accessible, interoperable, reusable) open government data is not without its challenges. For example, identifying and understanding Canada’s international commitments, policies, and guidelines on open data can be daunting. Similarly, open data sets within the Government of Canada are spread across a diversity of repositories and portals, which may hinder their discoverability. We describe Canada’s federal initiatives promoting open government data, and outline where data sets of relevance to environmental science can be found. We summarize research data management challenges identified by the Government of Canada, plans to modernize the approach to open data for environmental science and best practices for data discoverability, access, and reuse.},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {FACETS},
	author = {Roche, Dominique G. and Granados, Monica and Austin, Claire C. and Wilson, Scott and Mitchell, Gregory M. and Smith, Paul A. and Cooke, Steven J. and Bennett, Joseph R.},
	editor = {Love, Tanzy},
	month = jan,
	year = {2020},
	pages = {942--962},
}

@article{khan_open_2022,
	title = {Open science failed to penetrate academic hiring practices: a cross-sectional study},
	volume = {144},
	issn = {08954356},
	shorttitle = {Open science failed to penetrate academic hiring practices},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435621004005},
	doi = {10.1016/j.jclinepi.2021.12.003},
	language = {en},
	urldate = {2023-02-17},
	journal = {Journal of Clinical Epidemiology},
	author = {Khan, Hassan and Almoli, Elham and Franco, Marina Christ and Moher, David},
	month = apr,
	year = {2022},
	pages = {136--143},
}

@article{kathawalla_easing_2021,
	title = {Easing {Into} {Open} {Science}: {A} {Guide} for {Graduate} {Students} and {Their} {Advisors}},
	volume = {7},
	issn = {2474-7394},
	shorttitle = {Easing {Into} {Open} {Science}},
	url = {https://online.ucpress.edu/collabra/article/doi/10.1525/collabra.18684/115927/Easing-Into-Open-Science-A-Guide-for-Graduate},
	doi = {10.1525/collabra.18684},
	abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {Collabra: Psychology},
	author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
	month = jan,
	year = {2021},
	pages = {18684},
}

@article{soeharjono_reported_2021-1,
	title = {Reported {Individual} {Costs} and {Benefits} of {Sharing} {Open} {Data} among {Canadian} {Academic} {Faculty} in {Ecology} and {Evolution}},
	volume = {71},
	issn = {0006-3568, 1525-3244},
	url = {https://academic.oup.com/bioscience/article/71/7/750/6225906},
	doi = {10.1093/biosci/biab024},
	abstract = {Abstract Open data facilitate reproducibility and accelerate scientific discovery but are hindered by perceptions that researchers bear costs and gain few benefits from publicly sharing their data, with limited empirical evidence to the contrary. We surveyed 140 faculty members working in ecology and evolution across Canada's top 20 ranked universities and found that more researchers report benefits (47.9\%) and neutral outcomes (43.6\%) than costs (21.4\%) from openly sharing data. The benefits were independent of career stage and gender, but men and early career researchers were more likely to report costs. We outline mechanisms proposed by the study participants to reduce the individual costs and increase the benefits of open data for faculty members.},
	language = {en},
	number = {7},
	urldate = {2023-02-10},
	journal = {BioScience},
	author = {Soeharjono, Sandrine and Roche, Dominique G},
	month = jul,
	year = {2021},
	pages = {750--756},
}

@article{sarafoglou_survey_2022,
	title = {A survey on how preregistration affects the research workflow: better science but more work},
	volume = {9},
	issn = {2054-5703},
	shorttitle = {A survey on how preregistration affects the research workflow},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211997},
	doi = {10.1098/rsos.211997},
	abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.},
	language = {en},
	number = {7},
	urldate = {2022-09-12},
	journal = {Royal Society Open Science},
	author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence and Wagenmakers, Eric-Jan and Aczel, Balazs},
	month = jul,
	year = {2022},
	pages = {211997},
}

@article{stromme_close_2022,
	title = {Close to open—{Factors} that hinder and promote open science in ecology research and education},
	volume = {17},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0278339},
	doi = {10.1371/journal.pone.0278339},
	abstract = {The Open Science (OS) movement is rapidly gaining traction among policy-makers, research funders, scientific journals and individual scientists. Despite these tendencies, the pace of implementing OS throughout the scientific process and across the scientific community remains slow. Thus, a better understanding of the conditions that affect OS engagement, and in particular, of how practitioners learn, use, conduct and share research openly can guide those seeking to implement OS more broadly. We surveyed participants at an OS workshop hosted by the Living Norway Ecological Data Network in 2020 to learn how they perceived OS and its importance in their research, supervision and teaching. Further, we wanted to know what OS practices they had encountered in their education and what they saw as hindering or helping their engagement with OS. The survey contained scaled-response and open-ended questions, allowing for a mixed-methods approach. We obtained survey responses from 60 out of 128 workshop participants (47\%). Responses indicated that usage and sharing of open data and code, as well as open access publication, were the most frequent OS practices. Only a minority of respondents reported having encountered OS in their formal education. A majority also viewed OS as less important in their teaching than in their research and supervisory roles. The respondents’ suggestions for what would facilitate greater OS engagement in the future included knowledge, guidelines, and resources, but also social and structural support. These are aspects that could be strengthened by promoting explicit implementation of OS practices in higher education and by nurturing a more inclusive and equitable OS culture. We argue that incorporating OS in teaching and learning of science can yield substantial benefits to the research community, student learning, and ultimately, to the wider societal objectives of science and higher education.},
	language = {en},
	number = {12},
	urldate = {2023-06-09},
	journal = {PLOS ONE},
	author = {Strømme, Christian B. and Lane, A. Kelly and Halbritter, Aud H. and Law, Elizabeth and Nater, Chloe R. and Nilsen, Erlend B. and Boutouli, Grace D. and Egelkraut, Dagmar D. and Telford, Richard J. and Vandvik, Vigdis and Cotner, Sehoya H.},
	editor = {Baccini, Alberto},
	month = dec,
	year = {2022},
	pages = {e0278339},
}

@article{corneille_beware_2023,
	title = {Beware ‘persuasive communication devices’ when writing and reading scientific articles},
	volume = {12},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/88654},
	doi = {10.7554/eLife.88654},
	abstract = {Authors rely on a range of devices and techniques to attract and maintain the interest of readers, and to convince them of the merits of the author’s point of view. However, when writing a scientific article, authors must use these ‘persuasive communication devices’ carefully. In particular, they must be explicit about the limitations of their work, avoid obfuscation, and resist the temptation to oversell their results. Here we discuss a list of persuasive communication devices and we encourage authors, as well as reviewers and editors, to think carefully about their use.},
	language = {en},
	urldate = {2023-05-29},
	journal = {eLife},
	author = {Corneille, Olivier and Havemann, Jo and Henderson, Emma L and IJzerman, Hans and Hussey, Ian and Orban De Xivry, Jean-Jacques and Jussim, Lee and Holmes, Nicholas P and Pilacinski, Artur and Beffara, Brice and Carroll, Harriet and Outa, Nicholas Otieno and Lush, Peter and Lotter, Leon D},
	month = may,
	year = {2023},
	pages = {e88654},
}

@article{bertram_open_2023,
	title = {Open science},
	volume = {33},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982223006681},
	doi = {10.1016/j.cub.2023.05.036},
	language = {en},
	number = {15},
	urldate = {2023-08-28},
	journal = {Current Biology},
	author = {Bertram, Michael G. and Sundin, Josefin and Roche, Dominique G. and Sánchez-Tójar, Alfredo and Thoré, Eli S.J. and Brodin, Tomas},
	month = aug,
	year = {2023},
	pages = {R792--R797},
}

@article{caldwell_moving_nodate,
	title = {Moving {Sport} and {Exercise} {Science} {Forward}: {A} {Call} for the {Adoption} of {More} {Transparent} {Research} {Practices}},
	abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR iv: https://osf.io/ preprints/sportrxiv/fxe7a/.},
	language = {en},
	author = {Caldwell, Aaron R},
	pages = {11},
}

@article{silberzahn_many_2018,
	title = {Many {Analysts}, {One} {Data} {Set}: {Making} {Transparent} {How} {Variations} in {Analytic} {Choices} {Affect} {Results}},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Many {Analysts}, {One} {Data} {Set}},
	url = {http://journals.sagepub.com/doi/10.1177/2515245917747646},
	doi = {10.1177/2515245917747646},
	abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 ( Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
	language = {en},
	number = {3},
	urldate = {2022-09-06},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
	month = sep,
	year = {2018},
	pages = {337--356},
}

@article{roche_closing_2021,
	title = {Closing the knowledge‐action gap in conservation with open science},
	issn = {0888-8892, 1523-1739},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cobi.13835},
	doi = {10.1111/cobi.13835},
	language = {en},
	urldate = {2021-12-01},
	journal = {Conservation Biology},
	author = {Roche, Dominique G. and O'Dea, Rose E. and Kerr, Kecia A. and Rytwinski, Trina and Schuster, Richard and Nguyen, Vivian M. and Young, Nathan and Bennett, Joseph R. and Cooke, Steven J.},
	month = nov,
	year = {2021},
	pages = {cobi.13835},
}

@article{filazzola_call_2022,
	title = {A call for clean code to effectively communicate science},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13961},
	doi = {10.1111/2041-210X.13961},
	language = {en},
	urldate = {2022-08-29},
	journal = {Methods in Ecology and Evolution},
	author = {Filazzola, Alessandro and Lortie, Cj},
	month = aug,
	year = {2022},
	pages = {2041--210X.13961},
}

@article{towse_lustre_2022,
	title = {{LUSTRE}: {An} online data management and student project resource},
	issn = {2693-9169},
	shorttitle = {{LUSTRE}},
	url = {https://www.tandfonline.com/doi/full/10.1080/26939169.2022.2118645},
	doi = {10.1080/26939169.2022.2118645},
	language = {en},
	urldate = {2022-09-01},
	journal = {Journal of Statistics and Data Science Education},
	author = {Towse, John and Davies, Rob and Ball, Ellie and James, Rebecca and Gooding, Ben and Ivory, Matthew},
	month = aug,
	year = {2022},
	pages = {1--14},
}

@article{williamson_ideas_2021,
	title = {Ideas and perspectives: {When} ocean acidification experiments are not the same, repeatability is not tested},
	volume = {18},
	issn = {1726-4189},
	shorttitle = {Ideas and perspectives},
	url = {https://bg.copernicus.org/articles/18/1787/2021/},
	doi = {10.5194/bg-18-1787-2021},
	abstract = {Abstract. Can experimental studies on the behavioural impacts of ocean acidification be trusted? That question was raised in early 2020 when a high-profile paper failed to corroborate previously observed responses of coral reef fish to high CO2. New information on the methodologies used in the “replicated” studies now provides a plausible explanation: the experimental conditions were substantially different. High sensitivity to test conditions is characteristic of ocean acidification research; such response variability shows that effects are complex, interacting with many other factors. Open-minded assessment of all research results, both negative and positive, remains the best way to develop process-based understanding. As in other fields, replication studies in ocean acidification are most likely to contribute to scientific advancement when carried out in a spirit of collaboration rather than confrontation.},
	language = {en},
	number = {5},
	urldate = {2021-09-13},
	journal = {Biogeosciences},
	author = {Williamson, Phillip and Pörtner, Hans-Otto and Widdicombe, Steve and Gattuso, Jean-Pierre},
	month = mar,
	year = {2021},
	pages = {1787--1792},
}

@article{besancon_open_2021,
	title = {Open science saves lives: lessons from the {COVID}-19 pandemic},
	volume = {21},
	issn = {1471-2288},
	shorttitle = {Open science saves lives},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01304-y},
	doi = {10.1186/s12874-021-01304-y},
	abstract = {Abstract In the last decade Open Science principles have been successfully advocated for and are being slowly adopted in different research communities. In response to the COVID-19 pandemic many publishers and researchers have sped up their adoption of Open Science practices, sometimes embracing them fully and sometimes partially or in a sub-optimal manner. In this article, we express concerns about the violation of some of the Open Science principles and its potential impact on the quality of research output. We provide evidence of the misuses of these principles at different stages of the scientific process. We call for a wider adoption of Open Science practices in the hope that this work will encourage a broader endorsement of Open Science principles and serve as a reminder that science should always be a rigorous process, reliable and transparent, especially in the context of a pandemic where research findings are being translated into practice even more rapidly. We provide all data and scripts at https://osf.io/renxy/ .},
	language = {en},
	number = {1},
	urldate = {2021-09-07},
	journal = {BMC Medical Research Methodology},
	author = {Besançon, Lonni and Peiffer-Smadja, Nathan and Segalas, Corentin and Jiang, Haiting and Masuzzo, Paola and Smout, Cooper and Billy, Eric and Deforet, Maxime and Leyrat, Clémence},
	month = dec,
	year = {2021},
	pages = {117},
}

@article{munday_methods_2020,
	title = {Methods matter in repeating ocean acidification studies},
	volume = {586},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-020-2803-x},
	doi = {10.1038/s41586-020-2803-x},
	language = {en},
	number = {7830},
	urldate = {2021-09-02},
	journal = {Nature},
	author = {Munday, Philip L. and Dixson, Danielle L. and Welch, Megan J. and Chivers, Douglas P. and Domenici, Paolo and Grosell, Martin and Heuer, Rachael M. and Jones, Geoffrey P. and McCormick, Mark I. and Meekan, Mark and Nilsson, Göran E. and Ravasi, Timothy and Watson, Sue-Ann},
	month = oct,
	year = {2020},
	pages = {E20--E24},
}

@article{culina_low_2020,
	title = {Low availability of code in ecology: {A} call for urgent action},
	volume = {18},
	issn = {1545-7885},
	shorttitle = {Low availability of code in ecology},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000763},
	doi = {10.1371/journal.pbio.3000763},
	abstract = {Access to analytical code is essential for transparent and reproducible research. We review the state of code availability in ecology using a random sample of 346 nonmolecular articles published between 2015 and 2019 under mandatory or encouraged code-sharing policies. Our results call for urgent action to increase code availability: only 27\% of eligible articles were accompanied by code. In contrast, data were available for 79\% of eligible articles, highlighting that code availability is an important limiting factor for computational reproducibility in ecology. Although the percentage of ecological journals with mandatory or encouraged code-sharing policies has increased considerably, from 15\% in 2015 to 75\% in 2020, our results show that code-sharing policies are not adhered to by most authors. We hope these results will encourage journals, institutions, funding agencies, and researchers to address this alarming situation.},
	language = {en},
	number = {7},
	urldate = {2021-04-09},
	journal = {PLOS Biology},
	author = {Culina, Antica and Berg, Ilona van den and Evans, Simon and Sánchez-Tójar, Alfredo},
	month = jul,
	year = {2020},
	keywords = {Reproducibility, Computer software, Conservation science, Ecology, Graphical user interfaces, Institutional funding of science, Research reporting guidelines, Theoretical ecology},
	pages = {e3000763},
	annote = {Publisher: Public Library of Science},
}

@misc{noauthor_open_2019,
	title = {Open {Science} {Isn}'t {Always} {Open} to {All} {Scientists}},
	url = {https://www.americanscientist.org/article/open-science-isnt-always-open-to-all-scientists},
	abstract = {Current efforts to make research more accessible and transparent can reinforce inequality within STEM professions.},
	language = {en},
	urldate = {2020-10-27},
	month = jan,
	year = {2019},
	note = {Publication Title: American Scientist},
}

@article{rubin_p_2017,
	title = {Do p {Values} {Lose} {Their} {Meaning} in {Exploratory} {Analyses}? {It} {Depends} {How} {You} {Define} the {Familywise} {Error} {Rate}},
	volume = {21},
	issn = {1089-2680},
	shorttitle = {Do p {Values} {Lose} {Their} {Meaning} in {Exploratory} {Analyses}?},
	url = {https://doi.org/10.1037/gpr0000123},
	doi = {10.1037/gpr0000123},
	abstract = {Several researchers have recently argued that p values lose their meaning in exploratory analyses due to an unknown inflation of the alpha level (e.g., Nosek Wagenmakers, 2016). For this argument to be tenable, the familywise error rate must be defined in relation to the number of hypotheses that are tested in the same study or article. Under this conceptualization, the familywise error rate is usually unknowable in exploratory analyses because it is usually unclear how many hypotheses have been tested on a spontaneous basis and then omitted from the final research report. In the present article, I argue that it is inappropriate to conceptualize the familywise error rate in relation to the number of hypotheses that are tested. Instead, it is more appropriate to conceptualize familywise error in relation to the number of different tests that are conducted on the same null hypothesis in the same study. Under this conceptualization, alpha-level adjustments in exploratory analyses are (a) less necessary and (b) objectively verifiable. As a result, p values do not lose their meaning in exploratory analyses.},
	number = {3},
	urldate = {2020-10-08},
	journal = {Review of General Psychology},
	author = {Rubin, Mark},
	month = sep,
	year = {2017},
	pages = {269--275},
	annote = {Publisher: SAGE Publications Inc},
}

@article{sullivan_open_2019,
	title = {Open and {Reproducible} {Research} on {Open} {Science} {Framework}},
	volume = {18},
	copyright = {© 2019 John Wiley \& Sons, Inc.},
	issn = {1948-3430},
	url = {https://currentprotocols.onlinelibrary.wiley.com/doi/abs/10.1002/cpet.32},
	doi = {10.1002/cpet.32},
	abstract = {By implementing more transparent research practices, authors have the opportunity to stand out and showcase work that is more reproducible, easier to build upon, and more credible. Scientists gain by making work easier to share and maintain within their own laboratories, and the scientific community gains by making underlying data or research materials more available for confirmation or making new discoveries. The following protocol gives authors step-by-step instructions for using the free and open source Open Science Framework (OSF) to create a data management plan, preregister their study, use version control, share data and other research materials, or post a preprint for quick and easy dissemination. © 2019 by John Wiley \& Sons, Inc.},
	language = {en},
	number = {1},
	urldate = {2020-09-20},
	journal = {Current Protocols Essential Laboratory Techniques},
	author = {Sullivan, Ian and DeHaven, Alexander and Mellor, David},
	year = {2019},
	keywords = {open science, preregistration, data management, open data, reproducible research},
	pages = {e32},
	annote = {\_eprint: https://currentprotocols.onlinelibrary.wiley.com/doi/pdf/10.1002/cpet.32},
}

@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	shorttitle = {False-{Positive} {Psychology}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2020-03-17},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pages = {1359--1366},
}

@article{parker_making_2019,
	title = {Making conservation science more reliable with preregistration and registered reports},
	volume = {33},
	issn = {1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/cobi.13342},
	doi = {10.1111/cobi.13342},
	language = {en},
	number = {4},
	urldate = {2020-09-19},
	journal = {Conservation Biology},
	author = {Parker, Timothy and Fraser, Hannah and Nakagawa, Shinichi},
	year = {2019},
	pages = {747--750},
	annote = {\_eprint: https://conbio.onlinelibrary.wiley.com/doi/pdf/10.1111/cobi.13342},
}

@article{nichols_accumulating_2019,
	title = {Accumulating evidence in ecology: {Once} is not enough},
	volume = {9},
	issn = {2045-7758},
	shorttitle = {Accumulating evidence in ecology},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.5836},
	doi = {10.1002/ece3.5836},
	abstract = {Many published studies in ecological science are viewed as stand-alone investigations that purport to provide new insights into how ecological systems behave based on single analyses. But it is rare for results of single studies to provide definitive results, as evidenced in current discussions of the “reproducibility crisis” in science. The key step in science is the comparison of hypothesis-based predictions with observations, where the predictions are typically generated by hypothesis-specific models. Repeating this step allows us to gain confidence in the predictive ability of a model, and its corresponding hypothesis, and thus to accumulate evidence and eventually knowledge. This accumulation may occur via an ad hoc approach, via meta-analyses, or via a more systematic approach based on the anticipated evolution of an information state. We argue the merits of this latter approach, provide an example, and discuss implications for designing sequences of studies focused on a particular question. We conclude by discussing current data collection programs that are preadapted to use this approach and argue that expanded use would increase the rate of learning in ecology, as well as our confidence in what is learned.},
	language = {en},
	number = {24},
	urldate = {2020-09-19},
	journal = {Ecology and Evolution},
	author = {Nichols, James D. and Kendall, William L. and Boomer, Gregory Scott},
	year = {2019},
	keywords = {reproducibility, replication, Bayes theorem, ecology, evidence, information state, knowledge, science},
	pages = {13991--14004},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.5836},
}

@article{parker_empowering_2018,
	title = {Empowering peer reviewers with a checklist to improve transparency},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-018-0545-z},
	doi = {10.1038/s41559-018-0545-z},
	abstract = {Peer review is widely considered fundamental to maintaining the rigour of science, but it often fails to ensure transparency and reduce bias in published papers, and this systematically weakens the quality of published inferences. In part, this is because many reviewers are unaware of important questions to ask with respect to the soundness of the design and analyses, and the presentation of the methods and results; also some reviewers may expect others to be responsible for these tasks. We therefore present a reviewers’ checklist of ten questions that address these critical components. Checklists are commonly used by practitioners of other complex tasks, and we see great potential for the wider adoption of checklists for peer review, especially to reduce bias and facilitate transparency in published papers. We expect that such checklists will be well received by many reviewers.},
	language = {en},
	number = {6},
	urldate = {2020-09-19},
	journal = {Nature Ecology \& Evolution},
	author = {Parker, Timothy H. and Griffith, Simon C. and Bronstein, Judith L. and Fidler, Fiona and Foster, Susan and Fraser, Hannah and Forstmeier, Wolfgang and Gurevitch, Jessica and Koricheva, Julia and Seppelt, Ralf and Tingley, Morgan W. and Nakagawa, Shinichi},
	month = jun,
	year = {2018},
	pages = {929--935},
	annote = {Number: 6 Publisher: Nature Publishing Group},
}

@article{nilsen_exploratory_2020,
	title = {Exploratory and confirmatory research in the open science era},
	volume = {57},
	copyright = {© 2020 The Authors. Journal of Applied Ecology published by John Wiley \& Sons Ltd on behalf of British Ecological Society},
	issn = {1365-2664},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.13571},
	doi = {10.1111/1365-2664.13571},
	abstract = {Applied ecological research is increasingly inspired by the open science movement. However, new challenges about how we define our science when biodiversity data are being shared and re-used are not solved. Among these challenges is the risk associated with blurring the distinction between research that mainly seeks to explore patterns with no a-priori articulated hypotheses (exploratory research), and research that explicitly tests a-priori formulated hypotheses (confirmatory research). A rapid screening of a random selection of the peer-reviewed literature suggests that neither experimental protocols nor hypothesis-testing sensu stricto are common in applied ecological research. In addition, most experiments are carried out on small spatial scales, which contrast with current global policy needs and research trends towards addressing large spatial and temporal scales. This latter trend makes it unfeasible for policy to rely mainly on insights gained from experimental research. To solve fundamental local, regional and global societal challenges, we need both exploratory and confirmatory research. However, the fundamental roles that confirmatory research testing a-priori hypothesis play in establishing causal relationships need to be revaluated in applied ecological research. A clearer distinction between exploratory and confirmatory research is currently needed, and could be facilitated by allocating journal sections to different types of research; by embracing new tools offered by the open science era, such as pre-registration of hypothesis; by establishing new systems where post-hoc hypotheses emerging through exploration can also be registered for later testing; and by more broad adoption of causal inference methods that foster more structured testing of hypotheses about causal mechanisms from observational biodiversity data. Synthesis and applications. To gain the full benefits of the open science era, researchers, funding bodies and journal editors should explicitly consider approaches and incentives that encourage openness about methods and approaches, as well as value the plurality of scientific approaches needed to address questions in applied ecology and conservation science.},
	language = {en},
	number = {4},
	urldate = {2020-09-28},
	journal = {Journal of Applied Ecology},
	author = {Nilsen, Erlend B. and Bowler, Diana E. and Linnell, John D. C.},
	year = {2020},
	keywords = {open science, reproducible research, causal inference, confirmatory research, exploratory research, large-scale assessment, science philosophy},
	pages = {842--847},
	annote = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2664.13571},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	language = {en},
	number = {1},
	urldate = {2020-03-17},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	pages = {0021},
}

@article{ives_informative_2018,
	title = {Informative {Irreproducibility} and the {Use} of {Experiments} in {Ecology}},
	volume = {68},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/68/10/746/5065826},
	doi = {10.1093/biosci/biy090},
	abstract = {Before taking a drug, I want tests of the drug to be reproducible. I would expect testing across women and men, all demographic and ethnic groups, and people wi},
	language = {en},
	number = {10},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Ives, Anthony R.},
	month = oct,
	year = {2018},
	pages = {746--747},
	annote = {Publisher: Oxford Academic},
}

@article{milcu_genotypic_2018,
	title = {Genotypic variability enhances the reproducibility of an ecological study},
	volume = {2},
	copyright = {2018 © The Author(s) 2017, under exclusive licence to Macmillan Publishers Limited, part of Springer Nature},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-017-0434-x},
	doi = {10.1038/s41559-017-0434-x},
	abstract = {Many scientific disciplines are currently experiencing a 'reproducibility crisis' because numerous scientific findings cannot be repeated consistently. A novel but controversial hypothesis postulates that stringent levels of environmental and biotic standardization in experimental studies reduce reproducibility by amplifying the impacts of laboratory-specific environmental factors not accounted for in study designs. A corollary to this hypothesis is that a deliberate introduction of controlled systematic variability (CSV) in experimental designs may lead to increased reproducibility. To test this hypothesis, we had 14 European laboratories run a simple microcosm experiment using grass (Brachypodium distachyon L.) monocultures and grass and legume (Medicago truncatula Gaertn.) mixtures. Each laboratory introduced environmental and genotypic CSV within and among replicated microcosms established in either growth chambers (with stringent control of environmental conditions) or glasshouses (with more variable environmental conditions). The introduction of genotypic CSV led to 18\% lower among-laboratory variability in growth chambers, indicating increased reproducibility, but had no significant effect in glasshouses where reproducibility was generally lower. Environmental CSV had little effect on reproducibility. Although there are multiple causes for the 'reproducibility crisis', deliberately including genetic variability may be a simple solution for increasing the reproducibility of ecological studies performed under stringently controlled environmental conditions.},
	language = {en},
	number = {2},
	urldate = {2020-09-23},
	journal = {Nature Ecology \& Evolution},
	author = {Milcu, Alexandru and Puga-Freitas, Ruben and Ellison, Aaron M. and Blouin, Manuel and Scheu, Stefan and Freschet, Grégoire T. and Rose, Laura and Barot, Sebastien and Cesarz, Simone and Eisenhauer, Nico and Girin, Thomas and Assandri, Davide and Bonkowski, Michael and Buchmann, Nina and Butenschoen, Olaf and Devidal, Sebastien and Gleixner, Gerd and Gessler, Arthur and Gigon, Agnès and Greiner, Anna and Grignani, Carlo and Hansart, Amandine and Kayler, Zachary and Lange, Markus and Lata, Jean-Christophe and Le Galliard, Jean-François and Lukac, Martin and Mannerheim, Neringa and Müller, Marina E. H. and Pando, Anne and Rotter, Paula and Scherer-Lorenzen, Michael and Seyhun, Rahme and Urban-Mead, Katherine and Weigelt, Alexandra and Zavattaro, Laura and Roy, Jacques},
	month = feb,
	year = {2018},
	pages = {279--287},
	annote = {Number: 2 Publisher: Nature Publishing Group},
}

@article{mckiernan_how_2016,
	title = {How open science helps researchers succeed},
	volume = {5},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/16800},
	doi = {10.7554/eLife.16800},
	abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
	language = {en},
	urldate = {2020-03-17},
	journal = {eLife},
	author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
	month = jul,
	year = {2016},
	pages = {e16800},
}

@article{grahe_open_2020,
	title = {Open {Science} {Promotes} {Diverse}, {Just}, and {Sustainable} {Research} and {Educational} {Outcomes}},
	volume = {19},
	issn = {1475-7257},
	url = {https://doi.org/10.1177/1475725719869164},
	doi = {10.1177/1475725719869164},
	abstract = {Open science initiatives, which are often collaborative efforts focused on making research more transparent, have experienced increasing popularity in the past decade. Open science principles of openness and transparency provide opportunities to advance diversity, justice, and sustainability by promoting diverse, just, and sustainable outcomes among both undergraduate and senior researchers. We review models that demonstrate the importance of greater diversity, justice, and sustainability in psychological science before describing how open science initiatives promote these values. Open science initiatives also promote diversity, justice, and sustainability through increased levels of inclusion and access, equitable distribution of opportunities and dissemination of knowledge, and increased sustainability stemming from increased generalizability. In order to provide an application of the concepts discussed, we offer a set of diversity, justice, and sustainability lens questions for individuals to use while assessing research projects and other organizational systems and consider concrete classroom applications for these initiatives.},
	language = {en},
	number = {1},
	urldate = {2020-09-28},
	journal = {Psychology Learning \& Teaching},
	author = {Grahe, Jon E and Cuccolo, Kelly and Leighton, Dana C and Cramblet Alvarez, Leslie D},
	month = mar,
	year = {2020},
	pages = {5--20},
	annote = {Publisher: SAGE Publications},
}

@article{gallagher_open_2020,
	title = {Open {Science} principles for accelerating trait-based science across the {Tree} of {Life}},
	volume = {4},
	copyright = {2020 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-020-1109-6},
	doi = {10.1038/s41559-020-1109-6},
	abstract = {Synthesizing trait observations and knowledge across the Tree of Life remains a grand challenge for biodiversity science. Species traits are widely used in ecological and evolutionary science, and new data and methods have proliferated rapidly. Yet accessing and integrating disparate data sources remains a considerable challenge, slowing progress toward a global synthesis to integrate trait data across organisms. Trait science needs a vision for achieving global integration across all organisms. Here, we outline how the adoption of key Open Science principles—open data, open source and open methods—is transforming trait science, increasing transparency, democratizing access and accelerating global synthesis. To enhance widespread adoption of these principles, we introduce the Open Traits Network (OTN), a global, decentralized community welcoming all researchers and institutions pursuing the collaborative goal of standardizing and integrating trait data across organisms. We demonstrate how adherence to Open Science principles is key to the OTN community and outline five activities that can accelerate the synthesis of trait data across the Tree of Life, thereby facilitating rapid advances to address scientific inquiries and environmental issues. Lessons learned along the path to a global synthesis of trait data will provide a framework for addressing similarly complex data science and informatics challenges.},
	language = {en},
	number = {3},
	urldate = {2020-09-28},
	journal = {Nature Ecology \& Evolution},
	author = {Gallagher, Rachael V. and Falster, Daniel S. and Maitner, Brian S. and Salguero-Gómez, Roberto and Vandvik, Vigdis and Pearse, William D. and Schneider, Florian D. and Kattge, Jens and Poelen, Jorrit H. and Madin, Joshua S. and Ankenbrand, Markus J. and Penone, Caterina and Feng, Xiao and Adams, Vanessa M. and Alroy, John and Andrew, Samuel C. and Balk, Meghan A. and Bland, Lucie M. and Boyle, Brad L. and Bravo-Avila, Catherine H. and Brennan, Ian and Carthey, Alexandra J. R. and Catullo, Renee and Cavazos, Brittany R. and Conde, Dalia A. and Chown, Steven L. and Fadrique, Belen and Gibb, Heloise and Halbritter, Aud H. and Hammock, Jennifer and Hogan, J. Aaron and Holewa, Hamish and Hope, Michael and Iversen, Colleen M. and Jochum, Malte and Kearney, Michael and Keller, Alexander and Mabee, Paula and Manning, Peter and McCormack, Luke and Michaletz, Sean T. and Park, Daniel S. and Perez, Timothy M. and Pineda-Munoz, Silvia and Ray, Courtenay A. and Rossetto, Maurizio and Sauquet, Hervé and Sparrow, Benjamin and Spasojevic, Marko J. and Telford, Richard J. and Tobias, Joseph A. and Violle, Cyrille and Walls, Ramona and Weiss, Katherine C. B. and Westoby, Mark and Wright, Ian J. and Enquist, Brian J.},
	month = mar,
	year = {2020},
	pages = {294--303},
	annote = {Number: 3 Publisher: Nature Publishing Group},
}

@article{fidler_metaresearch_2017,
	title = {Metaresearch for {Evaluating} {Reproducibility} in {Ecology} and {Evolution}},
	volume = {67},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/67/3/282/2900173},
	doi = {10.1093/biosci/biw159},
	abstract = {Recent replication projects in other disciplines have uncovered disturbingly low levels of reproducibility, suggesting that those research literatures may contain unverifiable claims. The conditions contributing to irreproducibility in other disciplines are also present in ecology. These include a large discrepancy between the proportion of “positive” or “significant” results and the average statistical power of empirical research, incomplete reporting of sampling stopping rules and results, journal policies that discourage replication studies, and a prevailing publish-or-perish research culture that encourages questionable research practices. We argue that these conditions constitute sufficient reason to systematically evaluate the reproducibility of the evidence base in ecology and evolution. In some cases, the direct replication of ecological research is difficult because of strong temporal and spatial dependencies, so here, we propose metaresearch projects that will provide proxy measures of reproducibility.},
	language = {en},
	number = {3},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
	month = mar,
	year = {2017},
	pages = {282--289},
	annote = {Publisher: Oxford Academic},
}

@article{forstmeier_detecting_2017,
	title = {Detecting and avoiding likely false-positive findings – a practical guide},
	volume = {92},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12315},
	doi = {10.1111/brv.12315},
	abstract = {Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a significant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientific rigour.},
	language = {en},
	number = {4},
	urldate = {2020-05-17},
	journal = {Biological Reviews},
	author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
	year = {2017},
	keywords = {P-hacking, confirmation bias, preregistration, replication, HARKing, hindsight bias, overfitting, power, researcher degrees of freedom, Type I error},
	pages = {1941--1968},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12315},
}

@article{fanelli_meta-assessment_2017,
	title = {Meta-assessment of bias in science},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1618569114},
	doi = {10.1073/pnas.1618569114},
	abstract = {Numerous biases are believed to affect the scientific literature, but their actual prevalence across disciplines is unknown. To gain a comprehensive picture of the potential imprint of bias in science, we probed for the most commonly postulated bias-related patterns and risk factors, in a large random sample of meta-analyses taken from all disciplines. The magnitude of these biases varied widely across fields and was overall relatively small. However, we consistently observed a significant risk of small, early, and highly cited studies to overestimate effects and of studies not published in peer-reviewed journals to underestimate them. We also found at least partial confirmation of previous evidence suggesting that US studies and early studies might report more extreme effects, although these effects were smaller and more heterogeneously distributed across meta-analyses and disciplines. Authors publishing at high rates and receiving many citations were, overall, not at greater risk of bias. However, effect sizes were likely to be overestimated by early-career researchers, those working in small or long-distance collaborations, and those responsible for scientific misconduct, supporting hypotheses that connect bias to situational factors, lack of mutual control, and individual integrity. Some of these patterns and risk factors might have modestly increased in intensity over time, particularly in the social sciences. Our findings suggest that, besides one being routinely cautious that published small, highly-cited, and earlier studies may yield inflated results, the feasibility and costs of interventions to attenuate biases in the literature might need to be discussed on a discipline-specific and topic-specific basis.},
	language = {en},
	number = {14},
	urldate = {2020-09-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fanelli, Daniele and Costas, Rodrigo and Ioannidis, John P. A.},
	month = apr,
	year = {2017},
	pages = {3714--3719},
}

@article{clark_ocean_2020,
	title = {Ocean acidification does not impair the behaviour of coral reef fishes},
	volume = {577},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1903-y},
	doi = {10.1038/s41586-019-1903-y},
	abstract = {The partial pressure of CO2 in the oceans has increased rapidly over the past century, driving ocean acidification and raising concern for the stability of marine ecosystems1–3. Coral reef fishes are predicted to be especially susceptible to end-of-century ocean acidification on the basis of several high-profile papers4,5 that have reported profound behavioural and sensory impairments—for example, complete attraction to the chemical cues of predators under conditions of ocean acidification. Here, we comprehensively and transparently show that—in contrast to previous studies—end-of-century ocean acidification levels have negligible effects on important behaviours of coral reef fishes, such as the avoidance of chemical cues from predators, fish activity levels and behavioural lateralization (left–right turning preference). Using data simulations, we additionally show that the large effect sizes and small within-group variances that have been reported in several previous studies are highly improbable. Together, our findings indicate that the reported effects of ocean acidification on the behaviour of coral reef fishes are not reproducible, suggesting that behavioural perturbations will not be a major consequence for coral reef fishes in high CO2 oceans.},
	language = {en},
	number = {7790},
	urldate = {2020-09-20},
	journal = {Nature},
	author = {Clark, Timothy D. and Raby, Graham D. and Roche, Dominique G. and Binning, Sandra A. and Speers-Roesch, Ben and Jutfelt, Fredrik and Sundin, Josefin},
	month = jan,
	year = {2020},
	pages = {370--375},
	annote = {Number: 7790 Publisher: Nature Publishing Group},
}

@article{aczel_consensus-based_2020,
	title = {A consensus-based transparency checklist},
	volume = {4},
	copyright = {2019 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-019-0772-6},
	doi = {10.1038/s41562-019-0772-6},
	abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
	language = {en},
	number = {1},
	urldate = {2020-09-19},
	journal = {Nature Human Behaviour},
	author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharský, Šimon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munafò, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ronán M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and Giner-Sorolla, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and de la Guardia, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
	month = jan,
	year = {2020},
	pages = {4--6},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{ali-khan_identifying_2018,
	title = {Identifying the challenges in implementing open science},
	volume = {2},
	issn = {2515-5059},
	url = {https://mniopenresearch.org/articles/2-5/v1},
	doi = {10.12688/mniopenres.12805.1},
	abstract = {Areas of open science (OS) policy and practice are already relatively well-advanced in several countries and sectors through the initiatives of some governments, funders, philanthropy, researchers and the community. Nevertheless, the current research and innovation system, including in the focus of this report, the life sciences, remains weighted against OS. In October 2017, thought-leaders from across the world gathered at an Open Science Leadership Forum in the Washington DC office of the Bill and Melinda Gates Foundation to share their views on what successful OS looks like. We focused on OS partnerships as this is an emerging model that aims to accelerate science and innovation. These outcomes are captured in a first meeting report: Defining Success in Open Science. On several occasions, these conversations turned to the challenges that must be addressed and new policies required to effectively and sustainably advance OS practice. Thereupon, in this report, we describe the concerns raised and what is needed to address them supplemented by our review of the literature, and suggest the stakeholder groups that may be best placed to begin to take action. It emerges that to be successful, OS will require the active engagement of all stakeholders: while the research community must develop research questions, identify partners and networks, policy communities need to create an environment that is supportive of experimentation by removing barriers. This report aims to contribute to ongoing discussions about OS and its implementation. It is also part of a step-wise process to develop and mobilize a toolkit of quantitative and qualitative indicators to assist global stakeholders in implementing high value OS collaborations. Currently in co-development through an open and international process, this set of measures will allow the generation of needed evidence on the influence of OS partnerships on research, innovation, and critical social and economic goals.},
	language = {en},
	urldate = {2020-09-28},
	journal = {MNI Open Research},
	author = {Ali-Khan, Sarah E. and Jean, Antoine and Gold, E. Richard},
	month = oct,
	year = {2018},
	pages = {5},
}

@article{purgar_quantifying_2022-1,
	title = {Quantifying research waste in ecology},
	volume = {6},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-022-01820-0},
	doi = {10.1038/s41559-022-01820-0},
	language = {en},
	number = {9},
	urldate = {2025-02-27},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Klanjscek, Tin and Culina, Antica},
	month = jul,
	year = {2022},
	pages = {1390--1397},
}

@article{purgar_supporting_2024-1,
	title = {Supporting study registration to reduce research waste},
	volume = {8},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-024-02433-5},
	doi = {10.1038/s41559-024-02433-5},
	language = {en},
	number = {8},
	urldate = {2025-02-11},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Glasziou, Paul and Klanjscek, Tin and Nakagawa, Shinichi and Culina, Antica},
	month = jun,
	year = {2024},
	pages = {1391--1399},
}

@article{nakagawa_poor_2025-1,
	title = {Poor hypotheses and research waste in biology: learning from a theory crisis in psychology},
	volume = {23},
	issn = {1741-7007},
	shorttitle = {Poor hypotheses and research waste in biology},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-025-02134-w},
	doi = {10.1186/s12915-025-02134-w},
	abstract = {Abstract While psychologists have extensively discussed the notion of a “theory crisis” arising from vague and incorrect hypotheses, there has been no debate about such a crisis in biology. However, biologists have long discussed communication failures between theoreticians and empiricists. We argue such failure is one aspect of a theory crisis because misapplied and misunderstood theories lead to poor hypotheses and research waste. We review its solutions and compare them with methodology-focused solutions proposed for replication crises. We conclude by discussing how promoting inclusion, diversity, equity, and accessibility (IDEA) in theoretical biology could contribute to ameliorating breakdowns in the theory-empirical cycle.},
	language = {en},
	number = {1},
	urldate = {2025-02-11},
	journal = {BMC Biology},
	author = {Nakagawa, Shinichi and Armitage, David W. and Froese, Tom and Yang, Yefeng and Lagisz, Malgorzata},
	month = feb,
	year = {2025},
	pages = {33},
}

@article{gould_same_2025-1,
	title = {Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology},
	volume = {23},
	issn = {1741-7007},
	shorttitle = {Same data, different analysts},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-02101-x},
	doi = {10.1186/s12915-024-02101-x},
	language = {en},
	number = {1},
	urldate = {2025-08-28},
	journal = {BMC Biology},
	author = {Gould, Elliot and Fraser, Hannah S. and Parker, Timothy H. and Nakagawa, Shinichi and Griffith, Simon C. and Vesk, Peter A. and Fidler, Fiona and Hamilton, Daniel G. and Abbey-Lee, Robin N. and Abbott, Jessica K. and Aguirre, Luis A. and Alcaraz, Carles and Aloni, Irith and Altschul, Drew and Arekar, Kunal and Atkins, Jeff W. and Atkinson, Joe and Baker, Christopher M. and Barrett, Meghan and Bell, Kristian and Bello, Suleiman Kehinde and Beltrán, Iván and Berauer, Bernd J. and Bertram, Michael Grant and Billman, Peter D. and Blake, Charlie K. and Blake, Shannon and Bliard, Louis and Bonisoli-Alquati, Andrea and Bonnet, Timothée and Bordes, Camille Nina Marion and Bose, Aneesh P. H. and Botterill-James, Thomas and Boyd, Melissa Anna and Boyle, Sarah A. and Bradfer-Lawrence, Tom and Bradham, Jennifer and Brand, Jack A. and Brengdahl, Martin I. and Bulla, Martin and Bussière, Luc and Camerlenghi, Ettore and Campbell, Sara E. and Campos, Leonardo L. F. and Caravaggi, Anthony and Cardoso, Pedro and Carroll, Charles J. W. and Catanach, Therese A. and Chen, Xuan and Chik, Heung Ying Janet and Choy, Emily Sarah and Christie, Alec Philip and Chuang, Angela and Chunco, Amanda J. and Clark, Bethany L. and Contina, Andrea and Covernton, Garth A. and Cox, Murray P. and Cressman, Kimberly A. and Crotti, Marco and Crouch, Connor Davidson and D’Amelio, Pietro B. and De Sousa, Alexandra Allison and Döbert, Timm Fabian and Dobler, Ralph and Dobson, Adam J. and Doherty, Tim S. and Drobniak, Szymon Marian and Duffy, Alexandra Grace and Duncan, Alison B. and Dunn, Robert P. and Dunning, Jamie and Dutta, Trishna and Eberhart-Hertel, Luke and Elmore, Jared Alan and Elsherif, Mahmoud Medhat and English, Holly M. and Ensminger, David C. and Ernst, Ulrich Rainer and Ferguson, Stephen M. and Fernandez-Juricic, Esteban and Ferreira-Arruda, Thalita and Fieberg, John and Finch, Elizabeth A. and Fiorenza, Evan A. and Fisher, David N. and Fontaine, Amélie and Forstmeier, Wolfgang and Fourcade, Yoan and Frank, Graham S. and Freund, Cathryn A. and Fuentes-Lillo, Eduardo and Gandy, Sara L. and Gannon, Dustin G. and García-Cervigón, Ana I. and Garretson, Alexis C. and Ge, Xuezhen and Geary, William L. and Géron, Charly and Gilles, Marc and Girndt, Antje and Gliksman, Daniel and Goldspiel, Harrison B. and Gomes, Dylan G. E. and Good, Megan Kate and Goslee, Sarah C. and Gosnell, J. Stephen and Grames, Eliza M. and Gratton, Paolo and Grebe, Nicholas M. and Greenler, Skye M. and Griffioen, Maaike and Griffith, Daniel M. and Griffith, Frances J. and Grossman, Jake J. and Güncan, Ali and Haesen, Stef and Hagan, James G. and Hager, Heather A. and Harris, Jonathan Philo and Harrison, Natasha Dean and Hasnain, Sarah Syedia and Havird, Justin Chase and Heaton, Andrew J. and Herrera-Chaustre, María Laura and Howard, Tanner J. and Hsu, Bin-Yan and Iannarilli, Fabiola and Iranzo, Esperanza C. and Iverson, Erik N. K. and Jimoh, Saheed Olaide and Johnson, Douglas H. and Johnsson, Martin and Jorna, Jesse and Jucker, Tommaso and Jung, Martin and Kačergytė, Ineta and Kaltz, Oliver and Ke, Alison and Kelly, Clint D. and Keogan, Katharine and Keppeler, Friedrich Wolfgang and Killion, Alexander K. and Kim, Dongmin and Kochan, David P. and Korsten, Peter and Kothari, Shan and Kuppler, Jonas and Kusch, Jillian M. and Lagisz, Malgorzata and Lalla, Kristen Marianne and Larkin, Daniel J. and Larson, Courtney L. and Lauck, Katherine S. and Lauterbur, M. Elise and Law, Alan and Léandri-Breton, Don-Jean and Lembrechts, Jonas J. and L’Herpiniere, Kiara and Lievens, Eva J. P. and De Lima, Daniela Oliveira and Lindsay, Shane and Luquet, Martin and MacLeod, Ross and Macphie, Kirsty H. and Magellan, Kit and Mair, Magdalena M. and Malm, Lisa E. and Mammola, Stefano and Mandeville, Caitlin P. and Manhart, Michael and Manrique-Garzon, Laura Milena and Mäntylä, Elina and Marchand, Philippe and Marshall, Benjamin Michael and Martin, Charles A. and Martin, Dominic Andreas and Martin, Jake Mitchell and Martinig, April Robin and McCallum, Erin S. and McCauley, Mark and McNew, Sabrina M. and Meiners, Scott J. and Merkling, Thomas and Michelangeli, Marcus and Moiron, Maria and Moreira, Bruno and Mortensen, Jennifer and Mos, Benjamin and Muraina, Taofeek Olatunbosun and Murphy, Penelope Wrenn and Nelli, Luca and Niemelä, Petri and Nightingale, Josh and Nilsonne, Gustav and Nolazco, Sergio and Nooten, Sabine S. and Novotny, Jessie Lanterman and Olin, Agnes Birgitta and Organ, Chris L. and Ostevik, Kate L. and Palacio, Facundo Xavier and Paquet, Matthieu and Parker, Darren James and Pascall, David J. and Pasquarella, Valerie J. and Paterson, John Harold and Payo-Payo, Ana and Pedersen, Karen Marie and Perez, Grégoire and Perry, Kayla I. and Pottier, Patrice and Proulx, Michael J. and Proulx, Raphaël and Pruett, Jessica L and Ramananjato, Veronarindra and Randimbiarison, Finaritra Tolotra and Razafindratsima, Onja H. and Rennison, Diana J. and Riva, Federico and Riyahi, Sepand and Roast, Michael James and Rocha, Felipe Pereira and Roche, Dominique G. and Román-Palacios, Cristian and Rosenberg, Michael S. and Ross, Jessica and Rowland, Freya E. and Rugemalila, Deusdedith and Russell, Avery L. and Ruuskanen, Suvi and Saccone, Patrick and Sadeh, Asaf and Salazar, Stephen M. and Sales, Kris and Salmón, Pablo and Sánchez-Tójar, Alfredo and Santos, Leticia Pereira and Santostefano, Francesca and Schilling, Hayden T. and Schmidt, Marcus and Schmoll, Tim and Schneider, Adam C. and Schrock, Allie E. and Schroeder, Julia and Schtickzelle, Nicolas and Schultz, Nick L. and Scott, Drew A. and Scroggie, Michael Peter and Shapiro, Julie Teresa and Sharma, Nitika and Shearer, Caroline L. and Simón, Diego and Sitvarin, Michael I. and Skupien, Fabrício Luiz and Slinn, Heather Lea and Smith, Grania Polly and Smith, Jeremy A. and Sollmann, Rahel and Whitney, Kaitlin Stack and Still, Shannon Michael and Stuber, Erica F. and Sutton, Guy F. and Swallow, Ben and Taff, Conor Claverie and Takola, Elina and Tanentzap, Andrew J. and Tarjuelo, Rocío and Telford, Richard J. and Thawley, Christopher J. and Thierry, Hugo and Thomson, Jacqueline and Tidau, Svenja and Tompkins, Emily M. and Tortorelli, Claire Marie and Trlica, Andrew and Turnell, Biz R. and Urban, Lara and Van De Vondel, Stijn and Van Der Wal, Jessica Eva Megan and Van Eeckhoven, Jens and Van Oordt, Francis and Vanderwel, K. Michelle and Vanderwel, Mark C. and Vanderwolf, Karen J. and Vélez, Juliana and Vergara-Florez, Diana Carolina and Verrelli, Brian C. and Vieira, Marcus Vinícius and Villamil, Nora and Vitali, Valerio and Vollering, Julien and Walker, Jeffrey and Walker, Xanthe J. and Walter, Jonathan A. and Waryszak, Pawel and Weaver, Ryan J. and Wedegärtner, Ronja E. M. and Weller, Daniel L. and Whelan, Shannon and White, Rachel Louise and Wolfson, David William and Wood, Andrew and Yanco, Scott W. and Yen, Jian D. L. and Youngflesh, Casey and Zilio, Giacomo and Zimmer, Cédric and Zimmerman, Gregory Mark and Zitomer, Rachel A.},
	month = feb,
	year = {2025},
	pages = {35},
}

@article{bourne_ten_2017-1,
	title = {Ten simple rules to consider regarding preprint submission},
	volume = {13},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005473},
	doi = {10.1371/journal.pcbi.1005473},
	language = {en},
	number = {5},
	urldate = {2023-09-21},
	journal = {PLOS Computational Biology},
	author = {Bourne, Philip E. and Polka, Jessica K. and Vale, Ronald D. and Kiley, Robert},
	month = may,
	year = {2017},
	keywords = {Medicine and health sciences, Scientists, Scientific publishing, Internet, Mathematical physics, Medical journals, Peer review, Software tools},
	pages = {e1005473},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{gopalakrishna_prevalence_2022-1,
	title = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors: {A} survey among academic researchers in {The} {Netherlands}},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263023},
	doi = {10.1371/journal.pone.0263023},
	abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the “publish or perish” incentive system promotes research integrity.},
	language = {en},
	number = {2},
	urldate = {2023-09-05},
	journal = {PLOS ONE},
	author = {Gopalakrishna, Gowri and Riet, Gerben ter and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
	month = feb,
	year = {2022},
	keywords = {Deception, Linear regression analysis, Medical humanities, Medicine and health sciences, Open science, Research integrity, Scientific misconduct, Surveys},
	pages = {e0263023},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{ulrich_questionable_2020-1,
	title = {Questionable research practices may have little effect on replicability},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.58237},
	doi = {10.7554/eLife.58237},
	abstract = {This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.},
	urldate = {2023-09-05},
	journal = {eLife},
	author = {Ulrich, Rolf and Miller, Jeff},
	editor = {Rodgers, Peter and Thompson, William Hedley and Francis, Gregory},
	month = sep,
	year = {2020},
	keywords = {base rate of true effects, false positives, mathematical modelling of research process, meta-research, p-hacking, replicability},
	pages = {e58237},
	annote = {Publisher: eLife Sciences Publications, Ltd},
	annote = {Publisher: eLife Sciences Publications, Ltd},
}

@article{filazzola_replication_2021-1,
	title = {Replication in field ecology: {Identifying} challenges and proposing solutions},
	volume = {12},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Replication in field ecology},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13657},
	doi = {10.1111/2041-210X.13657},
	language = {en},
	number = {10},
	urldate = {2021-10-09},
	journal = {Methods in Ecology and Evolution},
	author = {Filazzola, Alessandro and Cahill, James F.},
	month = oct,
	year = {2021},
	pages = {1780--1792},
}

@misc{magazine_where_2023-1,
	title = {Where the ‘{Wood}-{Wide} {Web}’ {Narrative} {Went} {Wrong}},
	url = {https://undark.org/2023/05/25/where-the-wood-wide-web-narrative-went-wrong/},
	abstract = {Opinion {\textbackslash}textbackslashtextbar A fascinating story about forest fungal networks has captured the public imagination in recent years. Is any of it true?},
	language = {en-US},
	urldate = {2023-09-05},
	author = {Magazine, Undark},
	month = may,
	year = {2023},
	annote = {Publication Title: Undark Magazine},
}

@article{karst_positive_2023-1,
	title = {Positive citation bias and overinterpreted results lead to misinformation on common mycorrhizal networks in forests},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-01986-1},
	doi = {10.1038/s41559-023-01986-1},
	abstract = {A common mycorrhizal network (CMN) is formed when mycorrhizal fungal hyphae connect the roots of multiple plants of the same or different species belowground. Recently, CMNs have captured the interest of broad audiences, especially with respect to forest function and management. We are concerned, however, that recent claims in the popular media about CMNs in forests are disconnected from evidence, and that bias towards citing positive effects of CMNs has developed in the scientific literature. We first evaluated the evidence supporting three common claims. The claims that CMNs are widespread in forests and that resources are transferred through CMNs to increase seedling performance are insufficiently supported because results from field studies vary too widely, have alternative explanations or are too limited to support generalizations. The claim that mature trees preferentially send resources and defence signals to offspring through CMNs has no peer-reviewed, published evidence. We next examined how the results from CMN research are cited and found that unsupported claims have doubled in the past 25 years; a bias towards citing positive effects may obscure our understanding of the structure and function of CMNs in forests. We conclude that knowledge on CMNs is presently too sparse and unsettled to inform forest management.},
	language = {en},
	urldate = {2023-02-13},
	journal = {Nature Ecology \& Evolution},
	author = {Karst, Justine and Jones, Melanie D. and Hoeksema, Jason D.},
	month = feb,
	year = {2023},
	keywords = {Forest ecology, Forestry},
	pages = {1--11},
	annote = {Publisher: Nature Publishing Group},
	annote = {Publisher: Nature Publishing Group},
}

@article{roche_troubleshooting_2014-1,
	title = {Troubleshooting {Public} {Data} {Archiving}: {Suggestions} to {Increase} {Participation}},
	volume = {12},
	issn = {1545-7885},
	shorttitle = {Troubleshooting {Public} {Data} {Archiving}},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001779},
	doi = {10.1371/journal.pbio.1001779},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {PLoS Biology},
	author = {Roche, Dominique G. and Lanfear, Robert and Binning, Sandra A. and Haff, Tonya M. and Schwanz, Lisa E. and Cain, Kristal E. and Kokko, Hanna and Jennions, Michael D. and Kruuk, Loeske E. B.},
	editor = {Eisen, Jonathan A.},
	month = jan,
	year = {2014},
	pages = {e1001779},
}

@article{mejlgaard_research_2020-1,
	title = {Research integrity: nine ways to move from talk to walk},
	volume = {586},
	copyright = {2020 Nature},
	shorttitle = {Research integrity},
	url = {https://www.nature.com/articles/d41586-020-02847-8},
	doi = {10.1038/d41586-020-02847-8},
	abstract = {Counselling, coaches and collegiality — how institutions can share resources to promote best practice in science.},
	language = {en},
	number = {7829},
	urldate = {2020-10-27},
	journal = {Nature},
	author = {Mejlgaard, Niels and Bouter, Lex M. and Gaskell, George and Kavouras, Panagiotis and Allum, Nick and Bendtsen, Anna-Kathrine and Charitidis, Costas A. and Claesen, Nik and Dierickx, Kris and Domaradzka, Anna and Elizondo, Andrea Reyes and Foeger, Nicole and Hiney, Maura and Kaltenbrunner, Wolfgang and Labib, Krishma and Marušić, Ana and Sørensen, Mads P. and Ravn, Tine and Ščepanović, Rea and Tijdink, Joeri K. and Veltri, Giuseppe A.},
	month = oct,
	year = {2020},
	pages = {358--360},
	annote = {Number: 7829 Publisher: Nature Publishing Group},
	annote = {Number: 7829 Publisher: Nature Publishing Group},
}

@article{gelman_garden_nodate-1,
	title = {The garden of forking paths: {Why} multiple comparisons can be a problem, even when there is no “ﬁshing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
	abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of ﬁshing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-speciﬁed and, as a result, were contingent on data.},
	language = {en},
	author = {Gelman, Andrew and Loken, Eric},
}

@article{azevedo_towards_2022-1,
	title = {Towards a culture of open scholarship: the role of pedagogical communities},
	volume = {15},
	issn = {1756-0500},
	shorttitle = {Towards a culture of open scholarship},
	url = {https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-05944-1},
	doi = {10.1186/s13104-022-05944-1},
	abstract = {Abstract The UK House of Commons Science and Technology Committee has called for evidence on the roles that different stakeholders play in reproducibility and research integrity. Of central priority are proposals for improving research integrity and quality, as well as guidance and support for researchers. In response to this, we argue that there is one important component of research integrity that is often absent from discussion: the pedagogical consequences of how we teach, mentor, and supervise students through open scholarship. We justify the need to integrate open scholarship principles into research training within higher education and argue that pedagogical communities play a key role in fostering an inclusive culture of open scholarship. We illustrate these benefits by presenting the Framework for Open and Reproducible Research Training (FORRT) , an international grassroots community whose goal is to provide support, resources, visibility, and advocacy for the adoption of principled, open teaching and mentoring practices, whilst generating conversations about the ethics and social impact of higher-education pedagogy. Representing a diverse group of early-career researchers and students across specialisms, we advocate for greater recognition of and support for pedagogical communities, and encourage all research stakeholders to engage with these communities to enable long-term, sustainable change.},
	language = {en},
	number = {1},
	urldate = {2022-04-28},
	journal = {BMC Research Notes},
	author = {Azevedo, Flávio and Liu, Meng and Pennington, Charlotte R. and Pownall, Madeleine and Evans, Thomas Rhys and Parsons, Sam and Elsherif, Mahmoud Medhat and Micheli, Leticia and Westwood, Samuel J. and {Framework for Open, Reproducible Research Training (FORRT)}},
	month = dec,
	year = {2022},
	pages = {75},
}

@article{davis_writing_2023-1,
	title = {Writing statistical methods for ecologists},
	volume = {14},
	issn = {2150-8925, 2150-8925},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.4539},
	doi = {10.1002/ecs2.4539},
	language = {en},
	number = {5},
	urldate = {2023-05-26},
	journal = {Ecosphere},
	author = {Davis, Amy J. and Kay, Shannon},
	month = may,
	year = {2023},
	pages = {e4539},
}

@article{soeharjono_reported_2021-2,
	title = {Reported {Individual} {Costs} and {Benefits} of {Sharing} {Open} {Data} among {Canadian} {Academic} {Faculty} in {Ecology} and {Evolution}},
	volume = {71},
	issn = {0006-3568, 1525-3244},
	url = {https://academic.oup.com/bioscience/article/71/7/750/6225906},
	doi = {10.1093/biosci/biab024},
	abstract = {Abstract Open data facilitate reproducibility and accelerate scientific discovery but are hindered by perceptions that researchers bear costs and gain few benefits from publicly sharing their data, with limited empirical evidence to the contrary. We surveyed 140 faculty members working in ecology and evolution across Canada's top 20 ranked universities and found that more researchers report benefits (47.9\%) and neutral outcomes (43.6\%) than costs (21.4\%) from openly sharing data. The benefits were independent of career stage and gender, but men and early career researchers were more likely to report costs. We outline mechanisms proposed by the study participants to reduce the individual costs and increase the benefits of open data for faculty members.},
	language = {en},
	number = {7},
	urldate = {2023-02-10},
	journal = {BioScience},
	author = {Soeharjono, Sandrine and Roche, Dominique G},
	month = jul,
	year = {2021},
	pages = {750--756},
}

@article{jenkins_reproducibility_2023-1,
	title = {Reproducibility in ecology and evolution: {Minimum} standards for data and code},
	volume = {13},
	issn = {2045-7758, 2045-7758},
	shorttitle = {Reproducibility in ecology and evolution},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ece3.9961},
	doi = {10.1002/ece3.9961},
	language = {en},
	number = {5},
	urldate = {2023-05-12},
	journal = {Ecology and Evolution},
	author = {Jenkins, Gareth B. and Beckerman, Andrew P. and Bellard, Céline and Benítez‐López, Ana and Ellison, Aaron M. and Foote, Christopher G. and Hufton, Andrew L. and Lashley, Marcus A. and Lortie, Christopher J. and Ma, Zhaoxue and Moore, Allen J. and Narum, Shawn R. and Nilsson, Johan and O'Boyle, Bridget and Provete, Diogo B. and Razgour, Orly and Rieseberg, Loren and Riginos, Cynthia and Santini, Luca and Sibbett, Benjamin and Peres‐Neto, Pedro R.},
	month = may,
	year = {2023},
	pages = {e9961},
}

@article{yang_publication_2023-1,
	title = {Publication bias impacts on effect size, statistical power, and magnitude ({Type} {M}) and sign ({Type} {S}) errors in ecology and evolutionary biology},
	volume = {21},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-022-01485-y},
	doi = {10.1186/s12915-022-01485-y},
	abstract = {Collaborative efforts to directly replicate empirical studies in the medical and social sciences have revealed alarmingly low rates of replicability, a phenomenon dubbed the ‘replication crisis’. Poor replicability has spurred cultural changes targeted at improving reliability in these disciplines. Given the absence of equivalent replication projects in ecology and evolutionary biology, two inter-related indicators offer the opportunity to retrospectively assess replicability: publication bias and statistical power. This registered report assesses the prevalence and severity of small-study (i.e., smaller studies reporting larger effect sizes) and decline effects (i.e., effect sizes decreasing over time) across ecology and evolutionary biology using 87 meta-analyses comprising 4,250 primary studies and 17,638 effect sizes. Further, we estimate how publication bias might distort the estimation of effect sizes, statistical power, and errors in magnitude (Type M or exaggeration ratio) and sign (Type S). We show strong evidence for the pervasiveness of both small-study and decline effects in ecology and evolution. There was widespread prevalence of publication bias that resulted in meta-analytic means being over-estimated by (at least) 0.12 standard deviations. The prevalence of publication bias distorted confidence in meta-analytic results, with 66\% of initially statistically significant meta-analytic means becoming non-significant after correcting for publication bias. Ecological and evolutionary studies consistently had low statistical power (15\%) with a 4-fold exaggeration of effects on average (Type M error rates = 4.4). Notably, publication bias reduced power from 23\% to 15\% and increased type M error rates from 2.7 to 4.4 because it creates a non-random sample of effect size evidence. The sign errors of effect sizes (Type S error) increased from 5\% to 8\% because of publication bias. Our research provides clear evidence that many published ecological and evolutionary findings are inflated. Our results highlight the importance of designing high-power empirical studies (e.g., via collaborative team science), promoting and encouraging replication studies, testing and correcting for publication bias in meta-analyses, and adopting open and transparent research practices, such as (pre)registration, data- and code-sharing, and transparent reporting.},
	number = {1},
	urldate = {2023-09-03},
	journal = {BMC Biology},
	author = {Yang, Yefeng and Sánchez-Tójar, Alfredo and O’Dea, Rose E. and Noble, Daniel W. A. and Koricheva, Julia and Jennions, Michael D. and Parker, Timothy H. and Lagisz, Malgorzata and Nakagawa, Shinichi},
	month = apr,
	year = {2023},
	keywords = {Open science, Generalizability, Many labs, Meta-research, P-hacking, Questionable research practices, Registered report, Replicability, Reproducibility, Selective reporting, Transparency},
	pages = {71},
}

@article{gomes_why_2022-1,
	title = {Why don't we share data and code? {Perceived} barriers and benefits to public archiving practices},
	volume = {289},
	shorttitle = {Why don't we share data and code?},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2022.1113},
	doi = {10.1098/rspb.2022.1113},
	abstract = {The biological sciences community is increasingly recognizing the value of open, reproducible and transparent research practices for science and society at large. Despite this recognition, many researchers fail to share their data and code publicly. This pattern may arise from knowledge barriers about how to archive data and code, concerns about its reuse, and misaligned career incentives. Here, we define, categorize and discuss barriers to data and code sharing that are relevant to many research fields. We explore how real and perceived barriers might be overcome or reframed in the light of the benefits relative to costs. By elucidating these barriers and the contexts in which they arise, we can take steps to mitigate them and align our actions with the goals of open science, both as individual scientists and as a scientific community.},
	number = {1987},
	urldate = {2023-08-31},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Gomes, Dylan G. E. and Pottier, Patrice and Crystal-Ornelas, Robert and Hudgins, Emma J. and Foroughirad, Vivienne and Sánchez-Reyes, Luna L. and Turba, Rachel and Martinez, Paula Andrea and Moreau, David and Bertram, Michael G. and Smout, Cooper A. and Gaynor, Kaitlyn M.},
	month = nov,
	year = {2022},
	keywords = {code reuse‌, data reuse, data science, open science, reproducibility, transparency},
	pages = {20221113},
	annote = {Publisher: Royal Society},
	annote = {Publisher: Royal Society},
}

@article{simmons_preregistration_2021-1,
	title = {Pre‐registration: {Why} and {How}},
	volume = {31},
	issn = {1057-7408, 1532-7663},
	shorttitle = {Pre‐registration},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jcpy.1208},
	doi = {10.1002/jcpy.1208},
	language = {en},
	number = {1},
	urldate = {2023-07-31},
	journal = {Journal of Consumer Psychology},
	author = {Simmons, Joseph and Nelson, Leif and Simonsohn, Uri},
	month = jan,
	year = {2021},
	pages = {151--162},
}

@article{carroll_operationalizing_2021-1,
	title = {Operationalizing the {CARE} and {FAIR} {Principles} for {Indigenous} data futures},
	volume = {8},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-021-00892-0},
	doi = {10.1038/s41597-021-00892-0},
	language = {en},
	number = {1},
	urldate = {2022-01-28},
	journal = {Scientific Data},
	author = {Carroll, Stephanie Russo and Herczog, Edit and Hudson, Maui and Russell, Keith and Stall, Shelley},
	month = dec,
	year = {2021},
	pages = {108},
}

@article{smaldino_natural_2016-1,
	title = {The natural selection of bad science},
	volume = {3},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.160384},
	doi = {10.1098/rsos.160384},
	abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
	language = {en},
	number = {9},
	urldate = {2022-09-05},
	journal = {Royal Society Open Science},
	author = {Smaldino, Paul E. and McElreath, Richard},
	month = sep,
	year = {2016},
	pages = {160384},
}

@article{nosek_preregistration_2019-1,
	title = {Preregistration {Is} {Hard}, {And} {Worthwhile}},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319301846},
	doi = {10.1016/j.tics.2019.07.009},
	language = {en},
	number = {10},
	urldate = {2022-06-20},
	journal = {Trends in Cognitive Sciences},
	author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and van ’t Veer, Anna E. and Vazire, Simine},
	month = oct,
	year = {2019},
	pages = {815--818},
}

@article{oboyle_chrysalis_2017-1,
	title = {The {Chrysalis} {Effect}: {How} {Ugly} {Initial} {Results} {Metamorphosize} {Into} {Beautiful} {Articles}},
	volume = {43},
	issn = {0149-2063, 1557-1211},
	shorttitle = {The {Chrysalis} {Effect}},
	url = {http://journals.sagepub.com/doi/10.1177/0149206314527133},
	doi = {10.1177/0149206314527133},
	abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the “Chrysalis Effect.”},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {Journal of Management},
	author = {O’Boyle, Ernest Hugh and Banks, George Christopher and Gonzalez-Mulé, Erik},
	month = feb,
	year = {2017},
	pages = {376--399},
}

@article{thibault_rigour_2022-1,
	title = {Rigour and reproducibility in {Canadian} research: call for a coordinated approach},
	volume = {7},
	issn = {2371-1671},
	shorttitle = {Rigour and reproducibility in {Canadian} research},
	url = {https://facetsjournal.com/doi/10.1139/facets-2021-0162},
	doi = {10.1139/facets-2021-0162},
	abstract = {Shortcomings in the rigour and reproducibility of research have become well-known issues and persist despite repeated calls for improvement. A coordinated effort among researchers, institutions, funders, publishers, learned societies, and regulators may be the most effective way of tackling these issues. The UK Reproducibility Network (UKRN) has fostered collaboration across various stakeholders in research and are creating the infrastructure necessary to advance rigorous and reproducible research practices across the United Kingdom. Other Reproducibility Networks, modelled on UKRN, are now emerging in other countries. Canada could benefit from a comparable network to unify the voices around research quality and maximize the value of Canadian research.},
	language = {en},
	urldate = {2022-08-23},
	journal = {FACETS},
	author = {Thibault, Robert T. and Munafò, Marcus R. and Moher, David},
	editor = {Taylor, Iain E.P.},
	month = jan,
	year = {2022},
	pages = {18--24},
}

@article{button_power_2013-1,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Power failure},
	url = {http://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
	language = {en},
	number = {5},
	urldate = {2020-03-17},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	pages = {365--376},
}

@article{clark_reply_2020-1,
	title = {Reply to: {Methods} matter in repeating ocean acidification studies},
	volume = {586},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Reply to},
	url = {http://www.nature.com/articles/s41586-020-2804-9},
	doi = {10.1038/s41586-020-2804-9},
	language = {en},
	number = {7830},
	urldate = {2021-09-02},
	journal = {Nature},
	author = {Clark, Timothy D. and Raby, Graham D. and Roche, Dominique G. and Binning, Sandra A. and Speers-Roesch, Ben and Jutfelt, Fredrik and Sundin, Josefin},
	month = oct,
	year = {2020},
	pages = {E25--E27},
}

@article{odea_towards_2021-1,
	title = {Towards open, reliable, and transparent ecology and evolutionary biology},
	volume = {19},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-021-01006-3},
	doi = {10.1186/s12915-021-01006-3},
	abstract = {Unreliable research programmes waste funds, time, and even the lives of the organisms we seek to help and understand. Reducing this waste and increasing the value of scientific evidence require changing the actions of both individual researchers and the institutions they depend on for employment and promotion. While ecologists and evolutionary biologists have somewhat improved research transparency over the past decade (e.g. more data sharing), major obstacles remain. In this commentary, we lift our gaze to the horizon to imagine how researchers and institutions can clear the path towards more credible and effective research programmes.},
	number = {1},
	urldate = {2021-04-09},
	journal = {BMC Biology},
	author = {O’Dea, Rose E. and Parker, Timothy H. and Chee, Yung En and Culina, Antica and Drobniak, Szymon M. and Duncan, David H. and Fidler, Fiona and Gould, Elliot and Ihle, Malika and Kelly, Clint D. and Lagisz, Malgorzata and Roche, Dominique G. and Sánchez-Tójar, Alfredo and Wilkinson, David P. and Wintle, Bonnie C. and Nakagawa, Shinichi},
	month = apr,
	year = {2021},
	pages = {68},
}

@article{desjardins-proulx_case_2013-1,
	title = {The {Case} for {Open} {Preprints} in {Biology}},
	volume = {11},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001563},
	doi = {10.1371/journal.pbio.1001563},
	language = {en},
	number = {5},
	urldate = {2020-10-12},
	journal = {PLoS Biology},
	author = {Desjardins-Proulx, Philippe and White, Ethan P. and Adamson, Joel J. and Ram, Karthik and Poisot, Timothée and Gravel, Dominique},
	month = may,
	year = {2013},
	pages = {e1001563},
}

@article{hampton_tao_2015-1,
	title = {The {Tao} of open science for ecology},
	volume = {6},
	issn = {2150-8925},
	url = {http://doi.wiley.com/10.1890/ES14-00402.1},
	doi = {10.1890/ES14-00402.1},
	language = {en},
	number = {7},
	urldate = {2020-10-12},
	journal = {Ecosphere},
	author = {Hampton, Stephanie E. and Anderson, Sean S. and Bagby, Sarah C. and Gries, Corinna and Han, Xueying and Hart, Edmund M. and Jones, Matthew B. and Lenhardt, W. Christopher and MacDonald, Andrew and Michener, William K. and Mudge, Joe and Pourmokhtarian, Afshin and Schildhauer, Mark P. and Woo, Kara H. and Zimmerman, Naupaka},
	month = jul,
	year = {2015},
	pages = {art120},
}

@article{nosek_promoting_2015-1,
	title = {Promoting an open research culture},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab2374},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	urldate = {2020-10-07},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pages = {1422--1425},
}

@article{nosek_preregistration_2018-1,
	title = {The preregistration revolution},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	language = {en},
	number = {11},
	urldate = {2020-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	pages = {2600--2606},
}

@article{fanelli_positive_2010-1,
	title = {“{Positive}” {Results} {Increase} {Down} the {Hierarchy} of the {Sciences}},
	volume = {5},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010068},
	doi = {10.1371/journal.pone.0010068},
	abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
	language = {en},
	number = {4},
	urldate = {2020-10-06},
	journal = {PLOS ONE},
	author = {Fanelli, Daniele},
	month = apr,
	year = {2010},
	keywords = {Forecasting, Mental health and psychiatry, Physical sciences, Scientists, Social psychology, Social research, Social sciences, Sociology},
	pages = {e10068},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{parker_transparency_2016-1,
	title = {Transparency in {Ecology} and {Evolution}: {Real} {Problems}, {Real} {Solutions}},
	volume = {31},
	issn = {0169-5347},
	shorttitle = {Transparency in {Ecology} and {Evolution}},
	url = {https://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(16)30095-7},
	doi = {10.1016/j.tree.2016.07.002},
	abstract = {Evidence suggests that insufficient transparency is a problem across much of ecology and evolution. Results and methods are often reported in insufficient detail or go entirely unreported. Further, these unreported results are often a biased subset, thus substantially hampering interpretation and meta-analysis. Journals and other institutions, such as funding agencies, influence researchers’ decisions about disseminating results. There is a movement across empirical disciplines, including ecology and evolution, to shape institutional policies to better promote transparency. Institutions can promote transparency by requiring or encouraging more disclosure, as with the now-familiar data archiving, or by developing an incentive structure promoting disclosure, such as preregistration of studies and analysis plans. To make progress scientists need to know what other researchers have found and how they found it. However, transparency is often insufficient across much of ecology and evolution. Researchers often fail to report results and methods in detail sufficient to permit interpretation and meta-analysis, and many results go entirely unreported. Further, these unreported results are often a biased subset. Thus the conclusions we can draw from the published literature are themselves often biased and sometimes might be entirely incorrect. Fortunately there is a movement across empirical disciplines, and now within ecology and evolution, to shape editorial policies to better promote transparency. This can be done by either requiring more disclosure by scientists or by developing incentives to encourage disclosure.},
	language = {English},
	number = {9},
	urldate = {2020-09-19},
	journal = {Trends in Ecology \& Evolution},
	author = {Parker, Timothy H. and Forstmeier, Wolfgang and Koricheva, Julia and Fidler, Fiona and Hadfield, Jarrod D. and Chee, Yung En and Kelly, Clint D. and Gurevitch, Jessica and Nakagawa, Shinichi},
	month = sep,
	year = {2016},
	pmid = {27461041},
	keywords = {P-hacking, confirmation bias, inflated effect size, preregistration, replication, selective reporting},
	pages = {711--719},
	annote = {Publisher: Elsevier},
	annote = {Publisher: Elsevier},
}

@article{roche_public_2015-1,
	title = {Public {Data} {Archiving} in {Ecology} and {Evolution}: {How} {Well} {Are} {We} {Doing}?},
	volume = {13},
	issn = {1545-7885},
	shorttitle = {Public {Data} {Archiving} in {Ecology} and {Evolution}},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002295},
	doi = {10.1371/journal.pbio.1002295},
	abstract = {Policies that mandate public data archiving (PDA) successfully increase accessibility to data underlying scientific publications. However, is the data quality sufficient to allow reuse and reanalysis? We surveyed 100 datasets associated with nonmolecular studies in journals that commonly publish ecological and evolutionary research and have a strong PDA policy. Out of these datasets, 56\% were incomplete, and 64\% were archived in a way that partially or entirely prevented reuse. We suggest that cultural shifts facilitating clearer benefits to authors are necessary to achieve high-quality PDA and highlight key guidelines to help authors increase their data’s reuse potential and compliance with journal data policies.},
	language = {en},
	number = {11},
	urldate = {2020-09-19},
	journal = {PLOS Biology},
	author = {Roche, Dominique G. and Kruuk, Loeske E. B. and Lanfear, Robert and Binning, Sandra A.},
	month = nov,
	year = {2015},
	keywords = {Reproducibility, Archives, Computer software, Evolutionary biology, Metadata, Public policy, Science policy, Scientific publishing},
	pages = {e1002295},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{powers_open_2019-1,
	title = {Open science, reproducibility, and transparency in ecology},
	volume = {29},
	copyright = {© 2018 The Authors Ecological Applications published by Wiley Periodicals, Inc. on behalf of Ecological Society of America},
	issn = {1939-5582},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.1822},
	doi = {10.1002/eap.1822},
	abstract = {Reproducibility is a key tenet of the scientific process that dictates the reliability and generality of results and methods. The complexities of ecological observations and data present novel challenges in satisfying needs for reproducibility and also transparency. Ecological systems are dynamic and heterogeneous, interacting with numerous factors that sculpt natural history and that investigators cannot completely control. Observations may be highly dependent on spatial and temporal context, making them very difficult to reproduce, but computational reproducibility can still be achieved. Computational reproducibility often refers to the ability to produce equivalent analytical outcomes from the same data set using the same code and software as the original study. When coded workflows are shared, authors and editors provide transparency for readers and allow other researchers to build directly and efficiently on primary work. These qualities may be especially important in ecological applications that have important or controversial implications for science, management, and policy. Expectations for computational reproducibility and transparency are shifting rapidly in the sciences. In this work, we highlight many of the unique challenges for ecology along with practical guidelines for reproducibility and transparency, as ecologists continue to participate in the stewardship of critical environmental information and ensure that research methods demonstrate integrity.},
	language = {en},
	number = {1},
	urldate = {2020-09-21},
	journal = {Ecological Applications},
	author = {Powers, Stephen M. and Hampton, Stephanie E.},
	year = {2019},
	keywords = {replicability, data science, open science, collaborative tools, data policy, ecoinformatics, ecosystem, environmental science, repeatability, reproducible, transparent, workflows},
	pages = {e01822},
	annote = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822},
	annote = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822},
}

@techreport{marwick_packaging_2018-1,
	type = {preprint},
	title = {Packaging data analytical work reproducibly using {R} (and friends)},
	url = {https://peerj.com/preprints/3192v2},
	abstract = {Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
	language = {en},
	urldate = {2020-09-24},
	institution = {PeerJ Preprints},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	month = mar,
	year = {2018},
	doi = {10.7287/peerj.preprints.3192v2},
}

@article{kelly_rate_2019-1,
	title = {Rate and success of study replication in ecology and evolution},
	volume = {7},
	issn = {2167-8359},
	url = {https://peerj.com/articles/7654},
	doi = {10.7717/peerj.7654},
	abstract = {The recent replication crisis has caused several scientific disciplines to self-reflect on the frequency with which they replicate previously published studies and to assess their success in such endeavours. The rate of replication, however, has yet to be assessed for ecology and evolution. Here, I survey the open-access ecology and evolution literature to determine how often ecologists and evolutionary biologists replicate, or at least claim to replicate, previously published studies. I found that approximately 0.023\% of ecology and evolution studies are described by their authors as replications. Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of “replication failure” by the authors.},
	language = {en},
	urldate = {2020-09-19},
	journal = {PeerJ},
	author = {Kelly, Clint D.},
	month = sep,
	year = {2019},
	pages = {e7654},
	annote = {Publisher: PeerJ Inc.},
	annote = {Publisher: PeerJ Inc.},
}

@article{jamieson_signaling_2020-1,
	title = {Signaling the trustworthiness of science},
	volume = {117},
	number = {14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jamieson, Kathleen Hall and McNutt, Marcia and Kiermer, Veronique and Sever, Richard},
	year = {2020},
	pages = {8212--8212},
	annote = {Publisher: NATL ACAD SCIENCES 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA},
	annote = {Publisher: NATL ACAD SCIENCES 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA},
}

@article{ioannidis_why_2005-1,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2020-09-29},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	keywords = {Cancer risk factors, Finance, Genetic epidemiology, Genetics of disease, Metaanalysis, Randomized controlled trials, Research design, Schizophrenia},
	pages = {e124},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{fraser_questionable_2018-1,
	title = {Questionable research practices in ecology and evolution},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
	doi = {10.1371/journal.pone.0200303},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	language = {en},
	number = {7},
	urldate = {2020-09-19},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	month = jul,
	year = {2018},
	keywords = {Evolutionary biology, Behavioral ecology, Community ecology, Evolutionary ecology, Evolutionary rate, Psychology, Publication ethics, Statistical data},
	pages = {e0200303},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{fraser_role_2020-1,
	title = {The role of replication studies in ecology},
	volume = {10},
	copyright = {© 2020 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
	issn = {2045-7758},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.6330},
	doi = {10.1002/ece3.6330},
	abstract = {Recent large-scale projects in other disciplines have shown that results often fail to replicate when studies are repeated. The conditions contributing to this problem are also present in ecology, but there have not been any equivalent replication projects. Here, we survey ecologists' understanding of and opinions about replication studies. The majority of ecologists in our sample considered replication studies to be important (97\%), not prevalent enough (91\%), worth funding even given limited resources (61\%), and suitable for publication in all journals (62\%). However, there is a disconnect between this enthusiasm and the prevalence of direct replication studies in the literature which is much lower (0.023\%: Kelly 2019) than our participants' median estimate of 10\%. This may be explained by the obstacles our participants identified including the difficulty of conducting replication studies and of funding and publishing them. We conclude by offering suggestions for how replications could be better integrated into ecological research.},
	language = {en},
	number = {12},
	urldate = {2020-09-19},
	journal = {Ecology and Evolution},
	author = {Fraser, Hannah and Barnett, Ashley and Parker, Timothy H. and Fidler, Fiona},
	year = {2020},
	keywords = {replicability, open science, reproducibility, transparency, repeatability, conceptual replication, direct replication, generalizability},
	pages = {5197--5207},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.6330},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.6330},
}

@article{bafeta_ten_2020-1,
	title = {Ten simple rules for open human health research},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007846},
	doi = {10.1371/journal.pcbi.1007846},
	language = {en},
	number = {9},
	urldate = {2020-09-27},
	journal = {PLOS Computational Biology},
	author = {Bafeta, Aïda and Bobe, Jason and Clucas, Jon and Gonsalves, Pattie Pramila and Gruson-Daniel, Célya and Hudson, Kathy L. and Klein, Arno and Krishnakumar, Anirudh and McCollister-Slipp, Anna and Lindner, Ariel B. and Misevic, Dusan and Naslund, John A. and Nebeker, Camille and Nikolaidis, Aki and Pasquetto, Irene and Sanchez, Gabriela and Schapira, Matthieu and Scheininger, Tohar and Schoeller, Félix and Heinsfeld, Anibal Sólon and Taddei, François},
	month = sep,
	year = {2020},
	keywords = {Open science, Reproducibility, Scientists, Computer software, Research design, Medical risk factors, Research ethics, Social media},
	pages = {e1007846},
	annote = {Publisher: Public Library of Science},
	annote = {Publisher: Public Library of Science},
}

@article{culumber_widespread_2019-1,
	title = {Widespread {Biases} in {Ecological} and {Evolutionary} {Studies}},
	volume = {69},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/69/8/631/5523265},
	doi = {10.1093/biosci/biz063},
	abstract = {There has been widespread discussion of biases in the sciences. The extent of most forms of bias has scarcely been confronted with rigorous data. In the present article, we evaluated the potential for geographic, taxonomic, and citation biases in publications between temperate and tropical systems for nine broad topics in ecology and evolutionary biology. Across 1,800 papers sampled from 60,000 peer-reviewed, empirical studies, we found consistent patterns of bias in the form of increased numbers of studies in temperate systems. Tropical studies were nearly absent from some topics. Furthermore, there were strong taxonomic biases across topics and geographic regions, as well as evidence for citation biases in many topics. Our results indicate a strong geographic imbalance in publishing patterns and among different taxonomic groups across a wide range of topics. The task ahead is to address what these biases mean and how they influence the state of our knowledge in ecology and evolution.},
	language = {en},
	number = {8},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Culumber, Zachary W. and Anaya-Rojas, Jaime M. and Booker, William W. and Hooks, Alexandra P. and Lange, Elizabeth C. and Pluer, Benjamin and Ramírez-Bullón, Natali and Travis, Joseph},
	month = aug,
	year = {2019},
	pages = {631--640},
	annote = {Publisher: Oxford Academic},
	annote = {Publisher: Oxford Academic},
}

@misc{noauthor_replication_nodate-1,
	title = {Replication {Failures} {Highlight} {Biases} in {Ecology} and {Evolution} {Science}},
	url = {https://www.the-scientist.com/features/replication-failures-highlight-biases-in-ecology-and-evolution-science-64475},
	abstract = {As robust efforts fail to reproduce findings of influential zebra finch studies from the 1980s, scientists discuss ways to reduce bias in such research.},
	language = {en},
	urldate = {2020-09-30},
	annote = {Publication Title: The Scientist Magazine®},
}

@article{yang_large-scale_2024-1,
	title = {A large-scale in silico replication of ecological and evolutionary studies},
	volume = {8},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-024-02530-5},
	doi = {10.1038/s41559-024-02530-5},
	abstract = {Abstract Despite the growing concerns about the replicability of ecological and evolutionary studies, no results exist from a field-wide replication project. We conduct a large-scale in silico replication project, leveraging cutting-edge statistical methodologies. Replicability is 30\%–40\% for studies with marginal statistical significance in the absence of selective reporting, whereas the replicability of studies presenting ‘strong’ evidence against the null hypothesis H 0 is {\textbackslash}textbackslashtextgreater70\%. The former requires a sevenfold larger sample size to reach the latter’s replicability. We call for a change in planning, conducting and publishing research towards a transparent, credible and replicable ecology and evolution.},
	language = {en},
	number = {12},
	urldate = {2025-02-11},
	journal = {Nature Ecology \& Evolution},
	author = {Yang, Yefeng and Van Zwet, Erik and Ignatiadis, Nikolaos and Nakagawa, Shinichi},
	month = sep,
	year = {2024},
	pages = {2179--2183},
}

@article{kimmel_empirical_2023-1,
	title = {Empirical evidence of widespread exaggeration bias and selective reporting in ecology},
	volume = {7},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-02144-3},
	doi = {10.1038/s41559-023-02144-3},
	language = {en},
	number = {9},
	urldate = {2025-04-06},
	journal = {Nature Ecology \& Evolution},
	author = {Kimmel, Kaitlin and Avolio, Meghan L. and Ferraro, Paul J.},
	month = aug,
	year = {2023},
	pages = {1525--1536},
}

@article{buxton_avoiding_2021-1,
	title = {Avoiding wasted research resources in conservation science},
	volume = {3},
	issn = {2578-4854, 2578-4854},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/csp2.329},
	doi = {10.1111/csp2.329},
	language = {en},
	number = {2},
	urldate = {2022-01-17},
	journal = {Conservation Science and Practice},
	author = {Buxton, Rachel T. and Nyboer, Elizabeth A. and Pigeon, Karine E. and Raby, Graham D. and Rytwinski, Trina and Gallagher, Austin J. and Schuster, Richard and Lin, Hsien‐Yung and Fahrig, Lenore and Bennett, Joseph R. and Cooke, Steven J. and Roche, Dominique G.},
	month = feb,
	year = {2021},
}

@article{nakagawa_finding_2024-1,
	title = {Finding the right power balance: {Better} study design and collaboration can reduce dependence on statistical power},
	volume = {22},
	issn = {1545-7885},
	shorttitle = {Finding the right power balance},
	url = {https://dx.plos.org/10.1371/journal.pbio.3002423},
	doi = {10.1371/journal.pbio.3002423},
	abstract = {Power analysis currently dominates sample size determination for experiments, particularly in grant and ethics applications. Yet, this focus could paradoxically result in suboptimal study design because publication biases towards studies with the largest effects can lead to the overestimation of effect sizes. In this Essay, we propose a paradigm shift towards better study designs that focus less on statistical power. We also advocate for (pre)registration and obligatory reporting of all results (regardless of statistical significance), better facilitation of team science and multi-institutional collaboration that incorporates heterogenization, and the use of prospective and living meta-analyses to generate generalizable results. Such changes could make science more effective and, potentially, more equitable, helping to cultivate better collaborations.},
	language = {en},
	number = {1},
	urldate = {2025-08-28},
	journal = {PLOS Biology},
	author = {Nakagawa, Shinichi and Lagisz, Malgorzata and Yang, Yefeng and Drobniak, Szymon M.},
	month = jan,
	year = {2024},
	pages = {e3002423},
}

@misc{noauthor_data_nodate-1,
	title = {Data management checklist},
	url = {https://www.fairdata.fi/en/data-management-checklist/},
	abstract = {In order to make research more transparent, verifiable, replicable or reproducible, one should be able to provide the data supporting the findings. Many funding bodies nowadays require that a research…},
	language = {en-GB},
	urldate = {2023-09-21},
	annote = {Publication Title: Fairdata},
}

@article{sholler_enforcing_2019-1,
	title = {Enforcing public data archiving policies in academic publishing: {A} study of ecology journals},
	volume = {6},
	issn = {2053-9517, 2053-9517},
	shorttitle = {Enforcing public data archiving policies in academic publishing},
	url = {http://journals.sagepub.com/doi/10.1177/2053951719836258},
	doi = {10.1177/2053951719836258},
	abstract = {To improve the quality and efficiency of research, groups within the scientific community seek to exploit the value of data sharing. Funders, institutions, and specialist organizations are developing and implementing strategies to encourage or mandate data sharing within and across disciplines, with varying degrees of success. Academic journals in ecology and evolution have adopted several types of public data archiving policies requiring authors to make data underlying scholarly manuscripts freely available. The effort to increase data sharing in the sciences is one part of a broader “data revolution” that has prompted discussion about a paradigm shift in scientific research. Yet anecdotes from the community and studies evaluating data availability suggest that these policies have not obtained the desired effects, both in terms of quantity and quality of available datasets. We conducted a qualitative, interview-based study with journal editorial staff and other stakeholders in the academic publishing process to examine how journals enforce data archiving policies. We specifically sought to establish who editors and other stakeholders perceive as responsible for ensuring data completeness and quality in the peer review process. Our analysis revealed little consensus with regard to how data archiving policies should be enforced and who should hold authors accountable for dataset submissions. Themes in interviewee responses included hopefulness that reviewers would take the initiative to review datasets and trust in authors to ensure the completeness and quality of their datasets. We highlight problematic aspects of these thematic responses and offer potential starting points for improvement of the public data archiving process.},
	language = {en},
	number = {1},
	urldate = {2022-01-17},
	journal = {Big Data \& Society},
	author = {Sholler, Dan and Ram, Karthik and Boettiger, Carl and Katz, Daniel S},
	month = jan,
	year = {2019},
	pages = {205395171983625},
}

@article{ettinger_guide_2022-1,
	title = {A guide to preprinting for early-career researchers},
	volume = {11},
	issn = {2046-6390},
	url = {https://journals.biologists.com/bio/article/11/7/bio059310/276073/A-guide-to-preprinting-for-early-career},
	doi = {10.1242/bio.059310},
	abstract = {ABSTRACT The use of preprints, research manuscripts shared publicly before completing the traditional peer-review process, is becoming a more common practice among life science researchers. Early-career researchers (ECRs) benefit from posting preprints as they are shareable, citable, and prove productivity. However, preprinting a manuscript involves a discussion among all co-authors, and ECRs are often not the decision-makers. Therefore, ECRs may find themselves in situations where they are interested in depositing a preprint but are unsure how to approach their co-authors or advisor about preprinting. Leveraging our own experiences as ECRs, and feedback from the research community, we have constructed a guide for ECRs who are considering preprinting to enable them to take ownership over the process and to raise awareness about preprinting options. We hope that this guide helps ECRs to initiate conversations about preprinting with co-authors and encourage them to preprint their future research.},
	language = {en},
	number = {7},
	urldate = {2023-09-21},
	journal = {Biology Open},
	author = {Ettinger, Cassandra L. and Sadanandappa, Madhumala K. and Görgülü, Kıvanç and Coghlan, Karen L. and Hallenbeck, Kenneth K. and Puebla, Iratxe},
	month = jul,
	year = {2022},
	pages = {bio059310},
}

@article{blastland_five_2020-1,
	title = {Five rules for evidence communication},
	volume = {587},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/d41586-020-03189-1},
	doi = {10.1038/d41586-020-03189-1},
	language = {en},
	number = {7834},
	urldate = {2023-09-21},
	journal = {Nature},
	author = {Blastland, Michael and Freeman, Alexandra L. J. and Van Der Linden, Sander and Marteau, Theresa M. and Spiegelhalter, David},
	month = nov,
	year = {2020},
	pages = {362--364},
}

@article{roche_open_2020-1,
	title = {Open government data and environmental science: a federal {Canadian} perspective},
	volume = {5},
	issn = {2371-1671},
	shorttitle = {Open government data and environmental science},
	url = {https://facetsjournal.com/doi/10.1139/facets-2020-0008},
	doi = {10.1139/facets-2020-0008},
	abstract = {Governments worldwide are releasing data into the public domain via open government data initiatives. Many such data sets are directly relevant to environmental science and complement data collected by academic researchers to address complex and challenging environmental problems. The Government of Canada is a leader in open data among Organisation for Economic Co-operation and Development countries, generating and releasing troves of valuable research data. However, achieving comprehensive and FAIR (findable, accessible, interoperable, reusable) open government data is not without its challenges. For example, identifying and understanding Canada’s international commitments, policies, and guidelines on open data can be daunting. Similarly, open data sets within the Government of Canada are spread across a diversity of repositories and portals, which may hinder their discoverability. We describe Canada’s federal initiatives promoting open government data, and outline where data sets of relevance to environmental science can be found. We summarize research data management challenges identified by the Government of Canada, plans to modernize the approach to open data for environmental science and best practices for data discoverability, access, and reuse.},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {FACETS},
	author = {Roche, Dominique G. and Granados, Monica and Austin, Claire C. and Wilson, Scott and Mitchell, Gregory M. and Smith, Paul A. and Cooke, Steven J. and Bennett, Joseph R.},
	editor = {Love, Tanzy},
	month = jan,
	year = {2020},
	pages = {942--962},
}

@article{khan_open_2022-1,
	title = {Open science failed to penetrate academic hiring practices: a cross-sectional study},
	volume = {144},
	issn = {08954356},
	shorttitle = {Open science failed to penetrate academic hiring practices},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435621004005},
	doi = {10.1016/j.jclinepi.2021.12.003},
	language = {en},
	urldate = {2023-02-17},
	journal = {Journal of Clinical Epidemiology},
	author = {Khan, Hassan and Almoli, Elham and Franco, Marina Christ and Moher, David},
	month = apr,
	year = {2022},
	pages = {136--143},
}

@article{kathawalla_easing_2021-1,
	title = {Easing {Into} {Open} {Science}: {A} {Guide} for {Graduate} {Students} and {Their} {Advisors}},
	volume = {7},
	issn = {2474-7394},
	shorttitle = {Easing {Into} {Open} {Science}},
	url = {https://online.ucpress.edu/collabra/article/doi/10.1525/collabra.18684/115927/Easing-Into-Open-Science-A-Guide-for-Graduate},
	doi = {10.1525/collabra.18684},
	abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {Collabra: Psychology},
	author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
	month = jan,
	year = {2021},
	pages = {18684},
}

@article{soeharjono_reported_2021-3,
	title = {Reported {Individual} {Costs} and {Benefits} of {Sharing} {Open} {Data} among {Canadian} {Academic} {Faculty} in {Ecology} and {Evolution}},
	volume = {71},
	issn = {0006-3568, 1525-3244},
	url = {https://academic.oup.com/bioscience/article/71/7/750/6225906},
	doi = {10.1093/biosci/biab024},
	abstract = {Abstract Open data facilitate reproducibility and accelerate scientific discovery but are hindered by perceptions that researchers bear costs and gain few benefits from publicly sharing their data, with limited empirical evidence to the contrary. We surveyed 140 faculty members working in ecology and evolution across Canada's top 20 ranked universities and found that more researchers report benefits (47.9\%) and neutral outcomes (43.6\%) than costs (21.4\%) from openly sharing data. The benefits were independent of career stage and gender, but men and early career researchers were more likely to report costs. We outline mechanisms proposed by the study participants to reduce the individual costs and increase the benefits of open data for faculty members.},
	language = {en},
	number = {7},
	urldate = {2023-02-10},
	journal = {BioScience},
	author = {Soeharjono, Sandrine and Roche, Dominique G},
	month = jul,
	year = {2021},
	pages = {750--756},
}

@article{sarafoglou_survey_2022-1,
	title = {A survey on how preregistration affects the research workflow: better science but more work},
	volume = {9},
	issn = {2054-5703},
	shorttitle = {A survey on how preregistration affects the research workflow},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211997},
	doi = {10.1098/rsos.211997},
	abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.},
	language = {en},
	number = {7},
	urldate = {2022-09-12},
	journal = {Royal Society Open Science},
	author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence and Wagenmakers, Eric-Jan and Aczel, Balazs},
	month = jul,
	year = {2022},
	pages = {211997},
}

@article{stromme_close_2022-1,
	title = {Close to open—{Factors} that hinder and promote open science in ecology research and education},
	volume = {17},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0278339},
	doi = {10.1371/journal.pone.0278339},
	abstract = {The Open Science (OS) movement is rapidly gaining traction among policy-makers, research funders, scientific journals and individual scientists. Despite these tendencies, the pace of implementing OS throughout the scientific process and across the scientific community remains slow. Thus, a better understanding of the conditions that affect OS engagement, and in particular, of how practitioners learn, use, conduct and share research openly can guide those seeking to implement OS more broadly. We surveyed participants at an OS workshop hosted by the Living Norway Ecological Data Network in 2020 to learn how they perceived OS and its importance in their research, supervision and teaching. Further, we wanted to know what OS practices they had encountered in their education and what they saw as hindering or helping their engagement with OS. The survey contained scaled-response and open-ended questions, allowing for a mixed-methods approach. We obtained survey responses from 60 out of 128 workshop participants (47\%). Responses indicated that usage and sharing of open data and code, as well as open access publication, were the most frequent OS practices. Only a minority of respondents reported having encountered OS in their formal education. A majority also viewed OS as less important in their teaching than in their research and supervisory roles. The respondents’ suggestions for what would facilitate greater OS engagement in the future included knowledge, guidelines, and resources, but also social and structural support. These are aspects that could be strengthened by promoting explicit implementation of OS practices in higher education and by nurturing a more inclusive and equitable OS culture. We argue that incorporating OS in teaching and learning of science can yield substantial benefits to the research community, student learning, and ultimately, to the wider societal objectives of science and higher education.},
	language = {en},
	number = {12},
	urldate = {2023-06-09},
	journal = {PLOS ONE},
	author = {Strømme, Christian B. and Lane, A. Kelly and Halbritter, Aud H. and Law, Elizabeth and Nater, Chloe R. and Nilsen, Erlend B. and Boutouli, Grace D. and Egelkraut, Dagmar D. and Telford, Richard J. and Vandvik, Vigdis and Cotner, Sehoya H.},
	editor = {Baccini, Alberto},
	month = dec,
	year = {2022},
	pages = {e0278339},
}

@article{corneille_beware_2023-1,
	title = {Beware ‘persuasive communication devices’ when writing and reading scientific articles},
	volume = {12},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/88654},
	doi = {10.7554/eLife.88654},
	abstract = {Authors rely on a range of devices and techniques to attract and maintain the interest of readers, and to convince them of the merits of the author’s point of view. However, when writing a scientific article, authors must use these ‘persuasive communication devices’ carefully. In particular, they must be explicit about the limitations of their work, avoid obfuscation, and resist the temptation to oversell their results. Here we discuss a list of persuasive communication devices and we encourage authors, as well as reviewers and editors, to think carefully about their use.},
	language = {en},
	urldate = {2023-05-29},
	journal = {eLife},
	author = {Corneille, Olivier and Havemann, Jo and Henderson, Emma L and IJzerman, Hans and Hussey, Ian and Orban De Xivry, Jean-Jacques and Jussim, Lee and Holmes, Nicholas P and Pilacinski, Artur and Beffara, Brice and Carroll, Harriet and Outa, Nicholas Otieno and Lush, Peter and Lotter, Leon D},
	month = may,
	year = {2023},
	pages = {e88654},
}

@article{bertram_open_2023-1,
	title = {Open science},
	volume = {33},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982223006681},
	doi = {10.1016/j.cub.2023.05.036},
	language = {en},
	number = {15},
	urldate = {2023-08-28},
	journal = {Current Biology},
	author = {Bertram, Michael G. and Sundin, Josefin and Roche, Dominique G. and Sánchez-Tójar, Alfredo and Thoré, Eli S.J. and Brodin, Tomas},
	month = aug,
	year = {2023},
	pages = {R792--R797},
}

@article{caldwell_moving_nodate-1,
	title = {Moving {Sport} and {Exercise} {Science} {Forward}: {A} {Call} for the {Adoption} of {More} {Transparent} {Research} {Practices}},
	abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR iv: https://osf.io/ preprints/sportrxiv/fxe7a/.},
	language = {en},
	author = {Caldwell, Aaron R},
	pages = {11},
}

@article{silberzahn_many_2018-1,
	title = {Many {Analysts}, {One} {Data} {Set}: {Making} {Transparent} {How} {Variations} in {Analytic} {Choices} {Affect} {Results}},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Many {Analysts}, {One} {Data} {Set}},
	url = {http://journals.sagepub.com/doi/10.1177/2515245917747646},
	doi = {10.1177/2515245917747646},
	abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 ( Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
	language = {en},
	number = {3},
	urldate = {2022-09-06},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
	month = sep,
	year = {2018},
	pages = {337--356},
}

@article{roche_closing_2021-1,
	title = {Closing the knowledge‐action gap in conservation with open science},
	issn = {0888-8892, 1523-1739},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cobi.13835},
	doi = {10.1111/cobi.13835},
	language = {en},
	urldate = {2021-12-01},
	journal = {Conservation Biology},
	author = {Roche, Dominique G. and O'Dea, Rose E. and Kerr, Kecia A. and Rytwinski, Trina and Schuster, Richard and Nguyen, Vivian M. and Young, Nathan and Bennett, Joseph R. and Cooke, Steven J.},
	month = nov,
	year = {2021},
	pages = {cobi.13835},
}

@article{filazzola_call_2022-1,
	title = {A call for clean code to effectively communicate science},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13961},
	doi = {10.1111/2041-210X.13961},
	language = {en},
	urldate = {2022-08-29},
	journal = {Methods in Ecology and Evolution},
	author = {Filazzola, Alessandro and Lortie, Cj},
	month = aug,
	year = {2022},
	pages = {2041--210X.13961},
}

@article{towse_lustre_2022-1,
	title = {{LUSTRE}: {An} online data management and student project resource},
	issn = {2693-9169},
	shorttitle = {{LUSTRE}},
	url = {https://www.tandfonline.com/doi/full/10.1080/26939169.2022.2118645},
	doi = {10.1080/26939169.2022.2118645},
	language = {en},
	urldate = {2022-09-01},
	journal = {Journal of Statistics and Data Science Education},
	author = {Towse, John and Davies, Rob and Ball, Ellie and James, Rebecca and Gooding, Ben and Ivory, Matthew},
	month = aug,
	year = {2022},
	pages = {1--14},
}

@article{williamson_ideas_2021-1,
	title = {Ideas and perspectives: {When} ocean acidification experiments are not the same, repeatability is not tested},
	volume = {18},
	issn = {1726-4189},
	shorttitle = {Ideas and perspectives},
	url = {https://bg.copernicus.org/articles/18/1787/2021/},
	doi = {10.5194/bg-18-1787-2021},
	abstract = {Abstract. Can experimental studies on the behavioural impacts of ocean acidification be trusted? That question was raised in early 2020 when a high-profile paper failed to corroborate previously observed responses of coral reef fish to high CO2. New information on the methodologies used in the “replicated” studies now provides a plausible explanation: the experimental conditions were substantially different. High sensitivity to test conditions is characteristic of ocean acidification research; such response variability shows that effects are complex, interacting with many other factors. Open-minded assessment of all research results, both negative and positive, remains the best way to develop process-based understanding. As in other fields, replication studies in ocean acidification are most likely to contribute to scientific advancement when carried out in a spirit of collaboration rather than confrontation.},
	language = {en},
	number = {5},
	urldate = {2021-09-13},
	journal = {Biogeosciences},
	author = {Williamson, Phillip and Pörtner, Hans-Otto and Widdicombe, Steve and Gattuso, Jean-Pierre},
	month = mar,
	year = {2021},
	pages = {1787--1792},
}

@article{besancon_open_2021-1,
	title = {Open science saves lives: lessons from the {COVID}-19 pandemic},
	volume = {21},
	issn = {1471-2288},
	shorttitle = {Open science saves lives},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01304-y},
	doi = {10.1186/s12874-021-01304-y},
	abstract = {Abstract In the last decade Open Science principles have been successfully advocated for and are being slowly adopted in different research communities. In response to the COVID-19 pandemic many publishers and researchers have sped up their adoption of Open Science practices, sometimes embracing them fully and sometimes partially or in a sub-optimal manner. In this article, we express concerns about the violation of some of the Open Science principles and its potential impact on the quality of research output. We provide evidence of the misuses of these principles at different stages of the scientific process. We call for a wider adoption of Open Science practices in the hope that this work will encourage a broader endorsement of Open Science principles and serve as a reminder that science should always be a rigorous process, reliable and transparent, especially in the context of a pandemic where research findings are being translated into practice even more rapidly. We provide all data and scripts at https://osf.io/renxy/ .},
	language = {en},
	number = {1},
	urldate = {2021-09-07},
	journal = {BMC Medical Research Methodology},
	author = {Besançon, Lonni and Peiffer-Smadja, Nathan and Segalas, Corentin and Jiang, Haiting and Masuzzo, Paola and Smout, Cooper and Billy, Eric and Deforet, Maxime and Leyrat, Clémence},
	month = dec,
	year = {2021},
	pages = {117},
}

@article{munday_methods_2020-1,
	title = {Methods matter in repeating ocean acidification studies},
	volume = {586},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-020-2803-x},
	doi = {10.1038/s41586-020-2803-x},
	language = {en},
	number = {7830},
	urldate = {2021-09-02},
	journal = {Nature},
	author = {Munday, Philip L. and Dixson, Danielle L. and Welch, Megan J. and Chivers, Douglas P. and Domenici, Paolo and Grosell, Martin and Heuer, Rachael M. and Jones, Geoffrey P. and McCormick, Mark I. and Meekan, Mark and Nilsson, Göran E. and Ravasi, Timothy and Watson, Sue-Ann},
	month = oct,
	year = {2020},
	pages = {E20--E24},
}

@article{culina_low_2020-1,
	title = {Low availability of code in ecology: {A} call for urgent action},
	volume = {18},
	issn = {1545-7885},
	shorttitle = {Low availability of code in ecology},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000763},
	doi = {10.1371/journal.pbio.3000763},
	abstract = {Access to analytical code is essential for transparent and reproducible research. We review the state of code availability in ecology using a random sample of 346 nonmolecular articles published between 2015 and 2019 under mandatory or encouraged code-sharing policies. Our results call for urgent action to increase code availability: only 27\% of eligible articles were accompanied by code. In contrast, data were available for 79\% of eligible articles, highlighting that code availability is an important limiting factor for computational reproducibility in ecology. Although the percentage of ecological journals with mandatory or encouraged code-sharing policies has increased considerably, from 15\% in 2015 to 75\% in 2020, our results show that code-sharing policies are not adhered to by most authors. We hope these results will encourage journals, institutions, funding agencies, and researchers to address this alarming situation.},
	language = {en},
	number = {7},
	urldate = {2021-04-09},
	journal = {PLOS Biology},
	author = {Culina, Antica and Berg, Ilona van den and Evans, Simon and Sánchez-Tójar, Alfredo},
	month = jul,
	year = {2020},
	keywords = {Reproducibility, Computer software, Conservation science, Ecology, Graphical user interfaces, Institutional funding of science, Research reporting guidelines, Theoretical ecology},
	pages = {e3000763},
	annote = {Publisher: Public Library of Science},
}

@misc{noauthor_open_2019-1,
	title = {Open {Science} {Isn}'t {Always} {Open} to {All} {Scientists}},
	url = {https://www.americanscientist.org/article/open-science-isnt-always-open-to-all-scientists},
	abstract = {Current efforts to make research more accessible and transparent can reinforce inequality within STEM professions.},
	language = {en},
	urldate = {2020-10-27},
	month = jan,
	year = {2019},
	annote = {Publication Title: American Scientist},
}

@article{rubin_p_2017-1,
	title = {Do p {Values} {Lose} {Their} {Meaning} in {Exploratory} {Analyses}? {It} {Depends} {How} {You} {Define} the {Familywise} {Error} {Rate}},
	volume = {21},
	issn = {1089-2680},
	shorttitle = {Do p {Values} {Lose} {Their} {Meaning} in {Exploratory} {Analyses}?},
	url = {https://doi.org/10.1037/gpr0000123},
	doi = {10.1037/gpr0000123},
	abstract = {Several researchers have recently argued that p values lose their meaning in exploratory analyses due to an unknown inflation of the alpha level (e.g., Nosek Wagenmakers, 2016). For this argument to be tenable, the familywise error rate must be defined in relation to the number of hypotheses that are tested in the same study or article. Under this conceptualization, the familywise error rate is usually unknowable in exploratory analyses because it is usually unclear how many hypotheses have been tested on a spontaneous basis and then omitted from the final research report. In the present article, I argue that it is inappropriate to conceptualize the familywise error rate in relation to the number of hypotheses that are tested. Instead, it is more appropriate to conceptualize familywise error in relation to the number of different tests that are conducted on the same null hypothesis in the same study. Under this conceptualization, alpha-level adjustments in exploratory analyses are (a) less necessary and (b) objectively verifiable. As a result, p values do not lose their meaning in exploratory analyses.},
	number = {3},
	urldate = {2020-10-08},
	journal = {Review of General Psychology},
	author = {Rubin, Mark},
	month = sep,
	year = {2017},
	pages = {269--275},
	annote = {Publisher: SAGE Publications Inc},
}

@article{sullivan_open_2019-1,
	title = {Open and {Reproducible} {Research} on {Open} {Science} {Framework}},
	volume = {18},
	copyright = {© 2019 John Wiley \& Sons, Inc.},
	issn = {1948-3430},
	url = {https://currentprotocols.onlinelibrary.wiley.com/doi/abs/10.1002/cpet.32},
	doi = {10.1002/cpet.32},
	abstract = {By implementing more transparent research practices, authors have the opportunity to stand out and showcase work that is more reproducible, easier to build upon, and more credible. Scientists gain by making work easier to share and maintain within their own laboratories, and the scientific community gains by making underlying data or research materials more available for confirmation or making new discoveries. The following protocol gives authors step-by-step instructions for using the free and open source Open Science Framework (OSF) to create a data management plan, preregister their study, use version control, share data and other research materials, or post a preprint for quick and easy dissemination. © 2019 by John Wiley \& Sons, Inc.},
	language = {en},
	number = {1},
	urldate = {2020-09-20},
	journal = {Current Protocols Essential Laboratory Techniques},
	author = {Sullivan, Ian and DeHaven, Alexander and Mellor, David},
	year = {2019},
	keywords = {open science, preregistration, data management, open data, reproducible research},
	pages = {e32},
	annote = {\_eprint: https://currentprotocols.onlinelibrary.wiley.com/doi/pdf/10.1002/cpet.32},
}

@article{simmons_false-positive_2011-1,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	shorttitle = {False-{Positive} {Psychology}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2020-03-17},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pages = {1359--1366},
}

@article{parker_making_2019-1,
	title = {Making conservation science more reliable with preregistration and registered reports},
	volume = {33},
	issn = {1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/cobi.13342},
	doi = {10.1111/cobi.13342},
	language = {en},
	number = {4},
	urldate = {2020-09-19},
	journal = {Conservation Biology},
	author = {Parker, Timothy and Fraser, Hannah and Nakagawa, Shinichi},
	year = {2019},
	pages = {747--750},
	annote = {\_eprint: https://conbio.onlinelibrary.wiley.com/doi/pdf/10.1111/cobi.13342},
}

@article{nichols_accumulating_2019-1,
	title = {Accumulating evidence in ecology: {Once} is not enough},
	volume = {9},
	issn = {2045-7758},
	shorttitle = {Accumulating evidence in ecology},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.5836},
	doi = {10.1002/ece3.5836},
	abstract = {Many published studies in ecological science are viewed as stand-alone investigations that purport to provide new insights into how ecological systems behave based on single analyses. But it is rare for results of single studies to provide definitive results, as evidenced in current discussions of the “reproducibility crisis” in science. The key step in science is the comparison of hypothesis-based predictions with observations, where the predictions are typically generated by hypothesis-specific models. Repeating this step allows us to gain confidence in the predictive ability of a model, and its corresponding hypothesis, and thus to accumulate evidence and eventually knowledge. This accumulation may occur via an ad hoc approach, via meta-analyses, or via a more systematic approach based on the anticipated evolution of an information state. We argue the merits of this latter approach, provide an example, and discuss implications for designing sequences of studies focused on a particular question. We conclude by discussing current data collection programs that are preadapted to use this approach and argue that expanded use would increase the rate of learning in ecology, as well as our confidence in what is learned.},
	language = {en},
	number = {24},
	urldate = {2020-09-19},
	journal = {Ecology and Evolution},
	author = {Nichols, James D. and Kendall, William L. and Boomer, Gregory Scott},
	year = {2019},
	keywords = {reproducibility, replication, Bayes theorem, ecology, evidence, information state, knowledge, science},
	pages = {13991--14004},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.5836},
}

@article{parker_empowering_2018-1,
	title = {Empowering peer reviewers with a checklist to improve transparency},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-018-0545-z},
	doi = {10.1038/s41559-018-0545-z},
	abstract = {Peer review is widely considered fundamental to maintaining the rigour of science, but it often fails to ensure transparency and reduce bias in published papers, and this systematically weakens the quality of published inferences. In part, this is because many reviewers are unaware of important questions to ask with respect to the soundness of the design and analyses, and the presentation of the methods and results; also some reviewers may expect others to be responsible for these tasks. We therefore present a reviewers’ checklist of ten questions that address these critical components. Checklists are commonly used by practitioners of other complex tasks, and we see great potential for the wider adoption of checklists for peer review, especially to reduce bias and facilitate transparency in published papers. We expect that such checklists will be well received by many reviewers.},
	language = {en},
	number = {6},
	urldate = {2020-09-19},
	journal = {Nature Ecology \& Evolution},
	author = {Parker, Timothy H. and Griffith, Simon C. and Bronstein, Judith L. and Fidler, Fiona and Foster, Susan and Fraser, Hannah and Forstmeier, Wolfgang and Gurevitch, Jessica and Koricheva, Julia and Seppelt, Ralf and Tingley, Morgan W. and Nakagawa, Shinichi},
	month = jun,
	year = {2018},
	pages = {929--935},
	annote = {Number: 6 Publisher: Nature Publishing Group},
}

@article{nilsen_exploratory_2020-1,
	title = {Exploratory and confirmatory research in the open science era},
	volume = {57},
	copyright = {© 2020 The Authors. Journal of Applied Ecology published by John Wiley \& Sons Ltd on behalf of British Ecological Society},
	issn = {1365-2664},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.13571},
	doi = {10.1111/1365-2664.13571},
	abstract = {Applied ecological research is increasingly inspired by the open science movement. However, new challenges about how we define our science when biodiversity data are being shared and re-used are not solved. Among these challenges is the risk associated with blurring the distinction between research that mainly seeks to explore patterns with no a-priori articulated hypotheses (exploratory research), and research that explicitly tests a-priori formulated hypotheses (confirmatory research). A rapid screening of a random selection of the peer-reviewed literature suggests that neither experimental protocols nor hypothesis-testing sensu stricto are common in applied ecological research. In addition, most experiments are carried out on small spatial scales, which contrast with current global policy needs and research trends towards addressing large spatial and temporal scales. This latter trend makes it unfeasible for policy to rely mainly on insights gained from experimental research. To solve fundamental local, regional and global societal challenges, we need both exploratory and confirmatory research. However, the fundamental roles that confirmatory research testing a-priori hypothesis play in establishing causal relationships need to be revaluated in applied ecological research. A clearer distinction between exploratory and confirmatory research is currently needed, and could be facilitated by allocating journal sections to different types of research; by embracing new tools offered by the open science era, such as pre-registration of hypothesis; by establishing new systems where post-hoc hypotheses emerging through exploration can also be registered for later testing; and by more broad adoption of causal inference methods that foster more structured testing of hypotheses about causal mechanisms from observational biodiversity data. Synthesis and applications. To gain the full benefits of the open science era, researchers, funding bodies and journal editors should explicitly consider approaches and incentives that encourage openness about methods and approaches, as well as value the plurality of scientific approaches needed to address questions in applied ecology and conservation science.},
	language = {en},
	number = {4},
	urldate = {2020-09-28},
	journal = {Journal of Applied Ecology},
	author = {Nilsen, Erlend B. and Bowler, Diana E. and Linnell, John D. C.},
	year = {2020},
	keywords = {open science, reproducible research, causal inference, confirmatory research, exploratory research, large-scale assessment, science philosophy},
	pages = {842--847},
	annote = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2664.13571},
}

@article{munafo_manifesto_2017-1,
	title = {A manifesto for reproducible science},
	volume = {1},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	language = {en},
	number = {1},
	urldate = {2020-03-17},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	pages = {0021},
}

@article{ives_informative_2018-1,
	title = {Informative {Irreproducibility} and the {Use} of {Experiments} in {Ecology}},
	volume = {68},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/68/10/746/5065826},
	doi = {10.1093/biosci/biy090},
	abstract = {Before taking a drug, I want tests of the drug to be reproducible. I would expect testing across women and men, all demographic and ethnic groups, and people wi},
	language = {en},
	number = {10},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Ives, Anthony R.},
	month = oct,
	year = {2018},
	pages = {746--747},
	annote = {Publisher: Oxford Academic},
}

@article{milcu_genotypic_2018-1,
	title = {Genotypic variability enhances the reproducibility of an ecological study},
	volume = {2},
	copyright = {2018 © The Author(s) 2017, under exclusive licence to Macmillan Publishers Limited, part of Springer Nature},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-017-0434-x},
	doi = {10.1038/s41559-017-0434-x},
	abstract = {Many scientific disciplines are currently experiencing a 'reproducibility crisis' because numerous scientific findings cannot be repeated consistently. A novel but controversial hypothesis postulates that stringent levels of environmental and biotic standardization in experimental studies reduce reproducibility by amplifying the impacts of laboratory-specific environmental factors not accounted for in study designs. A corollary to this hypothesis is that a deliberate introduction of controlled systematic variability (CSV) in experimental designs may lead to increased reproducibility. To test this hypothesis, we had 14 European laboratories run a simple microcosm experiment using grass (Brachypodium distachyon L.) monocultures and grass and legume (Medicago truncatula Gaertn.) mixtures. Each laboratory introduced environmental and genotypic CSV within and among replicated microcosms established in either growth chambers (with stringent control of environmental conditions) or glasshouses (with more variable environmental conditions). The introduction of genotypic CSV led to 18\% lower among-laboratory variability in growth chambers, indicating increased reproducibility, but had no significant effect in glasshouses where reproducibility was generally lower. Environmental CSV had little effect on reproducibility. Although there are multiple causes for the 'reproducibility crisis', deliberately including genetic variability may be a simple solution for increasing the reproducibility of ecological studies performed under stringently controlled environmental conditions.},
	language = {en},
	number = {2},
	urldate = {2020-09-23},
	journal = {Nature Ecology \& Evolution},
	author = {Milcu, Alexandru and Puga-Freitas, Ruben and Ellison, Aaron M. and Blouin, Manuel and Scheu, Stefan and Freschet, Grégoire T. and Rose, Laura and Barot, Sebastien and Cesarz, Simone and Eisenhauer, Nico and Girin, Thomas and Assandri, Davide and Bonkowski, Michael and Buchmann, Nina and Butenschoen, Olaf and Devidal, Sebastien and Gleixner, Gerd and Gessler, Arthur and Gigon, Agnès and Greiner, Anna and Grignani, Carlo and Hansart, Amandine and Kayler, Zachary and Lange, Markus and Lata, Jean-Christophe and Le Galliard, Jean-François and Lukac, Martin and Mannerheim, Neringa and Müller, Marina E. H. and Pando, Anne and Rotter, Paula and Scherer-Lorenzen, Michael and Seyhun, Rahme and Urban-Mead, Katherine and Weigelt, Alexandra and Zavattaro, Laura and Roy, Jacques},
	month = feb,
	year = {2018},
	pages = {279--287},
	annote = {Number: 2 Publisher: Nature Publishing Group},
}

@article{mckiernan_how_2016-1,
	title = {How open science helps researchers succeed},
	volume = {5},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/16800},
	doi = {10.7554/eLife.16800},
	abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
	language = {en},
	urldate = {2020-03-17},
	journal = {eLife},
	author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
	month = jul,
	year = {2016},
	pages = {e16800},
}

@article{grahe_open_2020-1,
	title = {Open {Science} {Promotes} {Diverse}, {Just}, and {Sustainable} {Research} and {Educational} {Outcomes}},
	volume = {19},
	issn = {1475-7257},
	url = {https://doi.org/10.1177/1475725719869164},
	doi = {10.1177/1475725719869164},
	abstract = {Open science initiatives, which are often collaborative efforts focused on making research more transparent, have experienced increasing popularity in the past decade. Open science principles of openness and transparency provide opportunities to advance diversity, justice, and sustainability by promoting diverse, just, and sustainable outcomes among both undergraduate and senior researchers. We review models that demonstrate the importance of greater diversity, justice, and sustainability in psychological science before describing how open science initiatives promote these values. Open science initiatives also promote diversity, justice, and sustainability through increased levels of inclusion and access, equitable distribution of opportunities and dissemination of knowledge, and increased sustainability stemming from increased generalizability. In order to provide an application of the concepts discussed, we offer a set of diversity, justice, and sustainability lens questions for individuals to use while assessing research projects and other organizational systems and consider concrete classroom applications for these initiatives.},
	language = {en},
	number = {1},
	urldate = {2020-09-28},
	journal = {Psychology Learning \& Teaching},
	author = {Grahe, Jon E and Cuccolo, Kelly and Leighton, Dana C and Cramblet Alvarez, Leslie D},
	month = mar,
	year = {2020},
	pages = {5--20},
	annote = {Publisher: SAGE Publications},
}

@article{gallagher_open_2020-1,
	title = {Open {Science} principles for accelerating trait-based science across the {Tree} of {Life}},
	volume = {4},
	copyright = {2020 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-020-1109-6},
	doi = {10.1038/s41559-020-1109-6},
	abstract = {Synthesizing trait observations and knowledge across the Tree of Life remains a grand challenge for biodiversity science. Species traits are widely used in ecological and evolutionary science, and new data and methods have proliferated rapidly. Yet accessing and integrating disparate data sources remains a considerable challenge, slowing progress toward a global synthesis to integrate trait data across organisms. Trait science needs a vision for achieving global integration across all organisms. Here, we outline how the adoption of key Open Science principles—open data, open source and open methods—is transforming trait science, increasing transparency, democratizing access and accelerating global synthesis. To enhance widespread adoption of these principles, we introduce the Open Traits Network (OTN), a global, decentralized community welcoming all researchers and institutions pursuing the collaborative goal of standardizing and integrating trait data across organisms. We demonstrate how adherence to Open Science principles is key to the OTN community and outline five activities that can accelerate the synthesis of trait data across the Tree of Life, thereby facilitating rapid advances to address scientific inquiries and environmental issues. Lessons learned along the path to a global synthesis of trait data will provide a framework for addressing similarly complex data science and informatics challenges.},
	language = {en},
	number = {3},
	urldate = {2020-09-28},
	journal = {Nature Ecology \& Evolution},
	author = {Gallagher, Rachael V. and Falster, Daniel S. and Maitner, Brian S. and Salguero-Gómez, Roberto and Vandvik, Vigdis and Pearse, William D. and Schneider, Florian D. and Kattge, Jens and Poelen, Jorrit H. and Madin, Joshua S. and Ankenbrand, Markus J. and Penone, Caterina and Feng, Xiao and Adams, Vanessa M. and Alroy, John and Andrew, Samuel C. and Balk, Meghan A. and Bland, Lucie M. and Boyle, Brad L. and Bravo-Avila, Catherine H. and Brennan, Ian and Carthey, Alexandra J. R. and Catullo, Renee and Cavazos, Brittany R. and Conde, Dalia A. and Chown, Steven L. and Fadrique, Belen and Gibb, Heloise and Halbritter, Aud H. and Hammock, Jennifer and Hogan, J. Aaron and Holewa, Hamish and Hope, Michael and Iversen, Colleen M. and Jochum, Malte and Kearney, Michael and Keller, Alexander and Mabee, Paula and Manning, Peter and McCormack, Luke and Michaletz, Sean T. and Park, Daniel S. and Perez, Timothy M. and Pineda-Munoz, Silvia and Ray, Courtenay A. and Rossetto, Maurizio and Sauquet, Hervé and Sparrow, Benjamin and Spasojevic, Marko J. and Telford, Richard J. and Tobias, Joseph A. and Violle, Cyrille and Walls, Ramona and Weiss, Katherine C. B. and Westoby, Mark and Wright, Ian J. and Enquist, Brian J.},
	month = mar,
	year = {2020},
	pages = {294--303},
	annote = {Number: 3 Publisher: Nature Publishing Group},
}

@article{fidler_metaresearch_2017-1,
	title = {Metaresearch for {Evaluating} {Reproducibility} in {Ecology} and {Evolution}},
	volume = {67},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/67/3/282/2900173},
	doi = {10.1093/biosci/biw159},
	abstract = {Recent replication projects in other disciplines have uncovered disturbingly low levels of reproducibility, suggesting that those research literatures may contain unverifiable claims. The conditions contributing to irreproducibility in other disciplines are also present in ecology. These include a large discrepancy between the proportion of “positive” or “significant” results and the average statistical power of empirical research, incomplete reporting of sampling stopping rules and results, journal policies that discourage replication studies, and a prevailing publish-or-perish research culture that encourages questionable research practices. We argue that these conditions constitute sufficient reason to systematically evaluate the reproducibility of the evidence base in ecology and evolution. In some cases, the direct replication of ecological research is difficult because of strong temporal and spatial dependencies, so here, we propose metaresearch projects that will provide proxy measures of reproducibility.},
	language = {en},
	number = {3},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
	month = mar,
	year = {2017},
	pages = {282--289},
	annote = {Publisher: Oxford Academic},
}

@article{forstmeier_detecting_2017-1,
	title = {Detecting and avoiding likely false-positive findings – a practical guide},
	volume = {92},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12315},
	doi = {10.1111/brv.12315},
	abstract = {Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a significant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientific rigour.},
	language = {en},
	number = {4},
	urldate = {2020-05-17},
	journal = {Biological Reviews},
	author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
	year = {2017},
	keywords = {P-hacking, confirmation bias, preregistration, replication, HARKing, hindsight bias, overfitting, power, researcher degrees of freedom, Type I error},
	pages = {1941--1968},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12315},
}

@article{fanelli_meta-assessment_2017-1,
	title = {Meta-assessment of bias in science},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1618569114},
	doi = {10.1073/pnas.1618569114},
	abstract = {Numerous biases are believed to affect the scientific literature, but their actual prevalence across disciplines is unknown. To gain a comprehensive picture of the potential imprint of bias in science, we probed for the most commonly postulated bias-related patterns and risk factors, in a large random sample of meta-analyses taken from all disciplines. The magnitude of these biases varied widely across fields and was overall relatively small. However, we consistently observed a significant risk of small, early, and highly cited studies to overestimate effects and of studies not published in peer-reviewed journals to underestimate them. We also found at least partial confirmation of previous evidence suggesting that US studies and early studies might report more extreme effects, although these effects were smaller and more heterogeneously distributed across meta-analyses and disciplines. Authors publishing at high rates and receiving many citations were, overall, not at greater risk of bias. However, effect sizes were likely to be overestimated by early-career researchers, those working in small or long-distance collaborations, and those responsible for scientific misconduct, supporting hypotheses that connect bias to situational factors, lack of mutual control, and individual integrity. Some of these patterns and risk factors might have modestly increased in intensity over time, particularly in the social sciences. Our findings suggest that, besides one being routinely cautious that published small, highly-cited, and earlier studies may yield inflated results, the feasibility and costs of interventions to attenuate biases in the literature might need to be discussed on a discipline-specific and topic-specific basis.},
	language = {en},
	number = {14},
	urldate = {2020-09-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fanelli, Daniele and Costas, Rodrigo and Ioannidis, John P. A.},
	month = apr,
	year = {2017},
	pages = {3714--3719},
}

@article{clark_ocean_2020-1,
	title = {Ocean acidification does not impair the behaviour of coral reef fishes},
	volume = {577},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1903-y},
	doi = {10.1038/s41586-019-1903-y},
	abstract = {The partial pressure of CO2 in the oceans has increased rapidly over the past century, driving ocean acidification and raising concern for the stability of marine ecosystems1–3. Coral reef fishes are predicted to be especially susceptible to end-of-century ocean acidification on the basis of several high-profile papers4,5 that have reported profound behavioural and sensory impairments—for example, complete attraction to the chemical cues of predators under conditions of ocean acidification. Here, we comprehensively and transparently show that—in contrast to previous studies—end-of-century ocean acidification levels have negligible effects on important behaviours of coral reef fishes, such as the avoidance of chemical cues from predators, fish activity levels and behavioural lateralization (left–right turning preference). Using data simulations, we additionally show that the large effect sizes and small within-group variances that have been reported in several previous studies are highly improbable. Together, our findings indicate that the reported effects of ocean acidification on the behaviour of coral reef fishes are not reproducible, suggesting that behavioural perturbations will not be a major consequence for coral reef fishes in high CO2 oceans.},
	language = {en},
	number = {7790},
	urldate = {2020-09-20},
	journal = {Nature},
	author = {Clark, Timothy D. and Raby, Graham D. and Roche, Dominique G. and Binning, Sandra A. and Speers-Roesch, Ben and Jutfelt, Fredrik and Sundin, Josefin},
	month = jan,
	year = {2020},
	pages = {370--375},
	annote = {Number: 7790 Publisher: Nature Publishing Group},
}

@article{aczel_consensus-based_2020-1,
	title = {A consensus-based transparency checklist},
	volume = {4},
	copyright = {2019 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-019-0772-6},
	doi = {10.1038/s41562-019-0772-6},
	abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
	language = {en},
	number = {1},
	urldate = {2020-09-19},
	journal = {Nature Human Behaviour},
	author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharský, Šimon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munafò, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ronán M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and Giner-Sorolla, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and de la Guardia, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
	month = jan,
	year = {2020},
	pages = {4--6},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{ali-khan_identifying_2018-1,
	title = {Identifying the challenges in implementing open science},
	volume = {2},
	issn = {2515-5059},
	url = {https://mniopenresearch.org/articles/2-5/v1},
	doi = {10.12688/mniopenres.12805.1},
	abstract = {Areas of open science (OS) policy and practice are already relatively well-advanced in several countries and sectors through the initiatives of some governments, funders, philanthropy, researchers and the community. Nevertheless, the current research and innovation system, including in the focus of this report, the life sciences, remains weighted against OS. In October 2017, thought-leaders from across the world gathered at an Open Science Leadership Forum in the Washington DC office of the Bill and Melinda Gates Foundation to share their views on what successful OS looks like. We focused on OS partnerships as this is an emerging model that aims to accelerate science and innovation. These outcomes are captured in a first meeting report: Defining Success in Open Science. On several occasions, these conversations turned to the challenges that must be addressed and new policies required to effectively and sustainably advance OS practice. Thereupon, in this report, we describe the concerns raised and what is needed to address them supplemented by our review of the literature, and suggest the stakeholder groups that may be best placed to begin to take action. It emerges that to be successful, OS will require the active engagement of all stakeholders: while the research community must develop research questions, identify partners and networks, policy communities need to create an environment that is supportive of experimentation by removing barriers. This report aims to contribute to ongoing discussions about OS and its implementation. It is also part of a step-wise process to develop and mobilize a toolkit of quantitative and qualitative indicators to assist global stakeholders in implementing high value OS collaborations. Currently in co-development through an open and international process, this set of measures will allow the generation of needed evidence on the influence of OS partnerships on research, innovation, and critical social and economic goals.},
	language = {en},
	urldate = {2020-09-28},
	journal = {MNI Open Research},
	author = {Ali-Khan, Sarah E. and Jean, Antoine and Gold, E. Richard},
	month = oct,
	year = {2018},
	pages = {5},
}

@misc{srivastava_geographical_2022,
	title = {Geographical variation in the trait-based assembly patterns of multitrophic invertebrate communities},
	copyright = {Creative Commons Zero v1.0 Universal},
	url = {https://datadryad.org/dataset/doi:10.5061/dryad.vt4b8gtv4},
	doi = {10.5061/DRYAD.VT4B8GTV4},
	abstract = {It has been argued that the mechanisms structuring ecological communities
 may be more generalizable when based on traits than on species identities.
 If so, patterns in the assembly of community-level traits along
 environmental gradients should be similar in different places in the
 world. Alternatively, geographic change in the species pool and regional
 variation in climate might result in site-specific relationships between
 community traits and local environments. These competing hypotheses are
 particularly untested for animal communities. Here we test the geographic
 constancy of trait-based assembly patterns using a widespread
 multi-trophic community: aquatic macroinvertebrates within bromeliads. We
 used data on 615 invertebrate taxa from 1656 bromeliads in 26 field sites
 from Mexico to Argentina. We summarized invertebrate traits with four
 orthogonal axes, and used these trait axes to examine trait convergence
 and divergence assembly patterns along three environmental gradients:
 detrital biomass and water volume in bromeliads, and canopy cover over
 bromeliads. We found no overall signal of trait-based assembly patterns
 along any of the environmental gradients. However, individual sites did
 show trait convergence along detrital and water gradients, and we built
 predictive models to explore these site differences. Sites that showed
 trait convergence along detrital gradients were all north of the Northern
 Andes. This geographic pattern may be related to phylogeographic
 differences in bromeliad morphology. Bromeliads with low detritus were
 dominated by detritivorous collectors and filter feeders, where those with
 high detritus had more sclerotized and predatory invertebrates. Sites that
 showed the strongest trait convergence along gradients in bromeliad water
 were in regions with seasonal precipitation. In such sites, bromeliads
 with low water were dominated by soft-bodied, benthic invertebrates with
 simple life cycles. In less seasonal sites, traits associated with
 short-term desiccation resistance, such as hard exoskeletons, were more
 important. In summary, we show that there are strong geographic effects on
 the trait-based assembly patterns of this invertebrate community, driven
 by the biogeography of their foundational plant species as well as by
 regional climate. We suggest that inclusion of biogeography and climate in
 trait-based community ecology could help make it a truly general theory.
 (excerpted from Srivastava, DS et al. 2022. Geographical variation in the
 trait-based assembly patterns of multitrophic invertebrate communities.
 Functional Ecology)},
	language = {en},
	urldate = {2025-09-11},
	publisher = {Dryad},
	author = {Srivastava, Diane S. and MacDonald, A. Andrew M. and Pillar, Valério D. and Kratina, Pavel and Debastiani, Vanderlei J. and Guzman, Laura Melissa and Trzcinski, M. Kurtis and Dézerald, Olivier and Barberis, Ignacio M. and de Omena, Paula M. and Romero, Gustavo Q. and Ospina Bautista, Fabiola and Marino, Nicholas A. C. and Leroy, Céline and Farjalla, Vinicius F. and Richardson, Barbara A. and Gonçalves, Ana Z. and Corbara, Bruno and Petermann, Jana S. and Richardson, Michael J. and Melnychuk, Michael C. and Jocqué, Merlijn and Ngai, Jacqueline T. and Talaga, Stanislas and Piccoli, Gustavo C. O. and Montero, Guillermo and Kirby, Kathryn R. and Starzomski, Brian M. and Céréghino, Régis},
	month = may,
	year = {2022},
	note = {Artwork Size: 2615970 bytes
Pages: 2615970 bytes},
	keywords = {aquatic insect, bromeliad, community convergence, community divergence, environmental gradient, FOS: Biological sciences, functional biogeography, geographic contingency, habitat filtering, macroinvertebrate, Neotropics, phytotelmata, SYNCSA, tank bromeliad, trait, trait-based assembly, trait-environment matching},
	annote = {Methods
Bromeliad and macroinvertebrate data We compiled data on the aquatic
 macroinvertebrates in tank bromeliads previously sampled in 26 different
 sites throughout the natural distribution of the tank bromeliads
 (Bromeliaceae family). Field sites were distinct from each other in space,
 elevation and the species composition of invertebrate communities. For
 every bromeliad, all water and detritus contained in the plant were
 removed, either by dissecting the plant or by pipetting. The water and
 detritus were examined for aquatic macroinvertebrates in small
 size-fractioned aliquots in white trays. Macroinvertebrates were
 identified to morphospecies in the field, and subsequently to the lowest
 possible taxonomic level. The detritus was oven-dried and weighed to
 determine dry mass. Bromeliads were sampled across a range of habitats,
 from exposed restinga (coastal sand-based shrub habitat) in Brazil to
 cloud forests on Caribbean mountaintops to rainforests in Central America.
 As no bromeliad genus was found in all field sites, we sampled the most
 common genera in sites. Our analysis included three environmental
 characteristics of each sampled bromeliad: (1) the dry mass of all
 detritus in the bromeliad (hereafter “detritus”, measured in grams); (2)
 the volume of standing water in each bromeliad on the day of sampling
 (hereafter “water”, measured in mL); (3) the openness of the canopy above
 the bromeliad (hereafter “canopy”, a binary variable with 1 = open canopy
 and 0 = closed canopy). These variables were chosen because previous
 site-specific research had established that they were important
 environmental drivers of community structure and function. Trait data Our
 analysis considers interspecific, but not intraspecific, differences in
 traits. In Céréghino et al. (2018), each bromeliad invertebrate
 morphospecies was scored in terms of twelve traits: aquatic developmental
 stage, body form, maximum body size, cohort production interval, dispersal
 mode, food, feeding group, locomotion, morphological defence, reproduction
 mode, resistance forms and respiration mode.Each trait was represented by
 several modalities or categories (e.g. the modalities for the trait
 “dispersal mode” were passive and active), and the affinity of the taxa
 for each modality was fuzzy coded. In total, the 12 traits were
 represented by 64 modalities.  Céréghino et al. (2018) reduced
 these 64 trait modalities to four orthogonal axes using PCA (original
 data:https://knb.ecoinformatics.org/view/doi:10.5063/F1VD6WMF). Since
 then, there have been modest updates to the trait data: some trait scores
 were improved, the number of missing values reduced, a few microscopic or
 terrestrial species were removed, and the taxonomic resolution of some
 species identifications was improved. We therefore reran the PCA analysis
 on the updated trait matrix, using the same R script as in Céréghino et
 al. (2018), and used the first four axes in our current study. Site
 information We collated information on biogeographic, bioclimatic, and
 sampling characteristics of each field site in order to better
 contextualize differences among sites. Biogeographic characteristics
 included position north and north west versus south and southeast of the
 Northern Andes (simplified hereafter as north versus south of Andes). The
 Northern Andes are known to be a dispersal barrier for both
 bromeliads  and bromeliad invertebrates. Although the Southern
 and Central Andes could be a potential barrier between the west coast and
 center of South America, we have no bromeliad data from the former and so
 do not analyse these mountain ranges. We examined species pool richness,
 estimated with Chao’s method (implemented in the vegan R package), to test
 if richer sites had stronger trait-environment matching. From the
 WorldClim database, we extracted site-specific estimates of four
 bioclimatic variables that a previous study (Guzman et al., 2020) found to
 underlie spatial variation in bromeliad macroinvertebrate traits: mean
 diurnal range in temperature (BC2), temperature annual seasonality (BC4),
 precipitation annual seasonality (BC15) and precipitation of the driest
 quarter (BC17). WorldClim data was extracted at the 1km2 scale; when field
 sites exceeded 1 km2 in size we averaged data over the relevant 1 km2
 pixels. Finally, we examined sampling characteristics of each site,
 including the number of bromeliads sampled (which affects the power of
 tests) and the site mean of the focal environmental gradient (in case
 trait-environment relationships differ with site position on gradient).
 Data processing We organized the abundance data using the R package fwdata
 (developed by A.A.M.M, https://github.com/SrivastavaLab/fwdata). We then
 converted abundances to biomass by multiplying abundance by the estimated
 per capita biomass of each morphospecies, based on taxon-specific
 allometric relationships, using the hellometry R package (provided
 courtesy of P. Rogy, https://github.com/pierrerogy/hellometry). Summary In
 sum, the dataset consists of information on environmental attributes of
 bromeliads (n = 1656), macroinvertebrate morphospecies (n = 615),
 macroinvertebrate traits (n = 64), local environmental gradients (n = 3)
 and field sites (n = 26). (text excerpted from Srivastava, DS et al. 2022.
 Geographical variation in the trait-based assembly patterns of
 multitrophic invertebrate communities. Functional Ecology)Other
These data, in combination with archived R scripts, allow reproducibility
 of the results reported in the 2022 Functional Ecology ms
 "Geographical variation in the trait-based assembly patterns of
 multitrophic invertebrate communities" by DS Srivastava and
 co-authors. A README.TXT file is included with full metadata for all data
 files. The data files include input files for the R Scripts
 (visits\_owners.csv, bromeliads\_syncsa\_ready.csv, biomass\_syncsa\_ready.csv,
 traits\_pca\_ready.csv, site\_covariates.csv) as well as three output files
 (RES\_pca\_individuals\_0.7.7\_ranks\_cleaned.txt, allout\_lastrun.csv,
 allaxes1234\_lastrun.csv). We include the output files linked to the
 published results for full reproducibility as permutation-based results
 can change slightly between runs. Although this data is being made
 publicly available, we do ask that those intending to reuse the data for
 publication purposes to please first contact the data collectors (see
 "visits\_owners.csv" file for names and emails of data
 collectors). We can provide you with valuable context and system-specific
 knowledge that will help you make best use of the data. We also point out
 that many data collectors are from the Global South and that there is a
 history in ecology of neocolonial practices regarding the unacknowledged
 flow of data from poorer to wealthier countries. Spatial structure of
 dataWe defined a distinct site as one that was distinct in terms of:
 space, elevation and taxonomic identity of its species pool. All
 bromeliads surveyed within a site must occur in an area {\textless}3.5 km in
 linear dimension and within a 300 m range of elevation. The invertebrate
 species pool should also be {\textgreater}50\% dissimilar (Jaccard dissimilarity,
 based on presence-absence) between adjacent sites. For example, bromeliads
 surveyed along a transect covering an elevation gradient (Sonadora
 transect, Puerto Rico) are assigned to a low elevation (400-700m) and high
 elevation (750-1000m) site based on differences in elevation and
 dissimilarity in the invertebrate species.A dataset is defined as a
 distinct combination of time and location.  For example, if one
 habitat in a site was surveyed in two different years, these would be
 considered distinct datasets with one visit each. As datasets are often
 defined slightly differently by different data collectors, this is a less
 consistent spatial unit than sites and so is not the basis of our
 analysis.Visits are defined as a survey of bromeliad macroinvertebrates
 distinct in habitat, time and location. Multiple visits may be nested
 within a dataset. If two distinct habitats were surveyed at the same
 location at the same time of year, these would also be considered two
 visits within a dataset. Missing values are indicated by NA in all files.},
}
