
@article{purgar_quantifying_2022,
	title = {Quantifying research waste in ecology},
	volume = {6},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-022-01820-0},
	doi = {10.1038/s41559-022-01820-0},
	language = {en},
	number = {9},
	urldate = {2025-02-27},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Klanjscek, Tin and Culina, Antica},
	month = jul,
	year = {2022},
	pages = {1390--1397},
}

@article{purgar_supporting_2024,
	title = {Supporting study registration to reduce research waste},
	volume = {8},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-024-02433-5},
	doi = {10.1038/s41559-024-02433-5},
	language = {en},
	number = {8},
	urldate = {2025-02-11},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Glasziou, Paul and Klanjscek, Tin and Nakagawa, Shinichi and Culina, Antica},
	month = jun,
	year = {2024},
	pages = {1391--1399},
}

@article{nakagawa_poor_2025,
	title = {Poor hypotheses and research waste in biology: learning from a theory crisis in psychology},
	volume = {23},
	issn = {1741-7007},
	shorttitle = {Poor hypotheses and research waste in biology},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-025-02134-w},
	doi = {10.1186/s12915-025-02134-w},
	abstract = {Abstract
            While psychologists have extensively discussed the notion of a “theory crisis” arising from vague and incorrect hypotheses, there has been no debate about such a crisis in biology. However, biologists have long discussed communication failures between theoreticians and empiricists. We argue such failure is one aspect of a theory crisis because misapplied and misunderstood theories lead to poor hypotheses and research waste. We review its solutions and compare them with methodology-focused solutions proposed for replication crises. We conclude by discussing how promoting inclusion, diversity, equity, and accessibility (IDEA) in theoretical biology could contribute to ameliorating breakdowns in the theory-empirical cycle.},
	language = {en},
	number = {1},
	urldate = {2025-02-11},
	journal = {BMC Biology},
	author = {Nakagawa, Shinichi and Armitage, David W. and Froese, Tom and Yang, Yefeng and Lagisz, Malgorzata},
	month = feb,
	year = {2025},
	pages = {33},
}

@article{gould_same_2025,
	title = {Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology},
	volume = {23},
	issn = {1741-7007},
	shorttitle = {Same data, different analysts},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-02101-x},
	doi = {10.1186/s12915-024-02101-x},
	language = {en},
	number = {1},
	urldate = {2025-08-28},
	journal = {BMC Biology},
	author = {Gould, Elliot and Fraser, Hannah S. and Parker, Timothy H. and Nakagawa, Shinichi and Griffith, Simon C. and Vesk, Peter A. and Fidler, Fiona and Hamilton, Daniel G. and Abbey-Lee, Robin N. and Abbott, Jessica K. and Aguirre, Luis A. and Alcaraz, Carles and Aloni, Irith and Altschul, Drew and Arekar, Kunal and Atkins, Jeff W. and Atkinson, Joe and Baker, Christopher M. and Barrett, Meghan and Bell, Kristian and Bello, Suleiman Kehinde and Beltrán, Iván and Berauer, Bernd J. and Bertram, Michael Grant and Billman, Peter D. and Blake, Charlie K. and Blake, Shannon and Bliard, Louis and Bonisoli-Alquati, Andrea and Bonnet, Timothée and Bordes, Camille Nina Marion and Bose, Aneesh P. H. and Botterill-James, Thomas and Boyd, Melissa Anna and Boyle, Sarah A. and Bradfer-Lawrence, Tom and Bradham, Jennifer and Brand, Jack A. and Brengdahl, Martin I. and Bulla, Martin and Bussière, Luc and Camerlenghi, Ettore and Campbell, Sara E. and Campos, Leonardo L. F. and Caravaggi, Anthony and Cardoso, Pedro and Carroll, Charles J. W. and Catanach, Therese A. and Chen, Xuan and Chik, Heung Ying Janet and Choy, Emily Sarah and Christie, Alec Philip and Chuang, Angela and Chunco, Amanda J. and Clark, Bethany L. and Contina, Andrea and Covernton, Garth A. and Cox, Murray P. and Cressman, Kimberly A. and Crotti, Marco and Crouch, Connor Davidson and D’Amelio, Pietro B. and De Sousa, Alexandra Allison and Döbert, Timm Fabian and Dobler, Ralph and Dobson, Adam J. and Doherty, Tim S. and Drobniak, Szymon Marian and Duffy, Alexandra Grace and Duncan, Alison B. and Dunn, Robert P. and Dunning, Jamie and Dutta, Trishna and Eberhart-Hertel, Luke and Elmore, Jared Alan and Elsherif, Mahmoud Medhat and English, Holly M. and Ensminger, David C. and Ernst, Ulrich Rainer and Ferguson, Stephen M. and Fernandez-Juricic, Esteban and Ferreira-Arruda, Thalita and Fieberg, John and Finch, Elizabeth A. and Fiorenza, Evan A. and Fisher, David N. and Fontaine, Amélie and Forstmeier, Wolfgang and Fourcade, Yoan and Frank, Graham S. and Freund, Cathryn A. and Fuentes-Lillo, Eduardo and Gandy, Sara L. and Gannon, Dustin G. and García-Cervigón, Ana I. and Garretson, Alexis C. and Ge, Xuezhen and Geary, William L. and Géron, Charly and Gilles, Marc and Girndt, Antje and Gliksman, Daniel and Goldspiel, Harrison B. and Gomes, Dylan G. E. and Good, Megan Kate and Goslee, Sarah C. and Gosnell, J. Stephen and Grames, Eliza M. and Gratton, Paolo and Grebe, Nicholas M. and Greenler, Skye M. and Griffioen, Maaike and Griffith, Daniel M. and Griffith, Frances J. and Grossman, Jake J. and Güncan, Ali and Haesen, Stef and Hagan, James G. and Hager, Heather A. and Harris, Jonathan Philo and Harrison, Natasha Dean and Hasnain, Sarah Syedia and Havird, Justin Chase and Heaton, Andrew J. and Herrera-Chaustre, María Laura and Howard, Tanner J. and Hsu, Bin-Yan and Iannarilli, Fabiola and Iranzo, Esperanza C. and Iverson, Erik N. K. and Jimoh, Saheed Olaide and Johnson, Douglas H. and Johnsson, Martin and Jorna, Jesse and Jucker, Tommaso and Jung, Martin and Kačergytė, Ineta and Kaltz, Oliver and Ke, Alison and Kelly, Clint D. and Keogan, Katharine and Keppeler, Friedrich Wolfgang and Killion, Alexander K. and Kim, Dongmin and Kochan, David P. and Korsten, Peter and Kothari, Shan and Kuppler, Jonas and Kusch, Jillian M. and Lagisz, Malgorzata and Lalla, Kristen Marianne and Larkin, Daniel J. and Larson, Courtney L. and Lauck, Katherine S. and Lauterbur, M. Elise and Law, Alan and Léandri-Breton, Don-Jean and Lembrechts, Jonas J. and L’Herpiniere, Kiara and Lievens, Eva J. P. and De Lima, Daniela Oliveira and Lindsay, Shane and Luquet, Martin and MacLeod, Ross and Macphie, Kirsty H. and Magellan, Kit and Mair, Magdalena M. and Malm, Lisa E. and Mammola, Stefano and Mandeville, Caitlin P. and Manhart, Michael and Manrique-Garzon, Laura Milena and Mäntylä, Elina and Marchand, Philippe and Marshall, Benjamin Michael and Martin, Charles A. and Martin, Dominic Andreas and Martin, Jake Mitchell and Martinig, April Robin and McCallum, Erin S. and McCauley, Mark and McNew, Sabrina M. and Meiners, Scott J. and Merkling, Thomas and Michelangeli, Marcus and Moiron, Maria and Moreira, Bruno and Mortensen, Jennifer and Mos, Benjamin and Muraina, Taofeek Olatunbosun and Murphy, Penelope Wrenn and Nelli, Luca and Niemelä, Petri and Nightingale, Josh and Nilsonne, Gustav and Nolazco, Sergio and Nooten, Sabine S. and Novotny, Jessie Lanterman and Olin, Agnes Birgitta and Organ, Chris L. and Ostevik, Kate L. and Palacio, Facundo Xavier and Paquet, Matthieu and Parker, Darren James and Pascall, David J. and Pasquarella, Valerie J. and Paterson, John Harold and Payo-Payo, Ana and Pedersen, Karen Marie and Perez, Grégoire and Perry, Kayla I. and Pottier, Patrice and Proulx, Michael J. and Proulx, Raphaël and Pruett, Jessica L and Ramananjato, Veronarindra and Randimbiarison, Finaritra Tolotra and Razafindratsima, Onja H. and Rennison, Diana J. and Riva, Federico and Riyahi, Sepand and Roast, Michael James and Rocha, Felipe Pereira and Roche, Dominique G. and Román-Palacios, Cristian and Rosenberg, Michael S. and Ross, Jessica and Rowland, Freya E. and Rugemalila, Deusdedith and Russell, Avery L. and Ruuskanen, Suvi and Saccone, Patrick and Sadeh, Asaf and Salazar, Stephen M. and Sales, Kris and Salmón, Pablo and Sánchez-Tójar, Alfredo and Santos, Leticia Pereira and Santostefano, Francesca and Schilling, Hayden T. and Schmidt, Marcus and Schmoll, Tim and Schneider, Adam C. and Schrock, Allie E. and Schroeder, Julia and Schtickzelle, Nicolas and Schultz, Nick L. and Scott, Drew A. and Scroggie, Michael Peter and Shapiro, Julie Teresa and Sharma, Nitika and Shearer, Caroline L. and Simón, Diego and Sitvarin, Michael I. and Skupien, Fabrício Luiz and Slinn, Heather Lea and Smith, Grania Polly and Smith, Jeremy A. and Sollmann, Rahel and Whitney, Kaitlin Stack and Still, Shannon Michael and Stuber, Erica F. and Sutton, Guy F. and Swallow, Ben and Taff, Conor Claverie and Takola, Elina and Tanentzap, Andrew J. and Tarjuelo, Rocío and Telford, Richard J. and Thawley, Christopher J. and Thierry, Hugo and Thomson, Jacqueline and Tidau, Svenja and Tompkins, Emily M. and Tortorelli, Claire Marie and Trlica, Andrew and Turnell, Biz R. and Urban, Lara and Van De Vondel, Stijn and Van Der Wal, Jessica Eva Megan and Van Eeckhoven, Jens and Van Oordt, Francis and Vanderwel, K. Michelle and Vanderwel, Mark C. and Vanderwolf, Karen J. and Vélez, Juliana and Vergara-Florez, Diana Carolina and Verrelli, Brian C. and Vieira, Marcus Vinícius and Villamil, Nora and Vitali, Valerio and Vollering, Julien and Walker, Jeffrey and Walker, Xanthe J. and Walter, Jonathan A. and Waryszak, Pawel and Weaver, Ryan J. and Wedegärtner, Ronja E. M. and Weller, Daniel L. and Whelan, Shannon and White, Rachel Louise and Wolfson, David William and Wood, Andrew and Yanco, Scott W. and Yen, Jian D. L. and Youngflesh, Casey and Zilio, Giacomo and Zimmer, Cédric and Zimmerman, Gregory Mark and Zitomer, Rachel A.},
	month = feb,
	year = {2025},
	pages = {35},
}

@article{bourne_ten_2017,
	title = {Ten simple rules to consider regarding preprint submission},
	volume = {13},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005473},
	doi = {10.1371/journal.pcbi.1005473},
	language = {en},
	number = {5},
	urldate = {2023-09-21},
	journal = {PLOS Computational Biology},
	author = {Bourne, Philip E. and Polka, Jessica K. and Vale, Ronald D. and Kiley, Robert},
	month = may,
	year = {2017},
	pages = {e1005473},
}

@article{gopalakrishna_prevalence_2022,
	title = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors: {A} survey among academic researchers in {The} {Netherlands}},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263023},
	doi = {10.1371/journal.pone.0263023},
	abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the “publish or perish” incentive system promotes research integrity.},
	language = {en},
	number = {2},
	urldate = {2023-09-05},
	journal = {PLOS ONE},
	author = {Gopalakrishna, Gowri and Riet, Gerben ter and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
	month = feb,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Deception, Linear regression analysis, Medical humanities, Medicine and health sciences, Open science, Research integrity, Scientific misconduct, Surveys},
	pages = {e0263023},
}

@article{ulrich_questionable_2020,
	title = {Questionable research practices may have little effect on replicability},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.58237},
	doi = {10.7554/eLife.58237},
	abstract = {This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.},
	urldate = {2023-09-05},
	journal = {eLife},
	author = {Ulrich, Rolf and Miller, Jeff},
	editor = {Rodgers, Peter and Thompson, William Hedley and Francis, Gregory},
	month = sep,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {base rate of true effects, false positives, mathematical modelling of research process, meta-research, p-hacking, replicability},
	pages = {e58237},
}

@article{filazzola_replication_2021,
	title = {Replication in field ecology: {Identifying} challenges and proposing solutions},
	volume = {12},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Replication in field ecology},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13657},
	doi = {10.1111/2041-210X.13657},
	language = {en},
	number = {10},
	urldate = {2021-10-09},
	journal = {Methods in Ecology and Evolution},
	author = {Filazzola, Alessandro and Cahill, James F.},
	month = oct,
	year = {2021},
	pages = {1780--1792},
}

@misc{magazine_where_2023,
	title = {Where the ‘{Wood}-{Wide} {Web}’ {Narrative} {Went} {Wrong}},
	url = {https://undark.org/2023/05/25/where-the-wood-wide-web-narrative-went-wrong/},
	abstract = {Opinion {\textbar} A fascinating story about forest fungal networks has captured the public imagination in recent years. Is any of it true?},
	language = {en-US},
	urldate = {2023-09-05},
	journal = {Undark Magazine},
	author = {Magazine, Undark},
	month = may,
	year = {2023},
}

@article{karst_positive_2023,
	title = {Positive citation bias and overinterpreted results lead to misinformation on common mycorrhizal networks in forests},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-01986-1},
	doi = {10.1038/s41559-023-01986-1},
	abstract = {A common mycorrhizal network (CMN) is formed when mycorrhizal fungal hyphae connect the roots of multiple plants of the same or different species belowground. Recently, CMNs have captured the interest of broad audiences, especially with respect to forest function and management. We are concerned, however, that recent claims in the popular media about CMNs in forests are disconnected from evidence, and that bias towards citing positive effects of CMNs has developed in the scientific literature. We first evaluated the evidence supporting three common claims. The claims that CMNs are widespread in forests and that resources are transferred through CMNs to increase seedling performance are insufficiently supported because results from field studies vary too widely, have alternative explanations or are too limited to support generalizations. The claim that mature trees preferentially send resources and defence signals to offspring through CMNs has no peer-reviewed, published evidence. We next examined how the results from CMN research are cited and found that unsupported claims have doubled in the past 25 years; a bias towards citing positive effects may obscure our understanding of the structure and function of CMNs in forests. We conclude that knowledge on CMNs is presently too sparse and unsettled to inform forest management.},
	language = {en},
	urldate = {2023-02-13},
	journal = {Nature Ecology \& Evolution},
	author = {Karst, Justine and Jones, Melanie D. and Hoeksema, Jason D.},
	month = feb,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Forest ecology, Forestry},
	pages = {1--11},
}

@article{roche_troubleshooting_2014,
	title = {Troubleshooting {Public} {Data} {Archiving}: {Suggestions} to {Increase} {Participation}},
	volume = {12},
	issn = {1545-7885},
	shorttitle = {Troubleshooting {Public} {Data} {Archiving}},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001779},
	doi = {10.1371/journal.pbio.1001779},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {PLoS Biology},
	author = {Roche, Dominique G. and Lanfear, Robert and Binning, Sandra A. and Haff, Tonya M. and Schwanz, Lisa E. and Cain, Kristal E. and Kokko, Hanna and Jennions, Michael D. and Kruuk, Loeske E. B.},
	editor = {Eisen, Jonathan A.},
	month = jan,
	year = {2014},
	pages = {e1001779},
}

@article{mejlgaard_research_2020,
	title = {Research integrity: nine ways to move from talk to walk},
	volume = {586},
	copyright = {2020 Nature},
	shorttitle = {Research integrity},
	url = {https://www.nature.com/articles/d41586-020-02847-8},
	doi = {10.1038/d41586-020-02847-8},
	abstract = {Counselling, coaches and collegiality — how institutions can share resources to promote best practice in science.},
	language = {en},
	number = {7829},
	urldate = {2020-10-27},
	journal = {Nature},
	author = {Mejlgaard, Niels and Bouter, Lex M. and Gaskell, George and Kavouras, Panagiotis and Allum, Nick and Bendtsen, Anna-Kathrine and Charitidis, Costas A. and Claesen, Nik and Dierickx, Kris and Domaradzka, Anna and Elizondo, Andrea Reyes and Foeger, Nicole and Hiney, Maura and Kaltenbrunner, Wolfgang and Labib, Krishma and Marušić, Ana and Sørensen, Mads P. and Ravn, Tine and Ščepanović, Rea and Tijdink, Joeri K. and Veltri, Giuseppe A.},
	month = oct,
	year = {2020},
	note = {Number: 7829
Publisher: Nature Publishing Group},
	pages = {358--360},
}

@article{gelman_garden_nodate,
	title = {The garden of forking paths: {Why} multiple comparisons can be a problem, even when there is no “ﬁshing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
	abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of ﬁshing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-speciﬁed and, as a result, were contingent on data.},
	language = {en},
	author = {Gelman, Andrew and Loken, Eric},
}

@article{azevedo_towards_2022,
	title = {Towards a culture of open scholarship: the role of pedagogical communities},
	volume = {15},
	issn = {1756-0500},
	shorttitle = {Towards a culture of open scholarship},
	url = {https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-05944-1},
	doi = {10.1186/s13104-022-05944-1},
	abstract = {Abstract
            
              The UK House of Commons Science and Technology Committee has called for evidence on the roles that different stakeholders play in reproducibility and research integrity. Of central priority are proposals for improving research integrity and quality, as well as guidance and support for researchers. In response to this, we argue that there is one important component of research integrity that is often absent from discussion: the pedagogical consequences of how we teach, mentor, and supervise students through open scholarship. We justify the need to integrate open scholarship principles into research training within higher education and argue that pedagogical communities play a key role in fostering an inclusive culture of open scholarship. We illustrate these benefits by presenting the
              Framework for Open and Reproducible Research Training (FORRT)
              , an international grassroots community whose goal is to provide support, resources, visibility, and advocacy for the adoption of principled, open teaching and mentoring practices, whilst generating conversations about the ethics and social impact of higher-education pedagogy. Representing a diverse group of early-career researchers and students across specialisms, we advocate for greater recognition of and support for pedagogical communities, and encourage all research stakeholders to engage with these communities to enable long-term, sustainable change.},
	language = {en},
	number = {1},
	urldate = {2022-04-28},
	journal = {BMC Research Notes},
	author = {Azevedo, Flávio and Liu, Meng and Pennington, Charlotte R. and Pownall, Madeleine and Evans, Thomas Rhys and Parsons, Sam and Elsherif, Mahmoud Medhat and Micheli, Leticia and Westwood, Samuel J. and {Framework for Open, Reproducible Research Training (FORRT)}},
	month = dec,
	year = {2022},
	pages = {75},
}

@article{davis_writing_2023,
	title = {Writing statistical methods for ecologists},
	volume = {14},
	issn = {2150-8925, 2150-8925},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.4539},
	doi = {10.1002/ecs2.4539},
	language = {en},
	number = {5},
	urldate = {2023-05-26},
	journal = {Ecosphere},
	author = {Davis, Amy J. and Kay, Shannon},
	month = may,
	year = {2023},
	pages = {e4539},
}

@article{soeharjono_reported_2021,
	title = {Reported {Individual} {Costs} and {Benefits} of {Sharing} {Open} {Data} among {Canadian} {Academic} {Faculty} in {Ecology} and {Evolution}},
	volume = {71},
	issn = {0006-3568, 1525-3244},
	url = {https://academic.oup.com/bioscience/article/71/7/750/6225906},
	doi = {10.1093/biosci/biab024},
	abstract = {Abstract
            Open data facilitate reproducibility and accelerate scientific discovery but are hindered by perceptions that researchers bear costs and gain few benefits from publicly sharing their data, with limited empirical evidence to the contrary. We surveyed 140 faculty members working in ecology and evolution across Canada's top 20 ranked universities and found that more researchers report benefits (47.9\%) and neutral outcomes (43.6\%) than costs (21.4\%) from openly sharing data. The benefits were independent of career stage and gender, but men and early career researchers were more likely to report costs. We outline mechanisms proposed by the study participants to reduce the individual costs and increase the benefits of open data for faculty members.},
	language = {en},
	number = {7},
	urldate = {2023-02-10},
	journal = {BioScience},
	author = {Soeharjono, Sandrine and Roche, Dominique G},
	month = jul,
	year = {2021},
	pages = {750--756},
}

@article{jenkins_reproducibility_2023,
	title = {Reproducibility in ecology and evolution: {Minimum} standards for data and code},
	volume = {13},
	issn = {2045-7758, 2045-7758},
	shorttitle = {Reproducibility in ecology and evolution},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ece3.9961},
	doi = {10.1002/ece3.9961},
	language = {en},
	number = {5},
	urldate = {2023-05-12},
	journal = {Ecology and Evolution},
	author = {Jenkins, Gareth B. and Beckerman, Andrew P. and Bellard, Céline and Benítez‐López, Ana and Ellison, Aaron M. and Foote, Christopher G. and Hufton, Andrew L. and Lashley, Marcus A. and Lortie, Christopher J. and Ma, Zhaoxue and Moore, Allen J. and Narum, Shawn R. and Nilsson, Johan and O'Boyle, Bridget and Provete, Diogo B. and Razgour, Orly and Rieseberg, Loren and Riginos, Cynthia and Santini, Luca and Sibbett, Benjamin and Peres‐Neto, Pedro R.},
	month = may,
	year = {2023},
	pages = {e9961},
}

@article{yang_publication_2023,
	title = {Publication bias impacts on effect size, statistical power, and magnitude ({Type} {M}) and sign ({Type} {S}) errors in ecology and evolutionary biology},
	volume = {21},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-022-01485-y},
	doi = {10.1186/s12915-022-01485-y},
	abstract = {Collaborative efforts to directly replicate empirical studies in the medical and social sciences have revealed alarmingly low rates of replicability, a phenomenon dubbed the ‘replication crisis’. Poor replicability has spurred cultural changes targeted at improving reliability in these disciplines. Given the absence of equivalent replication projects in ecology and evolutionary biology, two inter-related indicators offer the opportunity to retrospectively assess replicability: publication bias and statistical power. This registered report assesses the prevalence and severity of small-study (i.e., smaller studies reporting larger effect sizes) and decline effects (i.e., effect sizes decreasing over time) across ecology and evolutionary biology using 87 meta-analyses comprising 4,250 primary studies and 17,638 effect sizes. Further, we estimate how publication bias might distort the estimation of effect sizes, statistical power, and errors in magnitude (Type M or exaggeration ratio) and sign (Type S). We show strong evidence for the pervasiveness of both small-study and decline effects in ecology and evolution. There was widespread prevalence of publication bias that resulted in meta-analytic means being over-estimated by (at least) 0.12 standard deviations. The prevalence of publication bias distorted confidence in meta-analytic results, with 66\% of initially statistically significant meta-analytic means becoming non-significant after correcting for publication bias. Ecological and evolutionary studies consistently had low statistical power (15\%) with a 4-fold exaggeration of effects on average (Type M error rates = 4.4). Notably, publication bias reduced power from 23\% to 15\% and increased type M error rates from 2.7 to 4.4 because it creates a non-random sample of effect size evidence. The sign errors of effect sizes (Type S error) increased from 5\% to 8\% because of publication bias. Our research provides clear evidence that many published ecological and evolutionary findings are inflated. Our results highlight the importance of designing high-power empirical studies (e.g., via collaborative team science), promoting and encouraging replication studies, testing and correcting for publication bias in meta-analyses, and adopting open and transparent research practices, such as (pre)registration, data- and code-sharing, and transparent reporting.},
	number = {1},
	urldate = {2023-09-03},
	journal = {BMC Biology},
	author = {Yang, Yefeng and Sánchez-Tójar, Alfredo and O’Dea, Rose E. and Noble, Daniel W. A. and Koricheva, Julia and Jennions, Michael D. and Parker, Timothy H. and Lagisz, Malgorzata and Nakagawa, Shinichi},
	month = apr,
	year = {2023},
	keywords = {Generalizability, Many labs, Meta-research, Open science, P-hacking, Questionable research practices, Registered report, Replicability, Reproducibility, Selective reporting, Transparency},
	pages = {71},
}

@article{gomes_why_2022,
	title = {Why don't we share data and code? {Perceived} barriers and benefits to public archiving practices},
	volume = {289},
	shorttitle = {Why don't we share data and code?},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2022.1113},
	doi = {10.1098/rspb.2022.1113},
	abstract = {The biological sciences community is increasingly recognizing the value of open, reproducible and transparent research practices for science and society at large. Despite this recognition, many researchers fail to share their data and code publicly. This pattern may arise from knowledge barriers about how to archive data and code, concerns about its reuse, and misaligned career incentives. Here, we define, categorize and discuss barriers to data and code sharing that are relevant to many research fields. We explore how real and perceived barriers might be overcome or reframed in the light of the benefits relative to costs. By elucidating these barriers and the contexts in which they arise, we can take steps to mitigate them and align our actions with the goals of open science, both as individual scientists and as a scientific community.},
	number = {1987},
	urldate = {2023-08-31},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Gomes, Dylan G. E. and Pottier, Patrice and Crystal-Ornelas, Robert and Hudgins, Emma J. and Foroughirad, Vivienne and Sánchez-Reyes, Luna L. and Turba, Rachel and Martinez, Paula Andrea and Moreau, David and Bertram, Michael G. and Smout, Cooper A. and Gaynor, Kaitlyn M.},
	month = nov,
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {code reuse‌, data reuse, data science, open science, reproducibility, transparency},
	pages = {20221113},
}

@article{simmons_preregistration_2021,
	title = {Pre‐registration: {Why} and {How}},
	volume = {31},
	issn = {1057-7408, 1532-7663},
	shorttitle = {Pre‐registration},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jcpy.1208},
	doi = {10.1002/jcpy.1208},
	language = {en},
	number = {1},
	urldate = {2023-07-31},
	journal = {Journal of Consumer Psychology},
	author = {Simmons, Joseph and Nelson, Leif and Simonsohn, Uri},
	month = jan,
	year = {2021},
	pages = {151--162},
}

@article{carroll_operationalizing_2021,
	title = {Operationalizing the {CARE} and {FAIR} {Principles} for {Indigenous} data futures},
	volume = {8},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-021-00892-0},
	doi = {10.1038/s41597-021-00892-0},
	language = {en},
	number = {1},
	urldate = {2022-01-28},
	journal = {Scientific Data},
	author = {Carroll, Stephanie Russo and Herczog, Edit and Hudson, Maui and Russell, Keith and Stall, Shelley},
	month = dec,
	year = {2021},
	pages = {108},
}

@article{smaldino_natural_2016,
	title = {The natural selection of bad science},
	volume = {3},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.160384},
	doi = {10.1098/rsos.160384},
	abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
	language = {en},
	number = {9},
	urldate = {2022-09-05},
	journal = {Royal Society Open Science},
	author = {Smaldino, Paul E. and McElreath, Richard},
	month = sep,
	year = {2016},
	pages = {160384},
}

@article{nosek_preregistration_2019,
	title = {Preregistration {Is} {Hard}, {And} {Worthwhile}},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319301846},
	doi = {10.1016/j.tics.2019.07.009},
	language = {en},
	number = {10},
	urldate = {2022-06-20},
	journal = {Trends in Cognitive Sciences},
	author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and van ’t Veer, Anna E. and Vazire, Simine},
	month = oct,
	year = {2019},
	pages = {815--818},
}

@article{oboyle_chrysalis_2017,
	title = {The {Chrysalis} {Effect}: {How} {Ugly} {Initial} {Results} {Metamorphosize} {Into} {Beautiful} {Articles}},
	volume = {43},
	issn = {0149-2063, 1557-1211},
	shorttitle = {The {Chrysalis} {Effect}},
	url = {http://journals.sagepub.com/doi/10.1177/0149206314527133},
	doi = {10.1177/0149206314527133},
	abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the “Chrysalis Effect.”},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {Journal of Management},
	author = {O’Boyle, Ernest Hugh and Banks, George Christopher and Gonzalez-Mulé, Erik},
	month = feb,
	year = {2017},
	pages = {376--399},
}

@article{thibault_rigour_2022,
	title = {Rigour and reproducibility in {Canadian} research: call for a coordinated approach},
	volume = {7},
	issn = {2371-1671},
	shorttitle = {Rigour and reproducibility in {Canadian} research},
	url = {https://facetsjournal.com/doi/10.1139/facets-2021-0162},
	doi = {10.1139/facets-2021-0162},
	abstract = {Shortcomings in the rigour and reproducibility of research have become well-known issues and persist despite repeated calls for improvement. A coordinated effort among researchers, institutions, funders, publishers, learned societies, and regulators may be the most effective way of tackling these issues. The UK Reproducibility Network (UKRN) has fostered collaboration across various stakeholders in research and are creating the infrastructure necessary to advance rigorous and reproducible research practices across the United Kingdom. Other Reproducibility Networks, modelled on UKRN, are now emerging in other countries. Canada could benefit from a comparable network to unify the voices around research quality and maximize the value of Canadian research.},
	language = {en},
	urldate = {2022-08-23},
	journal = {FACETS},
	author = {Thibault, Robert T. and Munafò, Marcus R. and Moher, David},
	editor = {Taylor, Iain E.P.},
	month = jan,
	year = {2022},
	pages = {18--24},
}

@article{button_power_2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Power failure},
	url = {http://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
	language = {en},
	number = {5},
	urldate = {2020-03-17},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	pages = {365--376},
}

@article{clark_reply_2020,
	title = {Reply to: {Methods} matter in repeating ocean acidification studies},
	volume = {586},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Reply to},
	url = {http://www.nature.com/articles/s41586-020-2804-9},
	doi = {10.1038/s41586-020-2804-9},
	language = {en},
	number = {7830},
	urldate = {2021-09-02},
	journal = {Nature},
	author = {Clark, Timothy D. and Raby, Graham D. and Roche, Dominique G. and Binning, Sandra A. and Speers-Roesch, Ben and Jutfelt, Fredrik and Sundin, Josefin},
	month = oct,
	year = {2020},
	pages = {E25--E27},
}

@article{odea_towards_2021,
	title = {Towards open, reliable, and transparent ecology and evolutionary biology},
	volume = {19},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-021-01006-3},
	doi = {10.1186/s12915-021-01006-3},
	abstract = {Unreliable research programmes waste funds, time, and even the lives of the organisms we seek to help and understand. Reducing this waste and increasing the value of scientific evidence require changing the actions of both individual researchers and the institutions they depend on for employment and promotion. While ecologists and evolutionary biologists have somewhat improved research transparency over the past decade (e.g. more data sharing), major obstacles remain. In this commentary, we lift our gaze to the horizon to imagine how researchers and institutions can clear the path towards more credible and effective research programmes.},
	number = {1},
	urldate = {2021-04-09},
	journal = {BMC Biology},
	author = {O’Dea, Rose E. and Parker, Timothy H. and Chee, Yung En and Culina, Antica and Drobniak, Szymon M. and Duncan, David H. and Fidler, Fiona and Gould, Elliot and Ihle, Malika and Kelly, Clint D. and Lagisz, Malgorzata and Roche, Dominique G. and Sánchez-Tójar, Alfredo and Wilkinson, David P. and Wintle, Bonnie C. and Nakagawa, Shinichi},
	month = apr,
	year = {2021},
	pages = {68},
}

@article{desjardins-proulx_case_2013,
	title = {The {Case} for {Open} {Preprints} in {Biology}},
	volume = {11},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001563},
	doi = {10.1371/journal.pbio.1001563},
	language = {en},
	number = {5},
	urldate = {2020-10-12},
	journal = {PLoS Biology},
	author = {Desjardins-Proulx, Philippe and White, Ethan P. and Adamson, Joel J. and Ram, Karthik and Poisot, Timothée and Gravel, Dominique},
	month = may,
	year = {2013},
	pages = {e1001563},
}

@article{hampton_tao_2015,
	title = {The {Tao} of open science for ecology},
	volume = {6},
	issn = {2150-8925},
	url = {http://doi.wiley.com/10.1890/ES14-00402.1},
	doi = {10.1890/ES14-00402.1},
	language = {en},
	number = {7},
	urldate = {2020-10-12},
	journal = {Ecosphere},
	author = {Hampton, Stephanie E. and Anderson, Sean S. and Bagby, Sarah C. and Gries, Corinna and Han, Xueying and Hart, Edmund M. and Jones, Matthew B. and Lenhardt, W. Christopher and MacDonald, Andrew and Michener, William K. and Mudge, Joe and Pourmokhtarian, Afshin and Schildhauer, Mark P. and Woo, Kara H. and Zimmerman, Naupaka},
	month = jul,
	year = {2015},
	pages = {art120},
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab2374},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	urldate = {2020-10-07},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pages = {1422--1425},
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	language = {en},
	number = {11},
	urldate = {2020-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	pages = {2600--2606},
}

@article{fanelli_positive_2010,
	title = {“{Positive}” {Results} {Increase} {Down} the {Hierarchy} of the {Sciences}},
	volume = {5},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010068},
	doi = {10.1371/journal.pone.0010068},
	abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
	language = {en},
	number = {4},
	urldate = {2020-10-06},
	journal = {PLOS ONE},
	author = {Fanelli, Daniele},
	month = apr,
	year = {2010},
	note = {Publisher: Public Library of Science},
	keywords = {Forecasting, Mental health and psychiatry, Physical sciences, Scientists, Social psychology, Social research, Social sciences, Sociology},
	pages = {e10068},
}

@article{parker_transparency_2016,
	title = {Transparency in {Ecology} and {Evolution}: {Real} {Problems}, {Real} {Solutions}},
	volume = {31},
	issn = {0169-5347},
	shorttitle = {Transparency in {Ecology} and {Evolution}},
	url = {https://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(16)30095-7},
	doi = {10.1016/j.tree.2016.07.002},
	abstract = {Evidence suggests that insufficient transparency is a problem across much of ecology and evolution. Results and methods are often reported in insufficient detail or go entirely unreported. Further, these unreported results are often a biased subset, thus substantially hampering interpretation and meta-analysis.
Journals and other institutions, such as funding agencies, influence researchers’ decisions about disseminating results. There is a movement across empirical disciplines, including ecology and evolution, to shape institutional policies to better promote transparency.
Institutions can promote transparency by requiring or encouraging more disclosure, as with the now-familiar data archiving, or by developing an incentive structure promoting disclosure, such as preregistration of studies and analysis plans.
To make progress scientists need to know what other researchers have found and how they found it. However, transparency is often insufficient across much of ecology and evolution. Researchers often fail to report results and methods in detail sufficient to permit interpretation and meta-analysis, and many results go entirely unreported. Further, these unreported results are often a biased subset. Thus the conclusions we can draw from the published literature are themselves often biased and sometimes might be entirely incorrect. Fortunately there is a movement across empirical disciplines, and now within ecology and evolution, to shape editorial policies to better promote transparency. This can be done by either requiring more disclosure by scientists or by developing incentives to encourage disclosure.},
	language = {English},
	number = {9},
	urldate = {2020-09-19},
	journal = {Trends in Ecology \& Evolution},
	author = {Parker, Timothy H. and Forstmeier, Wolfgang and Koricheva, Julia and Fidler, Fiona and Hadfield, Jarrod D. and Chee, Yung En and Kelly, Clint D. and Gurevitch, Jessica and Nakagawa, Shinichi},
	month = sep,
	year = {2016},
	pmid = {27461041},
	note = {Publisher: Elsevier},
	keywords = {P-hacking, confirmation bias, inflated effect size, preregistration, replication, selective reporting},
	pages = {711--719},
}

@article{roche_public_2015,
	title = {Public {Data} {Archiving} in {Ecology} and {Evolution}: {How} {Well} {Are} {We} {Doing}?},
	volume = {13},
	issn = {1545-7885},
	shorttitle = {Public {Data} {Archiving} in {Ecology} and {Evolution}},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002295},
	doi = {10.1371/journal.pbio.1002295},
	abstract = {Policies that mandate public data archiving (PDA) successfully increase accessibility to data underlying scientific publications. However, is the data quality sufficient to allow reuse and reanalysis? We surveyed 100 datasets associated with nonmolecular studies in journals that commonly publish ecological and evolutionary research and have a strong PDA policy. Out of these datasets, 56\% were incomplete, and 64\% were archived in a way that partially or entirely prevented reuse. We suggest that cultural shifts facilitating clearer benefits to authors are necessary to achieve high-quality PDA and highlight key guidelines to help authors increase their data’s reuse potential and compliance with journal data policies.},
	language = {en},
	number = {11},
	urldate = {2020-09-19},
	journal = {PLOS Biology},
	author = {Roche, Dominique G. and Kruuk, Loeske E. B. and Lanfear, Robert and Binning, Sandra A.},
	month = nov,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Archives, Computer software, Evolutionary biology, Metadata, Public policy, Reproducibility, Science policy, Scientific publishing},
	pages = {e1002295},
}

@article{powers_open_2019,
	title = {Open science, reproducibility, and transparency in ecology},
	volume = {29},
	copyright = {© 2018 The Authors Ecological Applications published by Wiley Periodicals, Inc. on behalf of Ecological Society of America},
	issn = {1939-5582},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.1822},
	doi = {10.1002/eap.1822},
	abstract = {Reproducibility is a key tenet of the scientific process that dictates the reliability and generality of results and methods. The complexities of ecological observations and data present novel challenges in satisfying needs for reproducibility and also transparency. Ecological systems are dynamic and heterogeneous, interacting with numerous factors that sculpt natural history and that investigators cannot completely control. Observations may be highly dependent on spatial and temporal context, making them very difficult to reproduce, but computational reproducibility can still be achieved. Computational reproducibility often refers to the ability to produce equivalent analytical outcomes from the same data set using the same code and software as the original study. When coded workflows are shared, authors and editors provide transparency for readers and allow other researchers to build directly and efficiently on primary work. These qualities may be especially important in ecological applications that have important or controversial implications for science, management, and policy. Expectations for computational reproducibility and transparency are shifting rapidly in the sciences. In this work, we highlight many of the unique challenges for ecology along with practical guidelines for reproducibility and transparency, as ecologists continue to participate in the stewardship of critical environmental information and ensure that research methods demonstrate integrity.},
	language = {en},
	number = {1},
	urldate = {2020-09-21},
	journal = {Ecological Applications},
	author = {Powers, Stephen M. and Hampton, Stephanie E.},
	year = {2019},
	note = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822},
	keywords = {collaborative tools, data policy, data science, ecoinformatics, ecosystem, environmental science, open science, repeatability, replicability, reproducible, transparent, workflows},
	pages = {e01822},
}

@techreport{marwick_packaging_2018,
	type = {preprint},
	title = {Packaging data analytical work reproducibly using {R} (and friends)},
	url = {https://peerj.com/preprints/3192v2},
	abstract = {Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
	language = {en},
	urldate = {2020-09-24},
	institution = {PeerJ Preprints},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	month = mar,
	year = {2018},
	doi = {10.7287/peerj.preprints.3192v2},
}

@article{kelly_rate_2019,
	title = {Rate and success of study replication in ecology and evolution},
	volume = {7},
	issn = {2167-8359},
	url = {https://peerj.com/articles/7654},
	doi = {10.7717/peerj.7654},
	abstract = {The recent replication crisis has caused several scientific disciplines to self-reflect on the frequency with which they replicate previously published studies and to assess their success in such endeavours. The rate of replication, however, has yet to be assessed for ecology and evolution. Here, I survey the open-access ecology and evolution literature to determine how often ecologists and evolutionary biologists replicate, or at least claim to replicate, previously published studies. I found that approximately 0.023\% of ecology and evolution studies are described by their authors as replications. Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of “replication failure” by the authors.},
	language = {en},
	urldate = {2020-09-19},
	journal = {PeerJ},
	author = {Kelly, Clint D.},
	month = sep,
	year = {2019},
	note = {Publisher: PeerJ Inc.},
	pages = {e7654},
}

@article{jamieson_signaling_2020,
	title = {Signaling the trustworthiness of science},
	volume = {117},
	number = {14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jamieson, Kathleen Hall and McNutt, Marcia and Kiermer, Veronique and Sever, Richard},
	year = {2020},
	note = {Publisher: NATL ACAD SCIENCES 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA},
	pages = {8212--8212},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2020-09-29},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Cancer risk factors, Finance, Genetic epidemiology, Genetics of disease, Metaanalysis, Randomized controlled trials, Research design, Schizophrenia},
	pages = {e124},
}

@article{fraser_questionable_2018,
	title = {Questionable research practices in ecology and evolution},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
	doi = {10.1371/journal.pone.0200303},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	language = {en},
	number = {7},
	urldate = {2020-09-19},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	month = jul,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Behavioral ecology, Community ecology, Evolutionary biology, Evolutionary ecology, Evolutionary rate, Psychology, Publication ethics, Statistical data},
	pages = {e0200303},
}

@article{fraser_role_2020,
	title = {The role of replication studies in ecology},
	volume = {10},
	copyright = {© 2020 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
	issn = {2045-7758},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.6330},
	doi = {10.1002/ece3.6330},
	abstract = {Recent large-scale projects in other disciplines have shown that results often fail to replicate when studies are repeated. The conditions contributing to this problem are also present in ecology, but there have not been any equivalent replication projects. Here, we survey ecologists' understanding of and opinions about replication studies. The majority of ecologists in our sample considered replication studies to be important (97\%), not prevalent enough (91\%), worth funding even given limited resources (61\%), and suitable for publication in all journals (62\%). However, there is a disconnect between this enthusiasm and the prevalence of direct replication studies in the literature which is much lower (0.023\%: Kelly 2019) than our participants' median estimate of 10\%. This may be explained by the obstacles our participants identified including the difficulty of conducting replication studies and of funding and publishing them. We conclude by offering suggestions for how replications could be better integrated into ecological research.},
	language = {en},
	number = {12},
	urldate = {2020-09-19},
	journal = {Ecology and Evolution},
	author = {Fraser, Hannah and Barnett, Ashley and Parker, Timothy H. and Fidler, Fiona},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.6330},
	keywords = {conceptual replication, direct replication, generalizability, open science, repeatability, replicability, reproducibility, transparency},
	pages = {5197--5207},
}

@article{bafeta_ten_2020,
	title = {Ten simple rules for open human health research},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007846},
	doi = {10.1371/journal.pcbi.1007846},
	language = {en},
	number = {9},
	urldate = {2020-09-27},
	journal = {PLOS Computational Biology},
	author = {Bafeta, Aïda and Bobe, Jason and Clucas, Jon and Gonsalves, Pattie Pramila and Gruson-Daniel, Célya and Hudson, Kathy L. and Klein, Arno and Krishnakumar, Anirudh and McCollister-Slipp, Anna and Lindner, Ariel B. and Misevic, Dusan and Naslund, John A. and Nebeker, Camille and Nikolaidis, Aki and Pasquetto, Irene and Sanchez, Gabriela and Schapira, Matthieu and Scheininger, Tohar and Schoeller, Félix and Heinsfeld, Anibal Sólon and Taddei, François},
	month = sep,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Computer software, Medical risk factors, Open science, Reproducibility, Research design, Research ethics, Scientists, Social media},
	pages = {e1007846},
}

@article{culumber_widespread_2019,
	title = {Widespread {Biases} in {Ecological} and {Evolutionary} {Studies}},
	volume = {69},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/69/8/631/5523265},
	doi = {10.1093/biosci/biz063},
	abstract = {There has been widespread discussion of biases in the sciences. The extent of most forms of bias has scarcely been confronted with rigorous data. In the present article, we evaluated the potential for geographic, taxonomic, and citation biases in publications between temperate and tropical systems for nine broad topics in ecology and evolutionary biology. Across 1,800 papers sampled from 60,000 peer-reviewed, empirical studies, we found consistent patterns of bias in the form of increased numbers of studies in temperate systems. Tropical studies were nearly absent from some topics. Furthermore, there were strong taxonomic biases across topics and geographic regions, as well as evidence for citation biases in many topics. Our results indicate a strong geographic imbalance in publishing patterns and among different taxonomic groups across a wide range of topics. The task ahead is to address what these biases mean and how they influence the state of our knowledge in ecology and evolution.},
	language = {en},
	number = {8},
	urldate = {2020-09-19},
	journal = {BioScience},
	author = {Culumber, Zachary W. and Anaya-Rojas, Jaime M. and Booker, William W. and Hooks, Alexandra P. and Lange, Elizabeth C. and Pluer, Benjamin and Ramírez-Bullón, Natali and Travis, Joseph},
	month = aug,
	year = {2019},
	note = {Publisher: Oxford Academic},
	pages = {631--640},
}

@article{bourne_ten_2017-1,
	title = {Ten simple rules to consider regarding preprint submission},
	volume = {13},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005473},
	doi = {10.1371/journal.pcbi.1005473},
	language = {en},
	number = {5},
	urldate = {2020-09-22},
	journal = {PLOS Computational Biology},
	author = {Bourne, Philip E. and Polka, Jessica K. and Vale, Ronald D. and Kiley, Robert},
	month = may,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Internet, Mathematical physics, Medical journals, Medicine and health sciences, Peer review, Scientific publishing, Scientists, Software tools},
	pages = {e1005473},
}

@misc{noauthor_replication_nodate,
	title = {Replication {Failures} {Highlight} {Biases} in {Ecology} and {Evolution} {Science}},
	url = {https://www.the-scientist.com/features/replication-failures-highlight-biases-in-ecology-and-evolution-science-64475},
	abstract = {As robust efforts fail to reproduce findings of influential zebra finch studies from the 1980s, scientists discuss ways to reduce bias in such research.},
	language = {en},
	urldate = {2020-09-30},
	journal = {The Scientist Magazine®},
}
